PAGE	Algebraic equation	'Equations'	'Polynomials'
In mathematics, an algebraic equation or polynomial equation is an equation of the form
where P and Q are polynomials with coefficients in some field, often the field of the rational numbers. For most authors, an algebraic equation is univariate, which means that it involves only one variable. On the other hand, a polynomial equation may involve several variables, in which case it is called multivariate and the term polynomial equation is usually preferred to algebraic equation.
For example,
is an algebraic equation with integer coefficients and
is a multivariate polynomial equation over the rationals.
Some but not all polynomial equations with rational coefficients have a solution that is an algebraic expression with a finite number of operations involving just those coefficients (that is, can be solved algebraically). This can be done for all such equations of degree one, two, three, or four; but for degree five or more it can only be done for some equations but not for all. A large amount of research has been devoted to compute efficiently accurate approximations of the real or complex solutions of an univariate algebraic equation (see Root-finding algorithm) and of the common solutions of several multivariate polynomial equations (see System of polynomial equations).
== History ==
The study of algebraic equations is probably as old as mathematics: the Babylonian mathematicians, as early as 2000 BC could solve some kind of quadratic equations (displayed on Old Babylonian clay tablets).
The algebraic equations over the rationals with only one variable are also called univariate equations. They have a very long history. Ancient mathematicians wanted the solutions in the form of radical expressions, like for the positive solution of . The ancient Egyptians knew how to solve equations of degree 2 in this manner. In the 9th century Muhammad ibn Musa al-Khwarizmi and other Islamic mathematicians derived the quadratic formula, the general solution of equations of degree 2, and recognized the importance of the discriminant. During the Renaissance in 1545, Gerolamo Cardano found the solution to equations of degree 3 and Lodovico Ferrari solved equations of degree 4. Finally Niels Henrik Abel proved, in 1824, that equations of degree 5 and equations of higher degree are not always solvable using radicals. Galois theory, named after &#201; variste Galois, was introduced to give criteria deciding if an equation is solvable using radicals.
== Areas of study ==
The algebraic equations are the basis of a number of areas of modern mathematics: Algebraic number theory is the study of (univariate) algebraic equations over the rationals. Galois theory has been introduced by &#201; variste Galois for getting criteria deciding if an algebraic equation may be solved in terms of radicals. In field theory, an algebraic extension is an extension such that every element is a root of an algebraic equation over the base field. Transcendence theory is the study of the real numbers which are not solutions to an algebraic equation over the rationals. A Diophantine equation is a (usually multivariate) polynomial equation with integer coefficients for which one is interested in the integer solutions. Algebraic geometry is the study of the solutions in an algebraically closed field of multivariate polynomial equations.
Two equations are equivalent if they have the same set of solutions. In particular the equation is equivalent with . It follows that the study of algebraic equations is equivalent to the study of polynomials.
A polynomial equation over the rationals can always be converted to an equivalent one in which the coefficients are integers. For example, multiplying through by 42 = 2 &#183; 3 &#183; 7 and grouping its terms in the first member, the previously mentioned polynomial equation becomes
Because sine, exponentiation, and 1/T are not polynomial functions,
is not a polynomial equation in the four variables x, y, z, and T over the rational numbers. However, it is a polynomial equation in the three variables x, y, and z over the field of the elementary functions in the variable T.
As for any equation, the solutions of an equation are the values of the variables for which the equation is true. For univariate algebraic equations these are also called roots, even if, properly speaking, one should say the solutions of the algebraic equation P=0 are the roots of the polynomial P. When solving an equation, it is important to specify in which set the solutions are allowed. For example, for an equation over the rationals one may look for solutions in which all the variables are integers. In this case the equation is a diophantine equation. One may also be interested only in the real solutions. However, for univariate algebraic equations, the number of solutions is finite and all solutions, are contained in any algebraically closed field containing the coefficients, for example, the field of complex numbers in case of equations over the rationals. It follows that without precision "root" and "solution" usually mean "solution in an algebraically closed field".
== See also ==
Algebraic function
Algebraic number
Root finding
Linear equation (degree = 1)
Quadratic equation (degree = 2)
Cubic equation (degree = 3)
Quartic equation (degree = 4)
Quintic equation (degree = 5)
Sextic equation (degree = 6)
Septic equation (degree = 7)
System of linear equations
System of polynomial equations
Linear Diophantine equation
Linear equation over a ring
== References ==
Hazewinkel, Michiel, ed. (2001), "Algebraic equation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Algebraic Equation", MathWorld.
PAGE	Algebraic expression	'Elementary algebra'
In mathematics, an algebraic expression is an expression built up from constants, variables, and the algebraic operations (addition, subtraction, multiplication, division and exponentiation by an exponent that is a rational number). For example, is an algebraic expression. Since taking the square root is the same as raising to the power ,
is also an algebraic expression.
A rational expression is an expression that may be rewritten to a rational fraction by using the properties of the arithmetic operations (commutative properties and associative properties of addition and multiplication, distributive property and rules for the operations on the fractions). In other words, a rational expression is an expression which may be constructed from the variables and the constants by using only the four operations of arithmetic. Thus, is a rational expression, whereas is not.
A rational equation is an equation in which two rational fractions (or rational expressions) of the form are set equal to each other. These expressions obey the same rules as fractions. The equations can be solved by cross-multiplying. Division by zero is undefined, so that a solution causing formal division by zero is rejected.
== Terminology ==
Algebra has its own terminology to describe parts of an expression:
1 &#8211; Exponent (power), 2 &#8211; coefficient, 3 &#8211; term, 4 &#8211; operator, 5 &#8211; constant, - variables
== Conventions ==
=== Variables ===
By convention, letters at the beginning of the alphabet (e.g. ) are typically used to represent constants, and those toward the end of the alphabet (e.g. and ) are used to represent variables. They are usually written in italics.
=== Exponents ===
By convention, terms with the highest power (exponent), are written on the left, for example, is written to the left of . When a coefficient is one, it is usually omitted (e.g. is written ). Likewise when the exponent (power) is one, (e.g. is written ), and, when the exponent is zero, the result is always 1 (e.g. is written , since is always ).
== Algebraic vs. other mathematical expressions ==
The table below summarizes how algebraic expressions compare with several other types of mathematical expressions.
A rational algebraic expression (or rational expression) is an algebraic expression that can be written as a quotient of polynomials, such as x2 + 4x + 4. An irrational algebraic expression is one that is not rational, such as &#8730; x + 4.
== See also ==
Algebraic equation
Linear_equation#Algebraic_equations
Algebraic function
Analytical expression
Arithmetic expression
Closed-form expression
Expression (mathematics)
Polynomial
Term (logic)
== Notes ==
== References ==
James, Robert Clarke; James, Glenn (1992). Mathematics dictionary. p. 8. 
== External links ==
Weisstein, Eric W., "Algebraic Expression", MathWorld.
PAGE	Algebraic fraction	'Elementary algebra'
In algebra, an algebraic fraction is a fraction whose numerator and denominator are algebraic expressions. Two examples of algebraic fractions are and . Algebraic fractions are subject to the same laws as arithmetic fractions.
A rational fraction is an algebraic fraction whose numerator and denominator are both polynomials. Thus is a rational fraction, but not because the numerator contains a square root function.
== Terminology ==
In the algebraic fraction , the dividend a is called the numerator and the divisor b is called the denominator. The numerator and denominator are called the terms of the algebraic fraction.
A complex fraction is a fraction whose numerator or denominator, or both, contains a fraction. A simple fraction contains no fraction either in its numerator or its denominator. A fraction is in lowest terms if the only factor common to the numerator and the denominator is 1.
An expression which is not in fractional form is an integral expression. An integral expression can always be written in fractional form by giving it the denominator 1. A mixed expression is the algebraic sum of one or more integral expressions and one or more fractional terms.
== Rational fractions ==
If the expressions a and b are polynomials, the algebraic fraction is called a rational algebraic fraction or simply rational fraction. Rational fractions are also known as rational expressions. A rational fraction is called proper if , and improper otherwise. For example, the rational fraction is proper, and the rational fractions and are improper. Any improper rational fraction can be expressed as the sum of a polynomial (possibly constant) and a proper rational fraction. In the first example of an improper fraction one has
where the second term is a proper rational fraction. The sum of two proper rational fractions is a proper rational fraction as well. The reverse process of expressing a proper rational fraction as the sum of two or more fractions is called resolving it into partial fractions. For example,
Here, the two terms on the right are called partial fractions.
== Irrational fractions ==
An irrational fraction is one that contains the variable under a fractional exponent. An example of an irrational fraction is
The process of transforming an irrational fraction to a rational fraction is known as rationalization. Every irrational fraction in which the radicals are monomials may be rationalized by finding the least common multiple of the indices of the roots, and substituting the variable for another variable with the least common multiple as exponent. In the example given, the least common multiple is 6, hence we can substitute to obtain
== Notes ==
== References ==
Brink, Raymond W. (1951). "IV. Fractions". College Algebra.
PAGE	Cartesian coordinate system	'Elementary mathematics'
A Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances from the point to two fixed perpendicular directed lines, measured in the same unit of length. Each reference line is called a coordinate axis or just axis of the system, and the point where they meet is its origin, usually at ordered pair (0, 0). The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.
One can use the same principle to specify the position of any point in three-dimensional space by three Cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). In general, n Cartesian coordinates (an element of real n-space) specify the point in an n-dimensional Euclidean space for any dimension n. These coordinates are equal, up to sign, to distances from the point to n mutually perpendicular hyperplanes.
The invention of Cartesian coordinates in the 17th century by Ren &#233; Descartes (Latinized name: Cartesius) revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2 in a plane may be described as the set of all points whose coordinates x and y satisfy the equation x2 + y2 = 4.
Cartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory, and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering, and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design, and other geometry-related data processing.
== History ==
The adjective Cartesian refers to the French mathematician and philosopher Ren &#233; Descartes (who used the name Cartesius in Latin).
The idea of this system was developed in 1637 in writings by Descartes and independently by Pierre de Fermat, although Fermat also worked in three dimensions and did not publish the discovery. Both authors used a single axis in their treatments and have a variable length measured in reference to this axis. The concept of using a pair of axes was introduced later, after Descartes' La G &#233; om &#233; trie was translated into Latin in 1649 by Frans van Schooten and his students. These commentators introduced several concepts while trying to clarify the ideas contained in Descartes' work.
The development of the Cartesian coordinate system would play an intrinsic role in the development of the calculus by Isaac Newton and Gottfried Wilhelm Leibniz.
Nicole Oresme, a French cleric and friend of the Dauphin (later to become King Charles V) of the 14th Century, used constructions similar to Cartesian coordinates well before the time of Descartes and Fermat.
Many other coordinate systems have been developed since Descartes, such as the polar coordinates for the plane, and the spherical and cylindrical coordinates for three-dimensional space.
== Description ==
=== One dimension ===
Choosing a Cartesian coordinate system for a one-dimensional space &#8212; that is, for a straight line &#8212; involves choosing a point O of the line (the origin), a unit of length, and an orientation for the line. An orientation chooses which of the two half-lines determined by O is the positive, and which is negative; we then say that the line "is oriented" (or "points") from the negative half towards the positive half. Then each point P of the line can be specified by its distance from O, taken with a + or &#8722; sign depending on which half-line contains P.
A line with a chosen Cartesian system is called a number line. Every real number has a unique location on the line. Conversely, every point on the line can be interpreted as a number in an ordered continuum such as the real numbers.
=== Two dimensions ===
The modern Cartesian coordinate system in two dimensions (also called a rectangular coordinate system) is defined by an ordered pair of perpendicular lines (axes), a single unit of length for both axes, and an orientation for each axis. (Early systems allowed "oblique" axes, that is, axes that did not meet at right angles.) The lines are commonly referred to as the x- and y-axes where the x-axis is taken to be horizontal and the y-axis is taken to be vertical. The point where the axes meet is taken as the origin for both, thus turning each axis into a number line. For a given point P, a line is drawn through P perpendicular to the x-axis to meet it at X and second line is drawn through P perpendicular to the y-axis to meet it at Y. The coordinates of P are then X and Y interpreted as numbers x and y on the corresponding number lines. The coordinates are written as an ordered pair (x, y).
The point where the axes meet is the common origin of the two number lines and is simply called the origin. It is often labeled O and if so then the axes are called Ox and Oy. A plane with x- and y-axes defined is often referred to as the Cartesian plane or xy plane. The value of x is called the x-coordinate or abscissa and the value of y is called the y-coordinate or ordinate.
The choices of letters come from the original convention, which is to use the latter part of the alphabet to indicate unknown values. The first part of the alphabet was used to designate known values.
In the Cartesian plane, reference is sometimes made to a unit circle or a unit hyperbola.
=== Three dimensions ===
Choosing a Cartesian coordinate system for a three-dimensional space means choosing an ordered triplet of lines (axes) that are pair-wise perpendicular, have a single unit of length for all three axes and have an orientation for each axis. As in the two-dimensional case, each axis becomes a number line. The coordinates of a point P are obtained by drawing a line through P perpendicular to each coordinate axis, and reading the points where these lines meet the axes as three numbers of these number lines.
Alternatively, the coordinates of a point P can also be taken as the (signed) distances from P to the three planes defined by the three axes. If the axes are named x, y, and z, then the x-coordinate is the distance from the plane defined by the y and z axes. The distance is to be taken with the + or &#8722; sign, depending on which of the two half-spaces separated by that plane contains P. The y and z coordinates can be obtained in the same way from the x &#8211; z and x &#8211; y planes respectively.
=== Higher dimensions ===
A Euclidean plane with a chosen Cartesian system is called a Cartesian plane. Since Cartesian coordinates are unique and non-ambiguous, the points of a Cartesian plane can be identified with pairs of real numbers; that is with the Cartesian product , where is the set of all reals. In the same way, the points any Euclidean space of dimension n be identified with the tuples (lists) of n real numbers, that is, with the Cartesian product .
=== Generalizations ===
The concept of Cartesian coordinates generalizes to allow axes that are not perpendicular to each other, and/or different units along each axis. In that case, each coordinate is obtained by projecting the point onto one axis along a direction that is parallel to the other axis (or, in general, to the hyperplane defined by all the other axes). In such an oblique coordinate system the computations of distances and angles must be modified from that in standard Cartesian systems, and many standard formulas (such as the Pythagorean formula for the distance) do not hold.
== Notations and conventions ==
The Cartesian coordinates of a point are usually written in parentheses and separated by commas, as in (10, 5) or (3, 5, 7). The origin is often labelled with the capital letter O. In analytic geometry, unknown or generic coordinates are often denoted by the letters x and y on the plane, and x, y, and z in three-dimensional space. This custom comes from a convention of algebra, which use letters near the end of the alphabet for unknown values (such as were the coordinates of points in many geometric problems), and letters near the beginning for given quantities.
These conventional names are often used in other domains, such as physics and engineering, although other letters may be used. For example, in a graph showing how a pressure varies with time, the graph coordinates may be denoted t and p. Each axis is usually named after the coordinate which is measured along it; so one says the x-axis, the y-axis, the t-axis, etc.
Another common convention for coordinate naming is to use subscripts, as in x1, x2, ... xn for the n coordinates in an n-dimensional space; especially when n is greater than 3, or not specified. Some authors prefer the numbering x0, x1, ... xn &#8722; 1. These notations are especially advantageous in computer programming: by storing the coordinates of a point as an array, instead of a record, the subscript can serve to index the coordinates.
In mathematical illustrations of two-dimensional Cartesian systems, the first coordinate (traditionally called the abscissa) is measured along a horizontal axis, oriented from left to right. The second coordinate (the ordinate) is then measured along a vertical axis, usually oriented from bottom to top.
However, computer graphics and image processing often use a coordinate system with the y axis oriented downwards on the computer display. This convention developed in the 1960s (or earlier) from the way that images were originally stored in display buffers.
For three-dimensional systems, a convention is to portray the x &#8211; y plane horizontally, with the z axis added to represent height (positive up). Furthermore, there is a convention to orient the x-axis toward the viewer, biased either to the right or left. If a diagram (3D projection or 2D perspective drawing) shows the x and y axis horizontally and vertically, respectively, then the z axis should be shown pointing "out of the page" towards the viewer or camera. In such a 2D diagram of a 3D coordinate system, the z axis would appear as a line or ray pointing down and to the left or down and to the right, depending on the presumed viewer or camera perspective. In any diagram or display, the orientation of the three axes, as a whole, is arbitrary. However, the orientation of the axes relative to each other should always comply with the right-hand rule, unless specifically stated otherwise. All laws of physics and math assume this right-handedness, which ensures consistency.
For 3D diagrams, the names "abscissa" and "ordinate" are rarely used for x and y, respectively. When they are, the z-coordinate is sometimes called the applicate. The words abscissa, ordinate and applicate are sometimes used to refer to coordinate axes rather than the coordinate values.
=== Quadrants and octants ===
The axes of a two-dimensional Cartesian system divide the plane into four infinite regions, called quadrants, each bounded by two half-axes. These are often numbered from 1st to 4th and denoted by Roman numerals: I (where the signs of the two coordinates are +,+), II ( &#8722; ,+), III ( &#8722; , &#8722; ), and IV (+, &#8722; ). When the axes are drawn according to the mathematical custom, the numbering goes counter-clockwise starting from the upper right ("north-east") quadrant.
Similarly, a three-dimensional Cartesian system defines a division of space into eight regions or octants, according to the signs of the coordinates of the points. The convention used for naming a specific octant is to list its signs, e.g. (+ + +) or ( &#8722; + &#8722; ). The generalization of the quadrant and octant to arbitrary number of dimensions is the orthant, and a similar naming system applies.
== Cartesian formulas for the plane ==
=== Distance between two points ===
The Euclidean distance between two points of the plane with Cartesian coordinates and is
This is the Cartesian version of Pythagoras's theorem. In three-dimensional space, the distance between points and is
which can be obtained by two consecutive applications of Pythagoras' theorem.
=== Euclidean transformations ===
The Euclidean transformations or Euclidean motions are the (bijective) mappings of points of the Euclidean plane to themselves which preserve distances between points. There are four types of these mappings (also called isometries): translations, rotations, reflections and glide reflections.
==== Translation ====
Translating a set of points of the plane, preserving the distances and directions between them, is equivalent to adding a fixed pair of numbers (a, b) to the Cartesian coordinates of every point in the set. That is, if the original coordinates of a point are (x, y), after the translation they will be
==== Rotation ====
To rotate a figure counterclockwise around the origin by some angle is equivalent to replacing every point with coordinates (x,y) by the point with coordinates (x',y'), where
Thus: 
==== Reflection ====
If (x, y) are the Cartesian coordinates of a point, then ( &#8722; x, y) are the coordinates of its reflection across the second coordinate axis (the Y-axis), as if that line were a mirror. Likewise, (x, &#8722; y) are the coordinates of its reflection across the first coordinate axis (the X-axis). In more generality, reflection across a line through the origin making an angle with the x-axis, is equivalent to replacing every point with coordinates (x, y) by the point with coordinates (x &#8242; ,y &#8242; ), where
Thus: 
==== Glide reflection ====
A glide reflection is the composition of a reflection across a line followed by a translation in the direction of that line. It can be seen that the order of these operations does not matter (the translation can come first, followed by the reflection).
==== General matrix form of the transformations ====
These Euclidean transformations of the plane can all be described in a uniform way by using matrices. The result of applying a Euclidean transformation to a point is given by the formula
where A is a 2 &#215; 2 orthogonal matrix and b = (b1, b2) is an arbitrary ordered pair of numbers; that is,
where
 [Note the use of row vectors for point coordinates and that the matrix is written on the right.]
To be orthogonal, the matrix A must have orthogonal rows with same Euclidean length of one, that is,
and
This is equivalent to saying that A times its transpose must be the identity matrix. If these conditions do not hold, the formula describes a more general affine transformation of the plane provided that the determinant of A is not zero.
The formula defines a translation if and only if A is the identity matrix. The transformation is a rotation around some point if and only if A is a rotation matrix, meaning that
A reflection or glide reflection is obtained when,
Assuming that translation is not used transformations can be combined by simply multiplying the associated transformation matrices.
==== Affine transformation ====
Another way to represent coordinate transformations in Cartesian coordinates is through affine transformations. In affine transformations an extra dimension is added and all points are given a value of 1 for this extra dimension. The advantage of doing this is that point translations can be specified in the final column of matrix A. In this way, all of the euclidean transformations become transactable as matrix point multiplications. The affine transformation is given by:
 [Note the matrix A from above was transposed. The matrix is on the left and column vectors for point coordinates are used.]
Using affine transformations multiple different euclidean transformations including translation can be combined by simply multiplying the corresponding matrices.
==== Scaling ====
An example of an affine transformation which is not a Euclidean motion is given by scaling. To make a figure larger or smaller is equivalent to multiplying the Cartesian coordinates of every point by the same positive number m. If (x, y) are the coordinates of a point on the original figure, the corresponding point on the scaled figure has coordinates
If m is greater than 1, the figure becomes larger; if m is between 0 and 1, it becomes smaller.
==== Shearing ====
A shearing transformation will push the top of a square sideways to form a parallelogram. Horizontal shearing is defined by:
Shearing can also be applied vertically:
== Orientation and handedness ==
=== In two dimensions ===
Fixing or choosing the x-axis determines the y-axis up to direction. Namely, the y-axis is necessarily the perpendicular to the x-axis through the point marked 0 on the x-axis. But there is a choice of which of the two half lines on the perpendicular to designate as positive and which as negative. Each of these two choices determines a different orientation (also called handedness) of the Cartesian plane.
The usual way of orienting the axes, with the positive x-axis pointing right and the positive y-axis pointing up (and the x-axis being the "first" and the y-axis the "second" axis) is considered the positive or standard orientation, also called the right-handed orientation.
A commonly used mnemonic for defining the positive orientation is the right hand rule. Placing a somewhat closed right hand on the plane with the thumb pointing up, the fingers point from the x-axis to the y-axis, in a positively oriented coordinate system.
The other way of orienting the axes is following the left hand rule, placing the left hand on the plane with the thumb pointing up.
When pointing the thumb away from the origin along an axis towards positive, the curvature of the fingers indicates a positive rotation along that axis.
Regardless of the rule used to orient the axes, rotating the coordinate system will preserve the orientation. Switching any two axes will reverse the orientation, but switching both will leave the orientation unchanged.
=== In three dimensions ===
Once the x- and y-axes are specified, they determine the line along which the z-axis should lie, but there are two possible directions on this line. The two possible coordinate systems which result are called 'right-handed' and 'left-handed'. The standard orientation, where the xy-plane is horizontal and the z-axis points up (and the x- and the y-axis form a positively oriented two-dimensional coordinate system in the xy-plane if observed from above the xy-plane) is called right-handed or positive.
The name derives from the right-hand rule. If the index finger of the right hand is pointed forward, the middle finger bent inward at a right angle to it, and the thumb placed at a right angle to both, the three fingers indicate the relative directions of the x-, y-, and z-axes in a right-handed system. The thumb indicates the x-axis, the index finger the y-axis and the middle finger the z-axis. Conversely, if the same is done with the left hand, a left-handed system results.
Figure 7 depicts a left and a right-handed coordinate system. Because a three-dimensional object is represented on the two-dimensional screen, distortion and ambiguity result. The axis pointing downward (and to the right) is also meant to point towards the observer, whereas the "middle" axis is meant to point away from the observer. The red circle is parallel to the horizontal xy-plane and indicates rotation from the x-axis to the y-axis (in both cases). Hence the red arrow passes in front of the z-axis.
Figure 8 is another attempt at depicting a right-handed coordinate system. Again, there is an ambiguity caused by projecting the three-dimensional coordinate system into the plane. Many observers see Figure 8 as "flipping in and out" between a convex cube and a concave "corner". This corresponds to the two possible orientations of the coordinate system. Seeing the figure as convex gives a left-handed coordinate system. Thus the "correct" way to view Figure 8 is to imagine the x-axis as pointing towards the observer and thus seeing a concave corner.
== Representing a vector in the standard basis ==
A point in space in a Cartesian coordinate system may also be represented by a position vector, which can be thought of as an arrow pointing from the origin of the coordinate system to the point. If the coordinates represent spatial positions (displacements), it is common to represent the vector from the origin to the point of interest as . In two dimensions, the vector from the origin to the point with Cartesian coordinates (x, y) can be written as:
where , and are unit vectors in the direction of the x-axis and y-axis respectively, generally referred to as the standard basis (in some application areas these may also be referred to as versors). Similarly, in three dimensions, the vector from the origin to the point with Cartesian coordinates can be written as:
where is the unit vector in the direction of the z-axis.
There is no natural interpretation of multiplying vectors to obtain another vector that works in all dimensions, however there is a way to use complex numbers to provide such a multiplication. In a two dimensional cartesian plane, identify the point with coordinates (x, y) with the complex number z = x + iy. Here, i is the imaginary unit and is identified with the point with coordinates (0, 1), so it is not the unit vector in the direction of the x-axis. Since the complex numbers can be multiplied giving another complex number, this identification provides a means to "multiply" vectors. In a three dimensional cartesian space a similar identification can be made with a subset of the quaternions.
== Applications ==
Cartesian coordinates are an abstraction that have a multitude of possible applications in the real world. However, three constructive steps are involved in superimposing coordinates on a problem application. 1) Units of distance must be decided defining the spatial size represented by the numbers used as coordinates. 2) An origin must be assigned to a specific spatial location or landmark, and 3) the orientation of the axes must be defined using available directional cues for (n-1) of the n axes.
Consider as an example superimposing 3D Cartesian coordinates over all points on the Earth (i.e. geospatial 3D). What units make sense? Kilometers are a good choice, since the original definition of the kilometer was geospatial...10,000 km equalling the surface distance from Equator to North Pole. Where to place the origin? Based on symmetry, the gravitational center of the Earth suggests a natural landmark (which can be sensed via satellite orbits). Finally, how to orient X, Y and Z axis directions? The axis of Earth's spin provides a natural direction strongly associated with "up vs. down", so positive Z can adopt the direction from geocenter to North Pole. A location on the Equator is needed to define the X-axis, and the Prime Meridian stands out as a reference direction, so the X-axis takes the direction from geocenter out to [ 0 degrees longitude, 0 degrees latitude ]. Note that with 3 dimensions, and two perpendicular axes directions pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right hand rule, the Y-axis must point out from the geocenter to [ 90 degrees longitude, 0 degrees latitude ]. So what are the geocentric coordinates of the Empire State Building in New York City? Using [ longitude = &#8722; 73.985656, latitude = 40.748433 ], Earth radius = 40,000/2 &#960; , and transforming from spherical --> Cartesian coordinates, you can estimate the geocentric coordinates of the Empire State Building, [ x, y, z ] = [ 1330.53 km, &#8211; 4635.75 km, 4155.46 km ]. GPS navigation relies on such geocentric coordinates.
In engineering projects, agreement on the definition of coordinates is a crucial foundation. One cannot assume that coordinates come predefined for a novel application, so knowledge of how to erect a coordinate system where there is none is essential to applying Ren &#233; Descartes' ingenious thinking.
While spatial apps employ identical units along all axes, in business and scientific apps, each axis may have different units of measurement associated with it (such as kilograms, seconds, pounds, etc.). Although four- and higher-dimensional spaces are difficult to visualize, the algebra of Cartesian coordinates can be extended relatively easily to four or more variables, so that certain calculations involving many variables can be done. (This sort of algebraic extension is what is used to define the geometry of higher-dimensional spaces.) Conversely, it is often helpful to use the geometry of Cartesian coordinates in two or three dimensions to visualize algebraic relationships between two or three of many non-spatial variables.
The graph of a function or relation is the set of all points satisfying that function or relation. For a function of one variable, f, the set of all points (x, y), where y = f(x) is the graph of the function f. For a function g of two variables, the set of all points (x, y, z), where z = g(x, y) is the graph of the function g. A sketch of the graph of such a function or relation would consist of all the salient parts of the function or relation which would include its relative extrema, its concavity and points of inflection, any points of discontinuity and its end behavior. All of these terms are more fully defined in calculus. Such graphs are useful in calculus to understand the nature and behavior of a function or relation.
== See also ==
Horizontal and vertical
Jones diagram, which plots four variables rather than two.
Orthogonal coordinates
Polar coordinate system
Spherical coordinate system
== Notes ==
== References ==
== Sources ==
Brennan, David A.; Esplen, Matthew F.; Gray, Jeremy J. (1998), Geometry, Cambridge: Cambridge University Press, ISBN 0-521-59787-0 
Burton, David M. (2011), The History of Mathematics/An Introduction (7th ed.), New York: McGraw-Hill, ISBN 978-0-07-338315-6 
Smart, James R. (1998), Modern Geometries (5th ed.), Pacific Grove: Brooks/Cole, ISBN 0-534-35188-3 
== Further reading ==
Descartes, Ren &#233; (2001). Discourse on Method, Optics, Geometry, and Meteorology. Trans. by Paul J. Oscamp (Revised ed.). Indianapolis, IN: Hackett Publishing. ISBN 0-87220-567-3. OCLC 488633510. 
Korn GA, Korn TM (1961). Mathematical Handbook for Scientists and Engineers (1st ed.). New York: McGraw-Hill. pp. 55 &#8211; 79. LCCN 59-14456. OCLC 19959906. 
Margenau H, Murphy GM (1956). The Mathematics of Physics and Chemistry. New York: D. van Nostrand. LCCN 55-10911. 
Moon P, Spencer DE (1988). "Rectangular Coordinates (x, y, z)". Field Theory Handbook, Including Coordinate Systems, Differential Equations, and Their Solutions (corrected 2nd, 3rd print ed.). New York: Springer-Verlag. pp. 9 &#8211; 11 (Table 1.01). ISBN 978-0-387-18430-2. 
Morse PM, Feshbach H (1953). Methods of Theoretical Physics, Part I. New York: McGraw-Hill. ISBN 0-07-043316-X. LCCN 52-11515. 
Sauer R, Szab &#243; I (1967). Mathematische Hilfsmittel des Ingenieurs. New York: Springer Verlag. LCCN 67-25285. 
== External links ==
Cartesian Coordinate System
Printable Cartesian Coordinates
Cartesian coordinates at PlanetMath.org.
MathWorld description of Cartesian coordinates
Coordinate Converter &#8211; converts between polar, Cartesian and spherical coordinates
Coordinates of a point Interactive tool to explore coordinates of a point
open source JavaScript class for 2D/3D Cartesian coordinate system manipulation
PAGE	Distance	'Elementary mathematics'
Distance, or farness, is a numerical description of how far apart objects are. In physics or everyday usage, distance may refer to a physical length, or an estimation based on other criteria (e.g. "two counties over"). In mathematics, a distance function or metric is a generalization of the concept of physical distance. A metric is a function that behaves according to a specific set of rules, and is a concrete way of describing what it means for elements of some space to be "close to" or "far away from" each other. In most cases, "distance from A to B" is interchangeable with "distance between B and A".
== Mathematics ==
=== Geometry ===
In analytic geometry, the distance between two points of the xy-plane can be found using the distance formula. The distance between (x1, y1) and (x2, y2) is given by:
Similarly, given points (x1, y1, z1) and (x2, y2, z2) in three-space, the distance between them is:
These formula are easily derived by constructing a right triangle with a leg on the hypotenuse of another (with the other leg orthogonal to the plane that contains the 1st triangle) and applying the Pythagorean theorem. In the study of complicated geometries,we call this (most common) type of distance Euclidean distance,as it is derived from the Pythagorean theorem,which does not hold in Non-Euclidean geometries.This distance formula can also be expanded into the arc-length formula.
=== Distance in Euclidean space ===
In the Euclidean space Rn, the distance between two points is usually given by the Euclidean distance (2-norm distance). Other distances, based on other norms, are sometimes used instead.
For a point (x1, x2, ...,xn) and a point (y1, y2, ...,yn), the Minkowski distance of order p (p-norm distance) is defined as:
p need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold.
The 2-norm distance is the Euclidean distance, a generalization of the Pythagorean theorem to more than two coordinates. It is what would be obtained if the distance between two points were measured with a ruler: the "intuitive" idea of distance.
The 1-norm distance is more colourfully called the taxicab norm or Manhattan distance, because it is the distance a car would drive in a city laid out in square blocks (if there are no one-way streets).
The infinity norm distance is also called Chebyshev distance. In 2D, it is the minimum number of moves kings require to travel between two squares on a chessboard.
The p-norm is rarely used for values of p other than 1, 2, and infinity, but see super ellipse.
In physical space the Euclidean distance is in a way the most natural one, because in this case the length of a rigid body does not change with rotation.
=== Variational formulation of distance ===
The Euclidean distance between two points in space ( and ) may be written in a variational form where the distance is the minimum value of an integral:
Here is the trajectory (path) between the two points. The value of the integral (D) represents the length of this trajectory. The distance is the minimal value of this integral and is obtained when where is the optimal trajectory. In the familiar Euclidean case (the above integral) this optimal trajectory is simply a straight line. It is well known that the shortest path between two points is a straight line. Straight lines can formally be obtained by solving the Euler &#8211; Lagrange equations for the above functional. In non-Euclidean manifolds (curved spaces) where the nature of the space is represented by a metric the integrand has be to modified to , where Einstein summation convention has been used.
=== Generalization to higher-dimensional objects ===
The Euclidean distance between two objects may also be generalized to the case where the objects are no longer points but are higher-dimensional manifolds, such as space curves, so in addition to talking about distance between two points one can discuss concepts of distance between two strings. Since the new objects that are dealt with are extended objects (not points anymore) additional concepts such as non-extensibility, curvature constraints, and non-local interactions that enforce non-crossing become central to the notion of distance. The distance between the two manifolds is the scalar quantity that results from minimizing the generalized distance functional, which represents a transformation between the two manifolds:
The above double integral is the generalized distance functional between two plymer conformation. is a spatial parameter and is pseudo-time. This means that is the polymer/string conformation at time and is parameterized along the string length by . Similarly is the trajectory of an infinitesimal segment of the string during transformation of the entire string from conformation to conformation . The term with cofactor is a Lagrange multiplier and its role is to ensure that the length of the polymer remains the same during the transformation. If two discrete polymers are inextensible, then the minimal-distance transformation between them no longer involves purely straight-line motion, even on a Euclidean metric. There is a potential application of such generalized distance to the problem of protein folding This generalized distance is analogous to the Nambu-Goto action in string theory, however there is no exact correspondence because the Euclidean distance in 3-space is inequivalent to the space-time distance minimized for the classical relativistic string.
=== Algebraic distance ===
This is a metric often used in computer vision that can be minimized by least squares estimation. [1][2] For curves or surfaces given by the equation (such as a conic in homogeneous coordinates), the algebraic distance from the point to the curve is simply . It may serve as an "initial guess" for geometric distance to refine estimations of the curve by more accurate methods, such as non-linear least squares.
=== General case ===
In mathematics, in particular geometry, a distance function on a given set M is a function d: M &#215; M &#8594; R, where R denotes the set of real numbers, that satisfies the following conditions:
d(x,y) &#8805; 0, and d(x,y) = 0 if and only if x = y. (Distance is positive between two different points, and is zero precisely from a point to itself.)
It is symmetric: d(x,y) = d(y,x). (The distance between x and y is the same in either direction.)
It satisfies the triangle inequality: d(x,z) &#8804; d(x,y) + d(y,z). (The distance between two points is the shortest distance along any path).
Such a distance function is known as a metric. Together with the set, it makes up a metric space.
For example, the usual definition of distance between two real numbers x and y is: d(x,y) = |x &#8722; y|. This definition satisfies the three conditions above, and corresponds to the standard topology of the real line. But distance on a given set is a definitional choice. Another possible choice is to define: d(x,y) = 0 if x = y, and 1 otherwise. This also defines a metric, but gives a completely different topology, the "discrete topology"; with this definition numbers cannot be arbitrarily close.
=== Distances between sets and between a point and a set ===
Various distance definitions are possible between objects. For example, between celestial bodies one should not confuse the surface-to-surface distance and the center-to-center distance. If the former is much less than the latter, as for a LEO, the first tends to be quoted (altitude), otherwise, e.g. for the Earth-Moon distance, the latter.
There are two common definitions for the distance between two non-empty subsets of a given set:
One version of distance between two non-empty sets is the infimum of the distances between any two of their respective points, which is the every-day meaning of the word, i.e.
This is a symmetric premetric. On a collection of sets of which some touch or overlap each other, it is not "separating", because the distance between two different but touching or overlapping sets is zero. Also it is not hemimetric, i.e., the triangle inequality does not hold, except in special cases. Therefore only in special cases this distance makes a collection of sets a metric space.
The Hausdorff distance is the larger of two values, one being the supremum, for a point ranging over one set, of the infimum, for a second point ranging over the other set, of the distance between the points, and the other value being likewise defined but with the roles of the two sets swapped. This distance makes the set of non-empty compact subsets of a metric space itself a metric space.
The distance between a point and a set is the infimum of the distances between the point and those in the set. This corresponds to the distance, according to the first-mentioned definition above of the distance between sets, from the set containing only this point to the other set.
In terms of this, the definition of the Hausdorff distance can be simplified: it is the larger of two values, one being the supremum, for a point ranging over one set, of the distance between the point and the set, and the other value being likewise defined but with the roles of the two sets swapped.
=== Graph theory ===
In graph theory the distance between two vertices is the length of the shortest path between those vertices.
== Distance versus directed distance and displacement ==
Distance cannot be negative and distance travelled never decreases. Distance is a scalar quantity or a magnitude, whereas displacement is a vector quantity with both magnitude and direction. Directed distance is a positive, zero, or negative scalar quantity.
The distance covered by a vehicle (for example as recorded by an odometer), person, animal, or object along a curved path from a point A to a point B should be distinguished from the straight line distance from A to B. For example whatever the distance covered during a round trip from A to B and back to A, the displacement is zero as start and end points coincide. In general the straight line distance does not equal distance travelled, except for journeys in a straight line.
=== Directed distance ===
Directed distances are distances with a directional sense. They can be determined along straight lines and along curved lines. A directed distance of a point C from point A in the direction of B on a line AB in a Euclidean vector space is the distance from A to C if C falls on the ray AB, but is the negative of that distance if C falls on the ray BA (I.e., if C is not on the same side of A as B is).
A directed distance along a curved line is not a vector and is represented by a segment of that curved line defined by endpoints A and B, with some specific information indicating the sense (or direction) of an ideal or real motion from one endpoint of the segment to the other (see figure). For instance, just labelling the two endpoints as A and B can indicate the sense, if the ordered sequence (A, B) is assumed, which implies that A is the starting point.
=== Displacement ===
A displacement (see above) is a special kind of directed distance defined in mechanics. A directed distance is called displacement when it is the distance along a straight line (minimum distance) from A and B, and when A and B are positions occupied by the same particle at two different instants of time. This implies motion of the particle. The distance traveled by a particle must always be greater than or equal to its displacement, with equality occurring only when the particle moves along a straight path.
Another kind of directed distance is that between two different particles or point masses at a given time. For instance, the distance from the center of gravity of the Earth A and the center of gravity of the Moon B (which does not strictly imply motion from A to B) falls into this category.
== Other "distances" ==
Canberra distance
Chebyshev distance
E-statistics, or energy statistics, which are functions of distances between statistical observations
Hamming distance and Lee distance, which are used in coding theory
Kullback &#8211; Leibler distance, which measures the difference between two probability distributions
Levenshtein distance
Mahalanobis distance is used in statistics
Circular distance is the distance traveled by a wheel. The circumference of the wheel is 2 &#960; &#215; radius, and assuming the radius to be 1, then each revolution of the wheel is equivalent of the distance 2 &#960; radians. In engineering &#969; = 2 &#960; &#402; is often used, where &#402; is the frequency.
== See also ==
== References ==
Deza, E.; Deza, M. (2006), Dictionary of Distances, Elsevier, ISBN 0-444-52087-2 .
PAGE	Elementary algebra	'Elementary algebra'
Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers, algebra introduces quantities without fixed values, known as variables. This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.
The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Most quantitative results in science and mathematics are expressed as algebraic equations.
== Algebraic notation ==
Algebraic notation describes how algebra is written. It follows certain rules and conventions, and has its own terminology. For example, the expression has the following components:
1 : Exponent (power), 2 : Coefficient, 3 : term, 4 : operator, 5 : constant, : variables
A coefficient is a numerical value which multiplies a variable (the operator is omitted). A term is an addend or a summand, a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators. Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. ) are typically used to represent constants, and those toward the end of the alphabet (e.g. and ) are used to represent variables. They are usually written in italics.
Algebraic operations work in the same way as arithmetic operations, such as addition, subtraction, multiplication, division and exponentiation. and are applied to algebraic variables and terms. Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a coefficient is used. For example, is written as , and may be written .
Usually terms with the highest power (exponent), are written on the left, for example, is written to the left of . When a coefficient is one, it is usually omitted (e.g. is written ). Likewise when the exponent (power) is one, (e.g. is written ). When the exponent is zero, the result is always 1 (e.g. is always rewritten to ). However , being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.
=== Alternative notation ===
Other types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. For example, exponents are usually formatted using superscripts, e.g. . In plain text, and in the TeX mark-up language, the caret symbol "^" represents exponents, so is written as "x^2". In programming languages such as Ada, Fortran, Perl, Python and Ruby, a double asterisk is used, so is written as "x**2". Many programming languages and calculators use a single asterisk to represent the multiplication symbol, and it must be explicitly used, for example, is written "3*x".
== Concepts ==
=== Variables ===
Elementary algebra builds on and extends arithmetic by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.
Variables may represent numbers whose values are not yet known. For example, if the temperature today, T, is 20 degrees higher than the temperature yesterday, Y, then the problem can be described algebraically as .
Variables allow one to describe general problems, without specifying the values of the quantities that are involved. For example, it can be stated specifically that 5 minutes is equivalent to seconds. A more general (algebraic) description may state that the number of seconds, , where m is the number of minutes.
Variables allow one to describe mathematical relationships between quantities that may vary. For example, the relationship between the circumference, c, and diameter, d, of a circle is described by .
Variables allow one to describe some mathematical properties. For example, a basic property of addition is commutativity which states that the order of numbers being added together does not matter. Commutativity is stated algebraically as .
=== Evaluating expressions ===
Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,
Added terms are simplified using coefficients. For example can be simplified as (where 3 is the coefficient).
Multiplied terms are simplified using exponents. For example is represented as 
Like terms are added together, for example, is written as , because the terms containing are added together, and, the terms containing are added together.
Brackets can be "multiplied out", using distributivity. For example, can be written as which can be written as 
Expressions can be factored. For example, , by dividing both terms by can be written as 
=== Equations ===
An equation states that two expressions are equal using the symbol for equality, (the equals sign). One of the most well-known equations describes Pythagoras' law relating the length of the sides of a right angle triangle:
This equation states that , representing the square of the length of the side that is the hypotenuse (the side opposite the right angle), is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by and .
An equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as ); such equations are called identities. Conditional equations are true for only some values of the involved variables, e.g. is true only for and . The values of the variables which make the equation true are the solutions of the equation and can be found through equation solving.
Another type of equation is an inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are: where represents 'greater than', and where represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.
==== Properties of equality ====
By definition, equality is an equivalence relation, meaning it has the properties (a) reflexive (i.e. ), (b) symmetric (i.e. if then ) (c) transitive (i.e. if and then ). It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:
if and then and ;
if then ;
more generally, for any function , if then .
==== Properties of inequality ====
The relations less than and greater than have the property of transitivity:
If and then ;
If and then ;
If and then ;
If and then .
By reversing the inequation, and can be swapped, for example:
 is equivalent to 
=== Substitution ===
Substitution is replacing the terms in an expression to create a new expression. Substituting 3 for a in the expression a*5 makes a new expression 3*5 with meaning 15. Substituting the terms of a statement makes a new statement. When the original statement is true independent of the values of the terms, the statement created by substitutions is also true. Hence definitions can be made in symbolic terms and interpreted through substitution: if , where := means "is defined to equal", substituting 3 for informs the reader of this statement that means 3*3=9. Often it's not known whether the statement is true independent of the values of the terms, and substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement x+1=0, if x is substituted with 1, this imples 1+1=2=0, which is false, which implies that if x+1=0 then x can't be 1.
If x and y are integers, rationals, or real numbers, then xy=0 implies x=0 or y=0. Suppose abc=0. Then, substituting a for x and bc for y, we learn a=0 or bc=0. Then we can substitute again, letting x=b and y=c, to show that if bc=0 then b=0 or c=0. Therefore if abc=0, then a=0 or (b=0 or c=0), so abc=0 implies a=0 or b=0 or c=0.
Consider if the original fact were stated as "ab=0 implies a=0 or b=0." Then when we say "suppose abc=0," we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if abc=0 then a=0 or b=0 or c=0 if instead of letting a=a and b=bc we substitute a for a and b for bc (and with bc=0, substituting b for a and c for b). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression a into the a term of the original equation, the a substituted does not refer to the a in the statement "ab=0 implies a=0 or b=0."
== Solving algebraic equations ==
The following sections lay out examples of some of the types of algebraic equations that may be encountered.
=== Linear equations with one variable ===
Linear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:
Problem in words: If you double my son's age and add 4, the resulting answer is 12. How old is my son?
Equivalent equation: where represent my son's age
To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable. This problem and its solution are as follows:
In words: my son's age is 4.
The general form of a linear equation with one variable, can be written as: 
Following the same procedure (i.e. subtract from both sides, and then divide by ), the general solution is given by 
=== Linear equations with two variables ===
A linear equation with two variables has many (i.e. an infinite number of) solutions. For example:
Problem in words: I am 22 years older than my son. How old are we?
Equivalent equation: where is my age, is my son's age.
This can not be worked out by itself. If I told you my son's age, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.
To solve a linear equation with two variables (unknowns), requires two related equations. For example, if I also revealed that:
Now there are two related linear equations, each with two unknowns, which lets us produce a linear equation with just one variable, by subtracting one from the other (called the elimination method):
In other words, my son is aged 12, and as I am 22 years older, I must be 34. In 10 years time, my son will 22, and I will be twice his age, 44. This problem is illustrated on the associated plot of the equations.
For other ways to solve this kind of equations, see below, System of linear equations.
=== Quadratic equations ===
A quadratic equation is one which includes a term with an exponent of 2, for example, , and no term with higher exponent. The name derives from the Latin quadrus, meaning square. In general, a quadratic equation can be expressed in the form , where is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term , which is known as the quadratic term. Hence , and so we may divide by and rearrange the equation into the standard form
where and . Solving this, by a process known as completing the square, leads to the quadratic formula
where the symbol " &#177; " indicates that both
are solutions of the quadratic equation.
Quadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:
Which is the same thing as
It follows from the zero-product property that either or are the solutions, since precisely one of the factors must be equal to zero. All quadratic equations will have two solutions in the complex number system, but need not have any in the real number system. For example,
has no real number solution since no real number squared equals &#8722; 1. Sometimes a quadratic equation has a root of multiplicity 2, such as:
For this equation, &#8722; 1 is a root of multiplicity 2. This means &#8722; 1 appears two times.
=== Exponential and logarithmic equations ===
An exponential equation is one which has the form for , which has solution
when . Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if
then, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain
whence
or
A logarithmic equation is an equation of the form for , which has solution
For example, if
then, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get
whence
from which we obtain
=== Radical equations ===
A radical equation is one that includes a radical sign, which includes square roots, , cube roots, , and nth roots, . Recall that an nth root can be rewritten in exponential format, so that is equivalent to . Combined with regular exponents (powers), then (the square root of cubed), can be rewritten as . So a common form of a radical equation is (equivalent to ) where and are integers. It has solution:
For example, if:
then
.
=== System of linear equations ===
There are different methods to solve a system of linear equations with two variables.
==== Elimination method ====
An example of solving a system of linear equations is by using the elimination method:
Multiplying the terms in the second equation by 2:
Adding the two equations together to get:
which simplifies to
Since the fact that is known, it is then possible to deduce that by either of the original two equations (by using 2 instead of ) The full solution to this problem is then
Note that this is not the only way to solve this specific system; could have been solved before .
==== Substitution method ====
Another way of solving the same system of linear equations is by substitution.
An equivalent for can be deduced by using one of the two equations. Using the second equation:
Subtracting from each side of the equation:
and multiplying by &#8722; 1:
Using this value in the first equation in the original system:
Adding 2 on each side of the equation:
which simplifies to
Using this value in one of the equations, the same solution as in the previous method is obtained.
Note that this is not the only way to solve this specific system; in this case as well, could have been solved before .
=== Other types of systems of linear equations ===
==== Inconsistent systems ====
In the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is
As 0 &#8800; 2, the second equation in the system has no solution. Therefore, the system has no solution. However, not all inconsistent systems are recognized at first sight. As an example, let us consider the system
Multiplying by 2 both sides of the second equation, and adding it to the first one results in
which has clearly no solution.
==== Undetermined systems ====
There are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for and ) For example:
Isolating in the second equation:
And using this value in the first equation in the system:
The equality is true, but it does not provide a value for . Indeed, one can easily verify (by just filling in some values of ) that for any there is a solution as long as . There is an infinite number of solutions for this system.
==== Over- and underdetermined systems ====
Systems with more variables than the number of linear equations do not have a unique solution. An example of such a system is
Such a system is called underdetermined; when trying to solve it, one is led to express some variables as functions of the other ones, but cannot express all solutions numerically.
A system with a greater number of equations than variables, in which necessarily some equations are linear combinations of the others if any solution exists, is called overdetermined.
== See also ==
History of elementary algebra
Binary operation
Gaussian elimination
Mathematics education
Number line
Polynomial
== References ==
Leonhard Euler, Elements of Algebra, 1770. English translation Tarquin Press, 2007, ISBN 978-1-899618-79-8, also online digitized editions 2006, 1822.
Charles Smith, A Treatise on Algebra, in Cornell University Library Historical Math Monographs.
Redden, John. Elementary Algebra. Flat World Knowledge, 2011
== External links ==
PAGE	Factorization of polynomials	'Polynomials'
In mathematics and computer algebra, factorization of polynomials or polynomial factorization refers to factoring a polynomial with coefficients in a given field or in the integers into irreducible factors with coefficients in same domain. Polynomial factorization is one of the fundamental tools of the computer algebra systems.
The history of polynomial factorization starts with Hermann Schubert who in 1793 described the first polynomial factorization algorithm, and Leopold Kronecker, who rediscovered Schubert's algorithm in 1882 and extended it to multivariate polynomials and coefficients in an algebraic extension. But most of the knowledge on this topic is not older than circa 1965 and the first computer algebra systems. In a survey of the subject, Erich Kaltofen wrote in 1982 (see the bibliography, below):
When the long-known finite step algorithms were first put on computers, they turned out to be highly inefficient. The fact that almost any uni- or multivariate polynomial of degree up to 100 and with coefficients of a moderate size (up to 100 bits) can be factored by modern algorithms in a few minutes of computer time indicates how successfully this problem has been attacked during the past fifteen years.
Nowadays one can quickly factor any univariate polynomial of degree 1000, and coefficients with thousands of digits.
== Formulation of the question ==
Polynomial rings over the integers or over a field are unique factorization domains. This means that every element of these rings is a product of a constant and a product of irreducible polynomials (those that are not the product of two non-constant polynomials). Moreover, this decomposition is unique up to multiplication of the factors by invertible constants.
Factorization depends on the base field. For example, the fundamental theorem of algebra, which states that every polynomial with complex coefficients has complex roots, implies that a polynomial with integer coefficients can be factored (with root-finding algorithms) into linear factors over the complex field C. Similarly, over the field of reals, the irreducible factors have degree at most two, while there are polynomials of any degree that are irreducible over the field of rationals Q.
The question of polynomial factorization makes sense only for coefficients in a computable field whose every element may be represented in a computer and for which there are algorithms for the arithmetic operations. Fr &#246; hlich and Shepherson have provided examples of such fields for which no factorization algorithm can exist.
The fields of coefficients for which factorization algorithms are known include prime fields (i.e. the field of rationals and prime modular arithmetic) and their finitely generated field extensions. Integer coefficients are also tractable: Kronecker's method is interesting only from a historical point of view, modern algorithms proceed by a succession of:
Square-free factorization
Factorization over finite fields
and reductions:
From the multivariate case to the univariate case.
From coefficients in a purely transcendental extension to the multivariate case over the ground field (see below).
From coefficients in an algebraic extension to coefficients in the ground field (see below).
From rational coefficients to integer coefficients (see below).
From integer coefficients to coefficients in a prime field with p elements, for a well chosen p (see below).
== Primitive part &#8211; content factorization ==
In this section, we show that factoring over Q (the rational numbers) and over Z (the integers) is essentially the same problem.
The content of a polynomial p &#8712; Z[X], denoted "cont(p)", is, up to its sign, the greatest common divisor of its coefficients. The primitive part of p is primpart(p)=p/cont(p), which is a primitive polynomial with integer coefficients. This defines a factorization of p into the product of an integer and a primitive polynomial. This factorization is unique up to the sign of the content. It is a usual convention to choose the sign of the content such that the leading coefficient of the primitive part is positive.
For example,
is a factorization into content and primitive part.
Every polynomial q with rational coefficients may be written
where p &#8712; Z[X] and c &#8712; Z: it suffices to take for c a multiple of all denominators of the coefficients of q (for example their product) and p = cq. The content of q is defined as:
and the primitive part of q is that of p. As for the polynomials with integer coefficients, this defines a factorization into a rational number and a primitive polynomial with integer coefficients. This factorization is also unique up to the choice of a sign.
For example,
is a factorization into content and primitive part.
Gauss has first proved that the product of two primitive polynomials is also primitive (Gauss's lemma). This implies that a primitive polynomial is irreducible over the rationals if and only if it is irreducible over the integers. This implies also that the factorization over the rationals of a polynomial with rational coefficients is the same as the factorization over the integers of its primitive part. On the other hand, the factorization over the integers of a polynomial with integer coefficients is the product of the factorization of its primitive part by the factorization of its content.
In other words, integer GCD computation allows to reduce the factorization of a polynomial over the rationals to the factorization of a primitive polynomial with integer coefficients, and to reduce the factorization over the integers to the factorization of an integer and a primitive polynomial.
Everything that precedes remains true if Z is replaced by a polynomial ring over a field F and Q is replaced by a field of rational functions over F in the same variables, with the only difference that "up to a sign" must be replaced by "up to the multiplication by an invertible constant in F". This allows to reduce the factorization over a purely transcendental field extension of F to the factorization of multivariate polynomials over F.
== Square-free factorization ==
If two or more factors of a polynomial are identical to each other, then the polynomial is a multiple of the square of this factor. In the case of univariate polynomials, this results in multiple roots. In this case, then the multiple factor is also a factor of the polynomial's derivative (with respect to any of the variables, if several). In the case of univariate polynomials over the rationals (or more generally over a field of characteristic zero), Yun's algorithm exploits this to factorize efficiently the polynomial into factors that are not multiple of a square and are therefore called square-free. To factorize the initial polynomial, it suffices to factorize each square-free factor. Square-free factorization is therefore the first step in most polynomial factorization algorithms.
Yun's algorithm extends to the multivariate case by considering a multivariate polynomial as an univariate polynomial over a polynomial ring.
In the case of a polynomial over a finite field, Yun's algorithm applies only if the degree is smaller than the characteristic, because, otherwise, the derivative of a non zero polynomial may be zero (over the field with p elements, the derivative of a polynomial in xp is always zero). Nevertheless a succession of GCD computations, starting from the polynomial and its derivative, allows to compute the square-free decomposition; see Polynomial factorization over finite fields#Square-free factorization.
== Classical methods ==
This section describes textbook methods that can be convenient when computing by hand. These methods are not used for computer computations because they use integer factorization, which at the moment has a much higher complexity than polynomial factorization.
=== Obtaining linear factors ===
All linear factors with rational coefficients can be found using the rational root test. If the polynomial to be factored is , then all possible linear factors are of the form , where is an integer factor of and is an integer factor of . All possible combinations of integer factors can be tested for validity, and each valid one can be factored out using polynomial long division. If the original polynomial is the product of factors, at least two of which are of degree 2 or higher, this technique only provides a partial factorization; otherwise the factorization is complete. Note that in the case of a cubic polynomial, if the cubic is factorisable at all, the rational root test gives a complete factorization, either into a linear factor and an irreducible quadratic factor, or into three linear factors.
=== Kronecker's method ===
Since integer polynomials must factor into integer polynomial factors, and evaluating integer polynomials at integer values must produce integers, the integer values of a polynomial can be factored in only a finite number of ways, and produce only a finite number of possible polynomial factors.
For example, consider
.
If this polynomial factors over Z, then at least one of its factors must be of degree two or less. We need three values to uniquely fit a second degree polynomial. We'll use , and . Note that if one of those values were 0 then you already found a root (and so a factor). If none is 0, then each one has a finite amount of divisors. Now, 2 can only factor as
1 &#215; 2, 2 &#215; 1, ( &#8722; 1) &#215; ( &#8722; 2), or ( &#8722; 2) &#215; ( &#8722; 1).
Therefore, if a second degree integer polynomial factor exists, it must take one of the values
1, 2, &#8722; 1, or &#8722; 2
at , and likewise at . There are eight different ways to factor 6 (one for each divisor of 6), so there are
4 &#215; 4 &#215; 8 = 128
possible combinations, of which half can be discarded as the negatives of the other half, corresponding to 64 possible second degree integer polynomials that must be checked. These are the only possible integer polynomial factors of . Testing them exhaustively reveals that
constructed from , and , factors .
Dividing by gives the other factor , so that . Now one can test recursively to find factors of and . It turns out they both are irreducible over the integers, so that the irreducible factorization of is
(Van der Waerden, Sections 5.4 and 5.6)
== Modern methods ==
=== Factoring over finite fields ===
=== Factoring univariate polynomials over the integers ===
If is a univariate polynomial over the integers, assumed to be content-free and square-free, one starts by computing a bound such that any factor will have coefficients of absolute value bounded by . This way, if is an integer larger than , and if is known modulo , then can be reconstructed from its image mod .
The Zassenhaus algorithm proceeds as follows. First, choose a prime number such that the image of mod remains square-free, and of the same degree as . Then factor mod . This produces integer polynomials whose product matches mod . Next, apply Hensel lifting, this updates the in such a way that now their product matches mod , where is chosen in such a way that is larger than . Modulo , the polynomial has (up to units) factors: for each subset of , the product is a factor of mod . However, a factor modulo need not correspond to a so-called "true factor": a factor of in . For each factor mod , we can test if it corresponds to a "true" factor, and if so, find that "true" factor, provided that exceeds . This way, all irreducible "true" factors can be found by checking at most cases. This is reduced to cases by skipping complements. If is reducible, the number of cases is reduced further by removing those that appear in an already found "true" factor. Zassenhaus algorithm processes each case (each subset) quickly, however, in the worst case, it considers an exponential number of cases.
The first polynomial time algorithm for factoring rational polynomials has been discovered by Lenstra, Lenstra and Lov &#225; sz and is an application of Lenstra &#8211; Lenstra &#8211; Lov &#225; sz lattice basis reduction algorithm, usually called "LLL algorithm". (Lenstra, Lenstra & Lov &#225; sz 1982) A simplified version of the LLL factorization algorithm is as follows: calculate a complex (or p-adic) root &#945; of the polynomial to high precision, then use the Lenstra &#8211; Lenstra &#8211; Lov &#225; sz lattice basis reduction algorithm to find an approximate linear relation between 1, &#945; , &#945; 2, &#945; 3, ... with integer coefficients, which might be an exact linear relation and a polynomial factor of . One can determine a bound for the precision that guarantees that this method produces either a factor, or an irreducibility proof. Although this method is polynomial time, it was not used in practice because the lattice has high dimension and huge entries, which makes the computation slow.
The exponential complexity in the algorithm of Zassenhaus comes from a combinatorial problem: how to select the right subsets of . State of the art factoring implementations work in a manner similar to Zassenhaus, except that the combinatorial problem is translated to a lattice problem that is then solved by LLL. In this approach, LLL is not used to compute coefficients of factors, instead, it is used to compute vectors with entries in {0,1} that encode the subsets of that correspond to the irreducible "true" factors.
=== Factoring over algebraic extensions (Trager's method) ===
We can factor a polynomial , where is a finite field extension of . First, using square-free factorization, we may suppose that the polynomial is square-free. Next we write explicitly as an algebra over . We next pick a random element . By the primitive element theorem, generates over with high probability. If this is the case, we can compute the minimal polynomial, of over . Factoring
over , we determine that
(notice that is a reduced ring since is square-free), where corresponds to the element . Note that this is the unique decomposition of as a product fields. Hence this decomposition is the same as
where
is the factorization of over . By writing and generators of as a polynomials in , we can determine the embeddings of and into the components . By finding the minimal polynomial of in this ring, we have computed , and thus factored over 
== Bibliography ==
Fr &#246; hlich, A.; Shepherson, J. C. (1955), "On the factorisation of polynomials in a finite number of steps", Mathematische Zeitschrift 62 (1), doi:10.1007/BF01180640, ISSN 0025-5874 
Trager, B.M., "Algebraic Factoring and Rational Function Integration", Proc. SYMSAC 76 http://dl.acm.org/citation.cfm?id=806338 
Bernard Beauzamy, Per Enflo, Paul Wang (October 1994). "Quantitative Estimates for Polynomials in One or Several Variables: From Analysis and Number Theory to Symbolic and Massively Parallel Computation". Mathematics Magazine 67 (4): 243 &#8211; 257. doi:10.2307/2690843. JSTOR 2690843. (accessible to readers with undergraduate mathematics)
Cohen, Henri (1993). A course in computational algebraic number theory. Graduate Texts in Mathematics 138. Berlin, New York: Springer-Verlag. ISBN 978-3-540-55640-4. MR 1228206. 
Kaltofen, Erich (1982), "Factorization of polynomials", in B. Buchberger; R. Loos; G. Collins, Computer Algebra, Springer Verlag, CiteSeerX: 10.1.1.39.7916 
Knuth, Donald E (1997). "4.6.2 Factorization of Polynomials". Seminumerical Algorithms. The Art of Computer Programming 2 (Third ed.). Reading, Massachusetts: Addison-Wesley. pp. 439 &#8211; 461, 678 &#8211; 691. ISBN 0-201-89684-2. 
Lenstra, A. K.; Lenstra, H. W.; Lov &#225; sz, L &#225; szl &#243; (1982). "Factoring polynomials with rational coefficients". Mathematische Annalen 261 (4): 515 &#8211; 534. doi:10.1007/BF01457454. ISSN 0025-5831. MR 682664. 
Van der Waerden, Algebra (1970), trans. Blum and Schulenberger, Frederick Ungar.
== Further reading ==
Kaltofen, Erich (1990), "Polynomial Factorization 1982-1986", in D. V. Chudnovsky; R. D. Jenks, Computers in Mathematics, Lecture Notes in Pure and Applied Mathematics 125, Marcel Dekker, Inc., CiteSeerX: 10.1.1.68.7461 
Kaltofen, Erich (1992), "Polynomial Factorization 1987 &#8211; 1991", Proceedings of Latin &#8217; 92, Springer Lect. Notes Comput. Sci. 583, Springer, retrieved October 14, 2012
PAGE	Factorization	'Elementary algebra'
In mathematics, factorization (also factorisation in some forms of British English) or factoring is the decomposition of an object (for example, a number, a polynomial, or a matrix) into a product of other objects, or factors, which when multiplied together give the original. For example, the number 15 factors into primes as 3 &#215; 5, and the polynomial x2 &#8722; 4 factors as (x &#8722; 2)(x + 2). In all cases, a product of simpler objects is obtained.
The aim of factoring is usually to reduce something to &#8220; basic building blocks &#8221; , such as numbers to prime numbers, or polynomials to irreducible polynomials. Factoring integers is covered by the fundamental theorem of arithmetic and factoring polynomials by the fundamental theorem of algebra. Vi &#232; te's formulas relate the coefficients of a polynomial to its roots.
The opposite of polynomial factorization is expansion, the multiplying together of polynomial factors to an &#8220; expanded &#8221; polynomial, written as just a sum of terms.
Integer factorization for large integers appears to be a difficult problem. There is no known method to carry it out quickly. Its complexity is the basis of the assumed security of some public key cryptography algorithms, such as RSA.
A matrix can also be factorized into a product of matrices of special types, for an application in which that form is convenient. One major example of this uses an orthogonal or unitary matrix, and a triangular matrix. There are different types: QR decomposition, LQ, QL, RQ, RZ.
Another example is the factorization of a function as the composition of other functions having certain properties; for example, every function can be viewed as the composition of a surjective function with an injective function. This situation is generalized by factorization systems.
== Integers ==
By the fundamental theorem of arithmetic, every positive integer greater than 1 has a unique prime factorization. Given an algorithm for integer factorization, one can factor any integer down to its constituent primes by repeated application of this algorithm. For very large numbers, no efficient classical algorithm is known.
== Polynomials ==
Modern techniques for factoring polynomials are fast and efficient, but use sophisticated mathematical ideas (see Factorization of polynomials). These techniques are used in the construction of computer routines for carrying out polynomial factorization in Computer algebra systems. The more classical hand techniques rely on either the polynomial to be factored having low degree or the recognition of the polynomial as belonging to a certain class of known examples and are not very suitable for computer implementation. This article is concerned with these classical techniques.
While the general notion of factoring just means writing an expression as a product of simpler expressions, the vague term "simpler" will be defined more precisely for special classes of expressions. When factoring polynomials this means that the factors are to be polynomials of smaller degree. Thus, while is a factorization of the expression, it is not a polynomial factorization since the factors are not polynomials. Also, the factoring of a constant term, as in would not be considered a polynomial factorization since one of the factors does not have a smaller degree than the original expression. Another issue concerns the coefficients of the factors. In basic treatments it is desirable to have the coefficients of the factors be of the same type as the coefficients of the original polynomial, that is factoring polynomials with integer coefficients into factors with integer coefficients, or factoring polynomials with real coefficients into polynomials with real coefficients. It is not always possible to do this, and a polynomial that can not be factored in this way is said to be irreducible over this type of coefficient. Thus, x2 -2 is irreducible over the integers and x2 + 4 is irreducible over the reals. In the first example, the integers 1 and -2 can also be thought of as real numbers, and if they are, then shows that this polynomial factors over the reals (sometimes it is said that the polynomial splits over the reals). Similarly, since the integers 1 and 4 can be thought of as real and hence complex numbers, x2 + 4 splits over the complex numbers, i.e. .
The Fundamental theorem of algebra can be stated as: Every polynomial of degree n with complex number coefficients splits completely into n linear factors. Since complex roots of polynomials with real coefficients come in conjugate pairs, this result implies that every polynomial with real coefficients splits into linear and irreducible quadratic factors with real coefficients. Even though the structure of the factorization is known in these cases, finding the actual factors can be computationally challenging.
=== General methods ===
There are only a few general methods that can be applied to any polynomial in either one variable (the univariate case) or several variables (the multivariate case).
==== Highest common factor ====
Finding, by inspection, the monomial that is the highest common factor (also called the greatest common divisor) of all the terms of the polynomial and factoring it out as a common factor is an application of the distributive law. This is the most commonly used factoring technique. For example:
==== Factoring by grouping ====
A method that is sometimes useful, but not guaranteed to work, is factoring by grouping.
Factoring by grouping is done by placing the terms in the polynomial into two or more groups, where each group can be factored by a known method. The results of these partial factorizations can sometimes be combined to give a factorization of the original expression.
For example, to factor the polynomial
:
group similar terms, 
factor out the highest common factor in each grouping, 
again factor out the binomial common factor, 
While grouping may not lead to a factorization in general, if the polynomial expression to be factored consists of four terms and is the result of multiplying two binomial expressions (by the FOIL method for instance), then the grouping technique can lead to a factorization, as in the above example.
==== Using the factor theorem ====
For a univariate polynomial, p(x), the factor theorem states that a is a root of the polynomial (that is, p(a) = 0, also called a zero of the polynomial) if and only if (x - a) is a factor of p(x). The other factor in such a factorization of p(x) can be obtained by polynomial long division or synthetic division.
For example, consider the polynomial By inspection we see that 1 is a root of this polynomial (observe that the coefficients add up to 0), so (x - 1) is a factor of the polynomial. By long division we have 
==== Univariate case, using properties of the roots ====
When a univariate polynomial is completely factored into linear factors (degree one factors), all of the roots of the polynomial are visible and by multiplying the factors together again, the relationship between the roots and the coefficients can be observed. Formally, these relationships are known as Vieta's formulas. These formulas do not help in factorizing the polynomial except as a guide to making good guesses at what possible roots may be. However, if some additional information about the roots is known, this can be combined with the formulas to obtain the roots and thus the factorization.
For example, we can factor if we know that the sum of two of its roots is zero. Let and be the three roots of this polynomial. Then Vieta's formulas are:
Assuming that immediately gives and reduces the other two equations to Thus the roots are 5, 4 and -4 and we have 
===== Finding rational roots =====
If a (univariate) polynomial, f(x), has a rational root, p/q (p and q are integers and q &#8800; 0), then by the factor theorem f(x) has the factor,
If, in addition, the polynomial f(x) has integer coefficients, then q must evenly divide the integer portion of the highest common factor of the terms of the polynomial, and, in the factorization of f(x), only the factor (qx - p) will be visible.
If a (univariate) polynomial with integer coefficients, say,
has a rational root p/q, where p and q are integers that are relatively prime, then p is an integer divisor of an and q is an integer divisor of a0.
If we wished to factorize the polynomial we could look for rational roots p/q where p divides -6, q divides 2 and p and q have no common factor greater than 1. By inspection we see that this polynomial can have no negative roots. Assume that q = 2 (otherwise we would be looking for integer roots), substitute x = p/2 and set the polynomial equal to 0. By dividing by 4, we obtain the polynomial equation that will have an integer solution of 1 or 3 if the original polynomial had a rational root of the type we seek. Since 3 is a solution of this equation (and 1 is not), the original polynomial had the rational root 3/2 and the corresponding factor (2x - 3). By polynomial long division we have the factorization 
For a quadratic polynomial with integer coefficients having rational roots, the above considerations lead to a factorization technique known as the ac method of factorization. Suppose that the quadratic polynomial with integer coefficients is:
and it has rational roots, p/q and u/v. (If the discriminant, , is a square number these exist, otherwise we have irrational or complex solutions, and there will be no rational roots.) Both q and v must be divisors of a so we may write these fractions with a common denominator of a, that is, they may be written as -r/a and -s/a (the use of the negatives is cosmetic and leads to a prettier final result.) Then,
So, we have:
where rs = ac and r + s = b. The ac method for factoring the quadratic polynomial is to find r and s, the two factors of the number ac whose sum is b and then use them in the factorization formula of the original quadratic above.
As an example consider the quadratic polynomial:
Inspection of the factors of ac = 36 leads to 4 + 9 = 13 = b.
=== Recognizable patterns ===
While taking the product of two (or more) expressions can be done by following a multiplication algorithm, the reverse process of factoring relies frequently on the recognition of a pattern in the expression to be factored and recalling how such a pattern arises. The following are some well known patterns.
==== Difference of two squares ====
A common type of algebraic factoring is called the difference of two squares. It is the application of the formula
to any two terms, whether or not they are perfect squares.
This basic form is often used with more complicated expressions that may not a first look like the difference of two squares. For example,
==== Sum/difference of two cubes ====
Another formula for factoring is the sum or difference of two cubes. The sum can be factored by
and the difference by
==== Difference of two fourth powers ====
Another formula is the difference of two fourth powers, which is
==== Sum/difference of two nth powers ====
The above factorizations of differences or sums of powers can be extended to any positive integer power n.
For any n, a general factorization is:
The corresponding formula for the sum of two nth powers depends on whether n is even or odd. If n is odd, b can be replaced by &#8722; b in the above formula, to give
If n is even, we consider two cases:
1. If n is a power of 2 then is unfactorable (more precisely, irreducible over the rational numbers).
2. Otherwise, where m is odd. In this case we have,
Specifically, for some small values of n we have:
==== Binomial expansions ====
The binomial theorem supplies patterns of coefficients that permit easily recognized factorizations when the polynomial is a power of a binomial expression.
For example, the perfect square trinomials are the quadratic polynomials that can be factored as follows:
and
Some cubic polynomials are four term perfect cubes that can be factored as:
and
In general, the coefficients of the expanded polynomial are given by the n-th row of Pascal's triangle. The coefficients of have the same absolute value but alternate in sign.
==== Other factorization formulae ====
=== Using formulas ===
Any quadratic polynomial (polynomials of the form ) can be factored using the quadratic formula, as follows:
where and are the two roots of the polynomial, found with the quadratic formula.
The quadratic formula is valid for all polynomials with coefficients in any field (in particular, the real or complex numbers) except those that have characteristic two.
There are also formulas for cubic and quartic polynomials which can be used in the same way. However, there are no formulas in terms of the coefficients that exist for higher degree (univariate) polynomials by the Abel-Ruffini theorem.
=== Factoring over the complex numbers ===
==== Sum of two squares ====
If a and b represent real numbers, then the sum of their squares can be written as the product of complex numbers. This produces the factorization formula:
For example, can be factored into .
==== Sum/difference of two nth powers over the field of the algebraic numbers ====
A thorough factorization can be obtained over the field of the algebraic numbers, as showed by the following reduction formulae, which are proved going through the complex conjugate roots of .
The sum of two even powers is factored by
The difference of two even powers is factored by
The sum or difference of two odd powers is factored by
For instance, the sum or difference of two fifth powers is factored by
and the sum of two fourth powers is factored by
== Matrices ==
== Unique factorization domains ==
=== Euclidean domains ===
== See also ==
Completing the square
Monoid factorisation
Prime factor
Fermat's factorization method
Euler's factorization method
Integer factorization
Multiplicative partition
Partition (number theory) - A way of writing a number as a sum of positive integers.
Program synthesis
Table of Gaussian integer factorizations
== Notes ==
== References ==
Burnside, William Snow; Panton, Arthur William (1960) [1912], The Theory of Equations with an introduction to the theory of binary algebraic forms (Volume one), Dover 
Dickson, Leonard Eugene (1922), First Course in the Theory of Equations, New York: John Wiley & Sons 
Fite, William Benjamin (1921), College Algebra (Revised), Boston: D. C. Heath & Co. 
Klein, Felix (1925), Elementary Mathematics from an Advanced Standpoint; Arithmetic, Algebra, Analysis, Dover 
Selby, Samuel M., CRC Standard Mathematical Tables (18th ed.), The Chemical Rubber Co. 
== External links ==
Hazewinkel, Michiel, ed. (2001), "Factorization of polynomials", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
One hundred million numbers factored on html pages.
WIMS Factoris is an online factorization tool.
Wolfram Alpha can factorize too.
PAGE	Function (mathematics)	'Elementary mathematics'
In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number x to its square x2. The output of a function f corresponding to an input x is denoted by f(x) (read "f of x"). In this example, if the input is &#8722; 3, then the output is 9, and we may write f( &#8722; 3) = 9. The input variable(s) are sometimes referred to as the argument(s) of the function.
Functions of various kinds are "the central objects of investigation" in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. Others are given by a picture, called the graph of the function. In science, functions are sometimes defined by a table that gives the outputs for selected inputs. A function could be described implicitly, for example as the inverse to another function or as a solution of a differential equation.
The input and output of a function can be expressed as an ordered pair, ordered so that the first element is the input (or tuple of inputs, if the function takes more than one input), and the second is the output. In the example above, f(x) = x2, we have the ordered pair ( &#8722; 3, 9). If both input and output are real numbers, this ordered pair can be viewed as the Cartesian coordinates of a point on the graph of the function. But no picture can exactly define every point in an infinite set.
In modern mathematics, a function is defined by its set of inputs, called the domain; a set containing the set of outputs, and possibly additional elements, as members, called its codomain; and the set of all input-output pairs, called its graph. (Sometimes the codomain is called the function's "range", but warning: the word "range" is sometimes used to mean, instead, specifically the set of outputs. An unambiguous word for the latter meaning is the function's "image". To avoid ambiguity, the words "codomain" and "image" are the preferred language for their concepts.) For example, we could define a function using the rule f(x) = x2 by saying that the domain and codomain are the real numbers, and that the graph consists of all pairs of real numbers (x, x2). Collections of functions with the same domain and the same codomain are called function spaces, the properties of which are studied in such mathematical disciplines as real analysis, complex analysis, and functional analysis.
In analogy with arithmetic, it is possible to define addition, subtraction, multiplication, and division of functions, in those cases where the output is a number. Another important operation defined on functions is function composition, where the output from one function becomes the input to another function.
== Introduction and examples ==
For an example of a function, let X be the set consisting of four shapes: a red triangle, a yellow rectangle, a green hexagon, and a red square; and let Y be the set consisting of five colors: red, blue, green, pink, and yellow. Linking each shape to its color is a function from X to Y: each shape is linked to a color (i.e., an element in Y), and each shape is "linked", or "mapped", to exactly one color. There is no shape that lacks a color and no shape that has two or more colors. This function will be referred to as the "color-of-the-shape function".
The input to a function is called the argument and the output is called the value. The set of all permitted inputs to a given function is called the domain of the function, while the set of permissible outputs is called the codomain. Thus, the domain of the "color-of-the-shape function" is the set of the four shapes, and the codomain consists of the five colors. The concept of a function does not require that every possible output is the value of some argument, e.g. the color blue is not the color of any of the four shapes in X.
A second example of a function is the following: the domain is chosen to be the set of natural numbers (1, 2, 3, 4, ...), and the codomain is the set of integers (..., &#8722; 3, &#8722; 2, &#8722; 1, 0, 1, 2, 3, ...). The function associates to any natural number n the number 4 &#8722; n. For example, to 1 it associates 3 and to 10 it associates &#8722; 6.
A third example of a function has the set of polygons as domain and the set of natural numbers as codomain. The function associates a polygon with its number of vertices. For example, a triangle is associated with the number 3, a square with the number 4, and so on.
The term range is sometimes used either for the codomain or for the set of all the actual values a function has. To avoid ambiguity this article avoids using the term.
== Definition ==
In order to avoid the use of the informally defined concepts of "rules" and "associates", the above intuitive explanation of functions is completed with a formal definition. This definition relies on the notion of the Cartesian product. The Cartesian product of two sets X and Y is the set of all ordered pairs, written (x, y), where x is an element of X and y is an element of Y. The x and the y are called the components of the ordered pair. The Cartesian product of X and Y is denoted by X &#215; Y.
A function f from X to Y is a subset of the Cartesian product X &#215; Y subject to the following condition: every element of X is the first component of one and only one ordered pair in the subset. In other words, for every x in X there is exactly one element y such that the ordered pair (x, y) is contained in the subset defining the function f. This formal definition is a precise rendition of the idea that to each x is associated an element y of Y, namely the uniquely specified element y with the property just mentioned.
Considering the "color-of-the-shape" function above, the set X is the domain consisting of the four shapes, while Y is the codomain consisting of five colors. There are twenty possible ordered pairs (four shapes times five colors), one of which is
("yellow rectangle", "red").
The "color-of-the-shape" function described above consists of the set of those ordered pairs,
(shape, color)
where the color is the actual color of the given shape. Thus, the pair ("red triangle", "red") is in the function, but the pair ("yellow rectangle", "red") is not.
== Notation ==
A function f with domain X and codomain Y is commonly denoted by
or
In this context, the elements of X are called arguments of f. For each argument x, the corresponding unique y in the codomain is called the function value at x or the image of x under f. It is written as f(x). One says that f associates y with x or maps x to y. This is abbreviated by
A general function is often denoted by f. Special functions have names, for example, the signum function is denoted by sgn. Given a real number x, its image under the signum function is then written as sgn(x). Here, the argument is denoted by the symbol x, but different symbols may be used in other contexts. For example, in physics, the velocity of some body, depending on the time, is denoted v(t). The parentheses around the argument may be omitted when there is little chance of confusion, thus: sin &#8202; x; this is known as prefix notation.
In order to denote a specific function, the notation (an arrow with a bar at its tail) is used. For example, the above function reads
The first part can be read as:
"f is a function from (the set of natural numbers) to (the set of integers)" or
"f is a -valued function of an -valued variable".
The second part is read:
"x maps to 4 &#8722; x."
In other words, this function has the natural numbers as domain, the integers as codomain. Strictly speaking, a function is properly defined only when the domain and codomain are specified. For example, the formula f(x) = 4 &#8722; x alone (without specifying the codomain and domain) is not a properly defined function. Moreover, the function
(with different domain) is not considered the same function, even though the formulas defining f and g agree, and similarly with a different codomain. Despite that, many authors drop the specification of the domain and codomain, especially if these are clear from the context. So in this example many just write f(x) = 4 &#8722; x. Sometimes, the maximal possible domain is also understood implicitly: a formula such as may mean that the domain of f is the set of real numbers x where the square root is defined (in this case x &#8804; 2 or x &#8805; 3).
To define a function, sometimes a dot notation is used in order to emphasize the functional nature of an expression without assigning a special symbol to the variable. For instance, stands for the function , stands for the integral function , and so on.
== Specifying a function ==
A function can be defined by any mathematical condition relating each argument (input value) to the corresponding output value. If the domain is finite, a function f may be defined by simply tabulating all the arguments x and their corresponding function values f(x). More commonly, a function is defined by a formula, or (more generally) an algorithm &#8212; a recipe that tells how to compute the value of f(x) given any x in the domain.
There are many other ways of defining functions. Examples include piecewise definitions, induction or recursion, algebraic or analytic closure, limits, analytic continuation, infinite series, and as solutions to integral and differential equations. The lambda calculus provides a powerful and flexible syntax for defining and combining functions of several variables. In advanced mathematics, some functions exist because of an axiom, such as the Axiom of Choice.
=== Graph ===
The graph of a function is its set of ordered pairs F. This is an abstraction of the idea of a graph as a picture showing the function plotted on a pair of coordinate axes; for example, (3, &#8201; 9), the point above 3 on the horizontal axis and to the right of 9 on the vertical axis, lies on the graph of y=x2.
=== Formulas and algorithms ===
Different formulas or algorithms may describe the same function. For instance f(x) = (x &#8201; + &#8201; 1) &#8201; (x &#8201; &#8722; &#8201; 1) is exactly the same function as f(x) = x2 &#8201; &#8722; &#8201; 1. Furthermore, a function need not be described by a formula, expression, or algorithm, nor need it deal with numbers at all: the domain and codomain of a function may be arbitrary sets. One example of a function that acts on non-numeric inputs takes English words as inputs and returns the first letter of the input word as output.
As an example, the factorial function is defined on the nonnegative integers and produces a nonnegative integer. It is defined by the following inductive algorithm: 0! is defined to be 1, and n! is defined to be for all positive integers n. The factorial function is denoted with the exclamation mark (serving as the symbol of the function) after the variable (postfix notation).
=== Computability ===
Functions that send integers to integers, or finite strings to finite strings, can sometimes be defined by an algorithm, which gives a precise description of a set of steps for computing the output of the function from its input. Functions definable by an algorithm are called computable functions. For example, the Euclidean algorithm gives a precise process to compute the greatest common divisor of two positive integers. Many of the functions studied in the context of number theory are computable.
Fundamental results of computability theory show that there are functions that can be precisely defined but are not computable. Moreover, in the sense of cardinality, almost all functions from the integers to integers are not computable. The number of computable functions from integers to integers is countable, because the number of possible algorithms is. The number of all functions from integers to integers is higher: the same as the cardinality of the real numbers. Thus most functions from integers to integers are not computable. Specific examples of uncomputable functions are known, including the busy beaver function and functions related to the halting problem and other undecidable problems.
== Basic properties ==
There are a number of general basic properties and notions. In this section, f is a function with domain X and codomain Y.
=== Image and preimage ===
If A is any subset of the domain X, then f(A) is the subset of the codomain Y consisting of all images of elements of A. We say the f(A) is the image of A under f. The image of f is given by f(X). On the other hand, the inverse image (or preimage, complete inverse image) of a subset B of the codomain Y under a function f is the subset of the domain X defined by
So, for example, the preimage of {4, 9} under the squaring function is the set { &#8722; 3, &#8722; 2,2,3}. The term range usually refers to the image, but sometimes it refers to the codomain.
By definition of a function, the image of an element x of the domain is always a single element y of the codomain. Conversely, though, the preimage of a singleton set (a set with exactly one element) may in general contain any number of elements. For example, if f(x) = 7 (the constant function taking value 7), then the preimage of {5} is the empty set but the preimage of {7} is the entire domain. It is customary to write f &#8722; 1(b) instead of f &#8722; 1({b}), i.e.
This set is sometimes called the fiber of b under f.
Use of f(A) to denote the image of a subset A &#8838; X is consistent so long as no subset of the domain is also an element of the domain. In some fields (e.g., in set theory, where ordinals are also sets of ordinals) it is convenient or even necessary to distinguish the two concepts; the customary notation is f[A] for the set { f(x): x &#8712; A }. Likewise, some authors use square brackets to avoid confusion between the inverse image and the inverse function. Thus they would write f &#8722; 1[B] and f &#8722; 1[b] for the preimage of a set and a singleton.
=== Injective and surjective functions ===
A function is called injective (or one-to-one, or an injection) if f(a) &#8800; f(b) for any two different elements a and b of the domain. It is called surjective (or onto) if f(X) = Y. That is, it is surjective if for every element y in the codomain there is an x in the domain such that f(x) = y. Finally f is called bijective if it is both injective and surjective. This nomenclature was introduced by the Bourbaki group.
The above "color-of-the-shape" function is not injective, since two distinct shapes (the red triangle and the red rectangle) are assigned the same value. Moreover, it is not surjective, since the image of the function contains only three, but not all five colors in the codomain.
=== Function composition ===
The function composition of two functions takes the output of one function as the input of a second one. More specifically, the composition of f with a function g: Y &#8594; Z is the function defined by
That is, the value of x is obtained by first applying f to x to obtain y = f(x) and then applying g to y to obtain z = g(y). In the notation , the function on the right, f, acts first and the function on the left, g acts second, reversing English reading order. The notation can be memorized by reading the notation as "g of f" or "g after f". The composition is only defined when the codomain of f is the domain of g. Assuming that, the composition in the opposite order need not be defined. Even if it is, i.e., if the codomain of f is the codomain of g, it is not in general true that
That is, the order of the composition is important. For example, suppose f(x) = x2 and g(x) = x+1. Then g(f(x)) = x2+1, while f(g(x)) = (x+1)2, which is x2+2x+1, a different function.
=== Identity function ===
The unique function over a set X that maps each element to itself is called the identity function for X, and typically denoted by idX. Each set has its own identity function, so the subscript cannot be omitted unless the set can be inferred from context. Under composition, an identity function is "neutral": if f is any function from X to Y, then
=== Restrictions and extensions ===
Informally, a restriction of a function f is the result of trimming its domain. More precisely, if S is any subset of X, the restriction of f to S is the function f|S from S to Y such that f|S(s) = f(s) for all s in S. If g is a restriction of f, then it is said that f is an extension of g.
The overriding of f: X &#8594; Y by g: W &#8594; Y (also called overriding union) is an extension of g denoted as (f &#8853; g): (X &#8746; W) &#8594; Y. Its graph is the set-theoretical union of the graphs of g and f|X \ W. Thus, it relates any element of the domain of g to its image under g, and any other element of the domain of f to its image under f. Overriding is an associative operation; it has the empty function as an identity element. If f|X &#8745; W and g|X &#8745; W are pointwise equal (e.g., the domains of f and g are disjoint), then the union of f and g is defined and is equal to their overriding union. This definition agrees with the definition of union for binary relations.
=== Inverse function ===
An inverse function for f, denoted by f &#8722; 1, is a function in the opposite direction, from Y to X, satisfying
That is, the two possible compositions of f and f &#8722; 1 need to be the respective identity maps of X and Y.
As a simple example, if f converts a temperature in degrees Celsius C to degrees Fahrenheit F, the function converting degrees Fahrenheit to degrees Celsius would be a suitable f &#8722; 1.
Such an inverse function exists if and only if f is bijective. In this case, f is called invertible. The notation (or, in some texts, just ) and f &#8722; 1 are akin to multiplication and reciprocal notation. With this analogy, identity functions are like the multiplicative identity, 1, and inverse functions are like reciprocals (hence the notation).
== Types of functions ==
=== Real-valued functions ===
A real-valued function f is one whose codomain is the set of real numbers or a subset thereof. If, in addition, the domain is also a subset of the reals, f is a real valued function of a real variable. The study of such functions is called real analysis.
Real-valued functions enjoy so-called pointwise operations. That is, given two functions
f, g: X &#8594; Y
where Y is a subset of the reals (and X is an arbitrary set), their (pointwise) sum f+g and product f &#8901; g are functions with the same domain and codomain. They are defined by the formulas:
In a similar vein, complex analysis studies functions whose domain and codomain are both the set of complex numbers. In most situations, the domain and codomain are understood from context, and only the relationship between the input and output is given, but if , then in real variables the domain is limited to non-negative numbers.
The following table contains a few particularly important types of real-valued functions:
=== Further types of functions ===
There are many other special classes of functions that are important to particular branches of mathematics, or particular applications. Here is a partial list:
== Function spaces ==
The set of all functions from a set X to a set Y is denoted by X &#8594; Y, by [X &#8594; Y], or by YX. The latter notation is motivated by the fact that, when X and Y are finite and of size |X| and |Y|, then the number of functions X &#8594; Y is |YX| = |Y||X|. This is an example of the convention from enumerative combinatorics that provides notations for sets based on their cardinalities. If X is infinite and there is more than one element in Y then there are uncountably many functions from X to Y, though only countably many of them can be expressed with a formula or algorithm.
=== Currying ===
An alternative approach to handling functions with multiple arguments is to transform them into a chain of functions that each takes a single argument. For instance, one can interpret Add(3,5) to mean "first produce a function that adds 3 to its argument, and then apply the 'Add 3' function to 5". This transformation is called currying: Add 3 is curry(Add) applied to 3. There is a bijection between the function spaces CA &#215; B and (CB)A.
When working with curried functions it is customary to use prefix notation with function application considered left-associative, since juxtaposition of multiple arguments &#8212; as in (f x y) &#8212; naturally maps to evaluation of a curried function. Conversely, the &#8594; and &#10236; symbols are considered to be right-associative, so that curried functions may be defined by a notation such as f: Z &#8594; Z &#8594; Z = x &#10236; y &#10236; x &#183; y.
== Variants and generalizations ==
=== Alternative definition of a function ===
The above definition of "a function from X to Y" is generally agreed on, however there are two different ways a "function" is normally defined where the domain X and codomain Y are not explicitly or implicitly specified. Usually this is not a problem as the domain and codomain normally will be known. With one definition saying the function defined by f(x) = x2 on the reals does not completely specify a function as the codomain is not specified, and in the other it is a valid definition.
In the other definition a function is defined as a set of ordered pairs where each first element only occurs once. The domain is the set of all the first elements of a pair and there is no explicit codomain separate from the image. Concepts like surjective have to be refined for such functions, more specifically by saying that a (given) function is surjective on a (given) set if its image equals that set. For example, we might say a function f is surjective on the set of real numbers.
If a function is defined as a set of ordered pairs with no specific codomain, then f: &#8201; X &#8201; &#8594; &#8201; Y indicates that f is a function whose domain is X and whose image is a subset of Y. This is the case in the ISO standard. Y may be referred to as the codomain but then any set including the image of f is a valid codomain of f. This is also referred to by saying that "f maps X into Y" In some usages X and Y may subset the ordered pairs, e.g. the function f on the real numbers such that y=x2 when used as in f: &#8201; [0,4] &#8201; &#8594; &#8201; [0,4] means the function defined only on the interval [0,2]. With the definition of a function as an ordered triple this would always be considered a partial function.
An alternative definition of the composite function g(f(x)) defines it for the set of all x in the domain of f such that f(x) is in the domain of g. Thus the real square root of &#8722; x2 is a function only defined at 0 where it has the value 0.
Functions are commonly defined as a type of relation. A relation from X to Y is a set of ordered pairs (x, &#8201; y) with x &#8712; X and y &#8712; Y. A function from X to Y can be described as a relation from X to Y that is left-total and right-unique. However when X and Y are not specified there is a disagreement about the definition of a relation that parallels that for functions. Normally a relation is just defined as a set of ordered pairs and a correspondence is defined as a triple (X, &#8201; Y, &#8201; F), however the distinction between the two is often blurred or a relation is never referred to without specifying the two sets. The definition of a function as a triple defines a function as a type of correspondence, whereas the definition of a function as a set of ordered pairs defines a function as a type of relation.
Many operations in set theory, such as the power set, have the class of all sets as their domain, and therefore, although they are informally described as functions, they do not fit the set-theoretical definition outlined above, because a class is not necessarily a set. However some definitions of relations and functions define them as classes of pairs rather than sets of pairs and therefore do include the power set as a function.
=== Partial and multi-valued functions ===
In some parts of mathematics, including recursion theory and functional analysis, it is convenient to study partial functions in which some values of the domain have no association in the graph; i.e., single-valued relations. For example, the function f such that f(x) = 1/x does not define a value for x = 0, since division by zero is not defined. Hence f is only a partial function from the real line to the real line. The term total function can be used to stress the fact that every element of the domain does appear as the first element of an ordered pair in the graph. In other parts of mathematics, non-single-valued relations are similarly conflated with functions: these are called multivalued functions, with the corresponding term single-valued function for ordinary functions.
=== Functions with multiple inputs and outputs ===
The concept of function can be extended to an object that takes a combination of two (or more) argument values to a single result. This intuitive concept is formalized by a function whose domain is the Cartesian product of two or more sets.
For example, consider the function that associates two integers to their product: f(x, y) = x &#183; y. This function can be defined formally as having domain Z &#215; Z, the set of all integer pairs; codomain Z; and, for graph, the set of all pairs ((x,y), x &#183; y). Note that the first component of any such pair is itself a pair (of integers), while the second component is a single integer.
The function value of the pair (x,y) is f((x,y)). However, it is customary to drop one set of parentheses and consider f(x,y) a function of two variables, x and y. Functions of two variables may be plotted on the three-dimensional Cartesian as ordered triples of the form (x,y,f(x,y)).
The concept can still further be extended by considering a function that also produces output that is expressed as several variables. For example, consider the integer divide function, with domain Z &#215; N and codomain Z &#215; N. The resultant (quotient, remainder) pair is a single value in the codomain seen as a Cartesian product.
==== Binary operations ====
The familiar binary operations of arithmetic, addition and multiplication, can be viewed as functions from R &#215; R to R. This view is generalized in abstract algebra, where n-ary functions are used to model the operations of arbitrary algebraic structures. For example, an abstract group is defined as a set X and a function f from X &#215; X to X that satisfies certain properties.
Traditionally, addition and multiplication are written in the infix notation: x+y and x &#215; y instead of +(x, y) and &#215; (x, y).
=== Functors ===
The idea of structure-preserving functions, or homomorphisms, led to the abstract notion of morphism, the key concept of category theory. In fact, functions f: X &#8594; Y are the morphisms in the category of sets, including the empty set: if the domain X is the empty set, then the subset of X &#215; Y describing the function is necessarily empty, too. However, this is still a well-defined function. Such a function is called an empty function. In particular, the identity function of the empty set is defined, a requirement for sets to form a category.
The concept of categorification is an attempt to replace set-theoretic notions by category-theoretic ones. In particular, according to this idea, sets are replaced by categories, while functions between sets are replaced by functors.
== History ==
== See also ==
== Notes ==
== References ==
Bartle, Robert (1967). The Elements of Real Analysis. John Wiley & Sons. 
Bloch, Ethan D. (2011). Proofs and Fundamentals: A First Course in Abstract Mathematics. Springer. ISBN 978-1-4419-7126-5. 
Halmos, Paul R. (1970). Naive Set Theory. Springer-Verlag. ISBN 0-387-90092-6. 
Spivak, Michael (2008). Calculus (4th ed.). Publish or Perish. ISBN 978-0-914098-91-1. 
== Further reading ==
Anton, Howard (1980). Calculus with Analytical Geometry. Wiley. ISBN 978-0-471-03248-9. 
Bartle, Robert G. (1976). The Elements of Real Analysis (2nd ed.). Wiley. ISBN 978-0-471-05464-1. 
Dubinsky, Ed; Harel, Guershon (1992). The Concept of Function: Aspects of Epistemology and Pedagogy. Mathematical Association of America. ISBN 0-88385-081-8. 
Hammack, Richard (2009). "12. Functions". Book of Proof. Virginia Commonwealth University. Retrieved 2012-08-01. 
Husch, Lawrence S. (2001). Visual Calculus. University of Tennessee. Retrieved 2007-09-27. 
Katz, Robert (1964). Axiomatic Analysis. D. C. Heath and Company. 
Kleiner, Israel (1989). Evolution of the Function Concept: A Brief Survey. The College Mathematics Journal 20 (4) (Mathematical Association of America). pp. 282 &#8211; 300. doi:10.2307/2686848. JSTOR 2686848. 
L &#252; tzen, Jesper (2003). "Between rigor and applications: Developments in the concept of function in mathematical analysis". In Roy Porter, ed. The Cambridge History of Science: The modern physical and mathematical sciences. Cambridge University Press. ISBN 0521571995. An approachable and diverting historical presentation.
Malik, M. A. (1980). Historical and pedagogical aspects of the definition of function. International Journal of Mathematical Education in Science and Technology 11 (4). pp. 489 &#8211; 492. doi:10.1080/0020739800110404. 
Reichenbach, Hans (1947) Elements of Symbolic Logic, Dover Publishing Inc., New York NY, ISBN 0-486-24004-5.
Ruthing, D. (1984). Some definitions of the concept of function from Bernoulli, Joh. to Bourbaki, N. Mathematical Intelligencer 6 (4). pp. 72 &#8211; 77. 
Thomas, George B.; Finney, Ross L. (1995). Calculus and Analytic Geometry (9th ed.). Addison-Wesley. ISBN 978-0-201-53174-9. 
== External links ==
Khan Academy: Functions, free online micro lectures
Hazewinkel, Michiel, ed. (2001), "Function", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Function", MathWorld.
The Wolfram Functions Site gives formulae and visualizations of many mathematical functions.
Shodor: Function Flyer, interactive Java applet for graphing and exploring functions.
xFunctions, a Java applet for exploring functions graphically.
Draw Function Graphs, online drawing program for mathematical functions.
Functions from cut-the-knot.
Function at ProvenMath.
Comprehensive web-based function graphing & evaluation tool.
Abstractmath.org articles on functions
PAGE	Hypotenuse	'Elementary geometry'
In geometry, a hypotenuse (alternate spelling: hypothenuse) is the longest side of a right-angled triangle, the side opposite of the right angle. The length of the hypotenuse of a right triangle can be found using the Pythagorean theorem, which states that the square of the length of the hypotenuse equals the sum of the squares of the lengths of the other two sides. For example, if one of the other sides has a length of 3 (when squared, 9) and the other has a length of 4 (when squared, 16), then their squares add up to 25. The length of the hypotenuse is the square root of 25, that is,5.
== Etymology ==
The word hypotenuse means essentially "length under", and derives from Latin hypot &#275; n &#363; sa, a transliteration of Ancient Greek hypote &#237; nousa (pleur &#257; &#769; or gramm &#7703; ), the feminine present participle of hypote &#237; n &#333; , a combination of hyp &#243; ("under") and te &#237; n &#333; ("I stretch" or "length"). The word &#8017; &#960; &#959; &#964; &#949; &#943; &#957; &#959; &#965; &#963; &#945; was used for the hypotenuse of a triangle by Plato in the Timaeus (dialogue) 54d and by many other ancient authors.
A folk etymology says that tenuse means "side", so hypotenuse means a support like a prop or buttress, but this is inaccurate.
== Calculating the hypotenuse ==
The length of the hypotenuse is calculated using the square root function implied by the Pythagorean theorem. Using the common notation that the length of the two legs of the triangle (the sides perpendicular to each other) are a and b and that of the hypotenuse is c, we have
The Pythagorean theorem, and hence this length, can also be derived from the law of cosines by observing that the angle opposite the hypotenuse is 90 &#176; and noting that its cosine is 0:
Many computer languages support the ISO C standard function hypot(x,y), which returns the value above. The function is designed not to fail where the straightforward calculation might overflow or underflow and can be slightly more accurate.
Some scientific calculators provide a function to convert from rectangular coordinates to polar coordinates. This gives both the length of the hypotenuse and the angle the hypotenuse makes with the base line (c1 above) at the same time when given x and y. The angle returned will normally be that given by atan2(y,x).
== Properties ==
Orthographic projections:
The length of the hypotenuse equals the sum of the lengths of the orthographic projections of both catheti. And
The square of the length of a cathetus equals the product of the lengths of its orthographic projection on the hypotenuse times the length of this.
b &#178; = a &#183; m
c &#178; = a &#183; n
Also, the length of a cathetus b is the proportional mean between the lengths of its projection m and the hypotenuse a.
a/b = b/m
a/c = c/n
== Trigonometric ratios ==
By means of trigonometric ratios, one can obtain the value of two acute angles, and , of the right triangle.
Given the length of the hypotenuse and of a cathetus , the ratio is:
The trigonometric inverse function is:
in which is the angle opposite the cathetus .
The adjacent angle of the catheti , will be = 90 &#176; &#8211; 
One may also obtain the value of the angle by the equation:
in which is the other cathetus.
== See also ==
Cathetus
Triangle
Space diagonal
Nonhypotenuse number
Taxicab geometry
Trigonometry
Special right triangles
Pythagoras
== Notes ==
== References ==
Hypotenuse at Encyclopaedia of Mathematics
Weisstein, Eric W., "Hypotenuse", MathWorld.
PAGE	Inequality (mathematics)	'Elementary algebra'
Not to be confused with Inequation. "Less than", "Greater than", and "More than" redirect here. For the use of the "<" and ">" signs as punctuation, see Bracket. For the UK insurance brand "More Th>n", see RSA Insurance Group.
In mathematics, an inequality is a relation that holds between two values when they are different (see also: equality).
The notation a &#8800; b means that a is not equal to b.
It does not say that one is greater than the other, or even that they can be compared in size.
If the values in question are elements of an ordered set, such as the integers or the real numbers, they can be compared in size.
The notation a < b means that a is less than b.
The notation a > b means that a is greater than b.
In either case, a is not equal to b. These relations are known as strict inequalities. The notation a < b may also be read as "a is strictly less than b".
In contrast to strict inequalities, there are two types of inequality relations that are not strict:
The notation a &#8804; b means that a is less than or equal to b (or, equivalently, not greater than b, or at most b: notation, a greater than sign bisected by a vertical line).
The notation a &#8805; b means that a is greater than or equal to b (or, equivalently, not less than b, or at least b: notation, a less than sign bisected by a vertical line).
An additional use of the notation is to show that one quantity is much greater than another, normally by several orders of magnitude.
The notation a &#8810; b means that a is much less than b. (In measure theory, however, this notation is used for absolute continuity, an unrelated concept.)
The notation a &#8811; b means that a is much greater than b.
== Properties ==
Inequalities are governed by the following properties. All of these properties also hold if all of the non-strict inequalities ( &#8804; and &#8805; ) are replaced by their corresponding strict inequalities (< and >) and (in the case of applying a function) monotonic functions are limited to strictly monotonic functions.
=== Transitivity ===
The Transitive property of inequality states:
For any real numbers a, b, c:
If a &#8805; b and b &#8805; c, then a &#8805; c.
If a &#8804; b and b &#8804; c, then a &#8804; c.
If either of the premises is a strict inequality, then the conclusion is a strict inequality.
E.g. if a &#8805; b and b > c, then a > c
An equality is of course a special case of a non-strict inequality.
E.g. if a = b and b > c, then a > c
=== Converse ===
The relations &#8804; and &#8805; are each other's converse:
For any real numbers a and b:
If a &#8804; b, then b &#8805; a.
If a &#8805; b, then b &#8804; a.
=== Addition and subtraction ===
A common constant c may be added to or subtracted from both sides of an inequality:
For any real numbers a, b, c
If a &#8804; b, then a + c &#8804; b + c and a &#8722; c &#8804; b &#8722; c.
If a &#8805; b, then a + c &#8805; b + c and a &#8722; c &#8805; b &#8722; c.
i.e., the real numbers are an ordered group under addition.
=== Multiplication and division ===
The properties that deal with multiplication and division state:
For any real numbers, a, b and non-zero c:
If c is positive, then multiplying or dividing by c does not change the inequality:
If a &#8805; b and c > 0, then ac &#8805; bc and a/c &#8805; b/c.
If a &#8804; b and c > 0, then ac &#8804; bc and a/c &#8804; b/c.
If c is negative, then multiplying or dividing by c inverts the inequality:
If a &#8805; b and c < 0, then ac &#8804; bc and a/c &#8804; b/c.
If a &#8804; b and c < 0, then ac &#8805; bc and a/c &#8805; b/c.
More generally, this applies for an ordered field, see below.
=== Additive inverse ===
The properties for the additive inverse state:
For any real numbers a and b, negation inverts the inequality:
If a &#8804; b, then &#8722; a &#8805; &#8722; b.
If a &#8805; b, then &#8722; a &#8804; &#8722; b.
=== Multiplicative inverse ===
The properties for the multiplicative inverse state:
For any non-zero real numbers a and b that are both positive or both negative:
If a &#8804; b, then 1/a &#8805; 1/b.
If a &#8805; b, then 1/a &#8804; 1/b.
If one of a and b is positive and the other is negative, then:
If a < b, then 1/a < 1/b.
If a > b, then 1/a > 1/b.
These can also be written in chained notation as:
For any non-zero real numbers a and b:
If 0 < a &#8804; b, then 1/a &#8805; 1/b > 0.
If a &#8804; b < 0, then 0 > 1/a &#8805; 1/b.
If a < 0 < b, then 1/a < 0 < 1/b.
If 0 > a &#8805; b, then 1/a &#8804; 1/b < 0.
If a &#8805; b > 0, then 0 < 1/a &#8804; 1/b.
If a > 0 > b, then 1/a > 0 > 1/b.
=== Applying a function to both sides ===
Any monotonically increasing function may be applied to both sides of an inequality (provided they are in the domain of that function) and it will still hold. Applying a monotonically decreasing function to both sides of an inequality means the opposite inequality now holds. The rules for additive and multiplicative inverses are both examples of applying a monotonically decreasing function.
If the inequality is strict (a < b, a > b) and the function is strictly monotonic, then the inequality remains strict. If only one of these conditions is strict, then the resultant inequality is non-strict. The rules for additive and multiplicative inverses are both examples of applying a strictly monotonically decreasing function.
As an example, consider the application of the natural logarithm to both sides of an inequality when a and b are positive real numbers:
a &#8804; b &#8660; ln(a) &#8804; ln(b).
a < b &#8660; ln(a) < ln(b).
This is true because the natural logarithm is a strictly increasing function.
== Ordered fields ==
If (F, +, &#215; ) is a field and &#8804; is a total order on F, then (F, +, &#215; , &#8804; ) is called an ordered field if and only if:
a &#8804; b implies a + c &#8804; b + c;
0 &#8804; a and 0 &#8804; b implies 0 &#8804; a &#215; b.
Note that both (Q, +, &#215; , &#8804; ) and (R, +, &#215; , &#8804; ) are ordered fields, but &#8804; cannot be defined in order to make (C, +, &#215; , &#8804; ) an ordered field, because &#8722; 1 is the square of i and would therefore be positive.
The non-strict inequalities &#8804; and &#8805; on real numbers are total orders. The strict inequalities < and > on real numbers are strict total orders.
== Chained notation ==
The notation a < b < c stands for "a < b and b < c", from which, by the transitivity property above, it also follows that a < c. Obviously, by the above laws, one can add/subtract the same number to all three terms, or multiply/divide all three terms by same nonzero number and reverse all inequalities according to sign. Hence, for example, a < b + e < c is equivalent to a &#8722; e < b < c &#8722; e.
This notation can be generalized to any number of terms: for instance, a1 &#8804; a2 &#8804; ... &#8804; an means that ai &#8804; ai+1 for i = 1, 2, ..., n &#8722; 1. By transitivity, this condition is equivalent to ai &#8804; aj for any 1 &#8804; i &#8804; j &#8804; n.
When solving inequalities using chained notation, it is possible and sometimes necessary to evaluate the terms independently. For instance to solve the inequality 4x < 2x + 1 &#8804; 3x + 2, it is not possible to isolate x in any one part of the inequality through addition or subtraction. Instead, the inequalities must be solved independently, yielding x < 1/2 and x &#8805; &#8722; 1 respectively, which can be combined into the final solution &#8722; 1 &#8804; x < 1/2.
Occasionally, chained notation is used with inequalities in different directions, in which case the meaning is the logical conjunction of the inequalities between adjacent terms. For instance, a < b = c &#8804; d means that a < b, b = c, and c &#8804; d. This notation exists in a few programming languages such as Python.
== Inequalities between means ==
There are many inequalities between means. For example, for any positive numbers a1, a2, &#8230; , an we have H &#8804; G &#8804; A &#8804; Q, where
== Power inequalities ==
A "Power inequality" is an inequality containing ab terms, where a and b are real positive numbers or variable expressions. They often appear in mathematical olympiads exercises.
=== Examples ===
For any real x,
If x > 0, then
If x &#8805; 1, then
If x, y, z > 0, then
For any real distinct numbers a and b,
If x, y > 0 and 0 < p < 1, then
If x, y, z > 0, then
If a, b > 0, then
This inequality was solved by I.Ilani in JSTOR,AMM,Vol.97,No.1,1990.
If a, b > 0, then
This inequality was solved by S.Manyama in AJMAA,Vol.7,Issue 2,No.1,2010 and by V.Cirtoaje in JNSA,Vol.4,Issue 2,130-137,2011.
If a, b, c > 0, then
If a, b > 0, then
This result was generalized by R. Ozols in 2002 who proved that if a1, ..., an > 0, then
(result is published in Latvian popular-scientific quarterly The Starry Sky, see references).
== Well-known inequalities ==
Mathematicians often use inequalities to bound quantities for which exact formulas cannot be computed easily. Some inequalities are used so often that they have names:
== Complex numbers and inequalities ==
The set of complex numbers with its operations of addition and multiplication is a field, but it is impossible to define any relation &#8804; so that becomes an ordered field. To make an ordered field, it would have to satisfy the following two properties:
if a &#8804; b then a + c &#8804; b + c
if 0 &#8804; a and 0 &#8804; b then 0 &#8804; a b
Because &#8804; is a total order, for any number a, either 0 &#8804; a or a &#8804; 0 (in which case the first property above implies that 0 &#8804; ). In either case 0 &#8804; a2; this means that and ; so and , which means ; contradiction.
However, an operation &#8804; can be defined so as to satisfy only the first property (namely, "if a &#8804; b then a + c &#8804; b + c"). Sometimes the lexicographical order definition is used:
a &#8804; b if < or ( and &#8804; )
It can easily be proven that for this definition a &#8804; b implies a + c &#8804; b + c.
== Vector inequalities ==
Inequality relationships similar to those defined above can also be defined for column vector. If we let the vectors (meaning that and where and are real numbers for ), we can define the following relationships.
 if for 
 if for 
 if for and 
 if for 
Similarly, we can define relationships for , , and . We note that this notation is consistent with that used by Matthias Ehrgott in Multicriteria Optimization (see References).
The property of Trichotomy (as stated above) is not valid for vector relationships. For example, when and , there exists no valid inequality relationship between these two vectors. Also, a multiplicative inverse would need to be defined on a vector before this property could be considered. However, for the rest of the aforementioned properties, a parallel property for vector inequalities exists.
== General Existence Theorems ==
For a general system of polynomial inequalities, one can find a condition for a solution to exist. Firstly, any system of polynomial inequalities can be reduced to a system of quadratic inequalities by increasing the number of variables and equations (for example by setting a square of a variable equal to a new variable). A single quadratic polynomial inequality in n-1 variables can be written as:
where X is a vector of the variables and A is a matrix. This has a solution, for example, when there is at least one positive element on the main diagonal of A.
Systems of inequalities can be written in terms of matrices A, B, C, etc. and the conditions for existence of solutions can be written as complicated expressions in terms of these matrices. The solution for two polynomial inequalities in two variables tells us whether two conic section regions overlap or are inside each other. The general solution is not known but such a solution could be theoretically used to solve such unsolved problems as the kissing number problem. However, the conditions would be so complicated as to require a great deal of computing time or clever algorithms.
== See also ==
Binary relation
Bracket (mathematics), for the use of similar &#8249; and &#8250; signs as brackets
Fourier-Motzkin elimination
Inclusion (set theory)
Inequation
Interval (mathematics)
List of inequalities
List of triangle inequalities
Partially ordered set
Relational operators, used in programming languages to denote inequality
== Notes ==
== References ==
Hardy, G., Littlewood J.E., P &#243; lya, G. (1999). Inequalities. Cambridge Mathematical Library, Cambridge University Press. ISBN 0-521-05206-8. 
Beckenbach, E.F., Bellman, R. (1975). An Introduction to Inequalities. Random House Inc. ISBN 0-394-01559-2. 
Drachman, Byron C., Cloud, Michael J. (1998). Inequalities: With Applications to Engineering. Springer-Verlag. ISBN 0-387-98404-6. 
Murray S. Klamkin. ""Quickie" inequalities" (PDF). Math Strategies. 
Arthur Lohwater (1982). "Introduction to Inequalities". Online e-book in PDF format. 
Harold Shapiro (2005,1972 &#8211; 1985). "Mathematical Problem Solving". The Old Problem Seminar. Kungliga Tekniska h &#246; gskolan. 
"3rd USAMO". Archived from the original on 2008-02-03. 
Pachpatte, B.G. (2005). Mathematical Inequalities. North-Holland Mathematical Library 67 (first ed.). Amsterdam, The Netherlands: Elsevier. ISBN 0-444-51795-2. ISSN 0924-6509. MR 2147066. Zbl 1091.26008. 
Ehrgott, Matthias (2005). Multicriteria Optimization. Springer-Berlin. ISBN 3-540-21398-8. 
Steele, J. Michael (2004). The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities. Cambridge University Press. ISBN 978-0-521-54677-5. 
== External links ==
Hazewinkel, Michiel, ed. (2001), "Inequality", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Graph of Inequalities by Ed Pegg, Jr., Wolfram Demonstrations Project.
AoPS Wiki entry about Inequalities
PAGE	Irreducible polynomial	'Polynomials'
In mathematics, an irreducible polynomial is, roughly speaking, a non-constant polynomial that may not be factored into the product of two non-constant polynomials. The property of irreducibility depends on the field or ring to which the coefficients are considered to belong. For example, the polynomial x2 - 2 is irreducible if the coefficients 1 and -2 are considered as integers and factors as if the coefficients are considered as real numbers. One says "the polynomial x2 - 2 is irreducible over the integers but not over the reals".
A polynomial that is not irreducible is sometimes said to be reducible. However this term must be used with care, as it may refer to other notions of reduction.
Irreducible polynomials appear naturally in polynomial factorization and algebraic field extensions.
It is helpful to compare irreducible polynomials to prime numbers: prime numbers (together with the corresponding negative numbers of equal magnitude) are the irreducible integers. They exhibit many of the general properties of the concept of 'irreducibility' that equally apply to irreducible polynomials, such as the essentially unique factorization into prime or irreducible factors.
== Definition ==
If F is a field, a non-constant polynomial is irreducible over F if its coefficients belong to F and it cannot be factored into the product of two non-constant polynomials with coefficients in F.
A polynomial with integer coefficients, or, more generally, with coefficients in a unique factorization domain R is sometimes said to be irreducible over R if it is an irreducible element of the polynomial ring (a polynomial ring over a unique factorization domain is also a unique factorization domain), that is, it is not invertible, nor zero and cannot be factored into the product of two non-invertible polynomials with coefficients in R. Another definition is frequently used, saying that a polynomial is irreducible over R if it is irreducible over the field of fractions of R (the field of rational numbers, if R is the integers). Both definitions generalize the definition given for the case of coefficients in a field, because, in this case, the non constant polynomials are exactly the polynomials that are non-invertible and non zero.
== Simple examples ==
The following six polynomials demonstrate some elementary properties of reducible and irreducible polynomials:
,
,
,
,
,
.
Over the ring of integers, the first three polynomials are reducible (the third one is reducible because the factor 3 is not invertible in the integers), the last two are irreducible. (The fourth, of course, is not a polynomial over the integers.)
Over the field of rational numbers, the first two and the fourth polynomials are reducible, but the other three polynomials are irreducible (as a polynomial over the rationals, 3 is a unit, and, therefore, does not count as a factor).
Over the field of real numbers, the first five polynomials are reducible, but is still irreducible.
Over the field of complex numbers, all six polynomials are reducible.
== Over the complexes ==
Over the complex field, and, more generally, over an algebraically closed field, a univariate polynomial is irreducible if and only if its degree is one. This is the Fundamental theorem of algebra in the case of complexes and, in general, the definition of "algebraically closed".
It follows that every nonconstant univariate polynomial can be factored as
where is the degree, the leading coefficient and the zeros of the polynomial (not necessarily distincts).
There are irreducible multivariate polynomials of every degree over the complexes. For example, the polynomial
which defines Fermat curve, is irreducible for every positive n.
== Over the reals ==
Over the field of reals, the degree of an irreducible univariate polynomial is either one or two. More precisely, the irreducible polynomials are the polynomials of degree one and the quadratic polynomials that have a negative discriminant 
It follows that every non-constant univariate polynomial can be factored as a product of polynomials of degree at most two. For example, factors over the real numbers as and it cannot be factored further, as both factors have a negative discriminant: 
As in the complex case, there are irreducible polynomials in two (or more) variables of every degree.
== Unique factorization property ==
Every polynomial over a field F may be factored in a product of a non-zero constant and a finite number of irreducible (over F) polynomials. This decomposition is unique up to the order of the factors and the multiplication of the factors by non-zero constants whose product is 1.
Over a unique factorization domain the same theorem is true, but is more accurately formulated by using the notion of primitive polynomial. A primitive polynomial is a polynomial over a unique factorization domain, such that 1 is a greatest common divisor of its coefficients.
Let F be a unique factorization domain. A non-constant irreducible polynomial over F is primitive. A primitive polynomial over F is irreducible over F if and only if it is irreducible over the field of fractions of F. Every polynomial over F may be decomposed into the product of a non zero constant and a finite number of non-constant irreducible primitive polynomials. The non-zero constant may itself be decomposed into the product of a unit of F and a finite number of irreducible elements of F. Both factorizations are unique up to the order of the factors and the multiplication of the factors by a unit of F
This is this theorem which motivates that the definition of irreducible polynomial over a unique factorization domain often suppose that the polynomial is non-constant.
All algorithms, which are presently implemented, for factoring polynomials over the integers and over the rational numbers use this result (see Factorization of polynomials).
== Over the integers ==
The irreducibility of a polynomial over the integers is related to that over the field of elements (for a prime ). In particular, if a univariate polynomial f over is irrreducible over for some prime that does not divide the leading coefficient of f (the coefficient of the higher power of the variable), then f is irreducible over . Eisenstein's criterion is a variant of this property where irreducibility over is also involved.
The converse, however, is not true, there are polynomials of arbitrary large degree that are irreducible over the integers and reducible over every finite field. A simple example of such a polynomial is which is irreducible over the integers and reducible over every finite field.
The relationship between irreducibility over the integers and irreducibility modulo p is deeper than the previous result: to date, all implemented algorithms for factorization and irreducibility over the integers and over the rational numbers use the factorization over finite fields as subroutine.
== Algorithms ==
The unique factorization property of polynomials does not mean that the factorization of a given polynomial may always be computed. Even the irreducibility of a polynomial may not always been proved by a computation: there are fields over which no algorithm can exist for deciding the irreducibility of any polynomial.
Algorithms for factoring polynomials and deciding irreducibility are known and implemented in computer algebra systems for polynomials over the integers, the rational numbers, finite fields and finitely generated field extension of these fields. All these algorithms use the algorithms for factorization of polynomials over finite fields.
== Field extension ==
The notions of irreducible polynomial and of algebraic field extension are strongly related, in the following way.
Let x be an element of an extension L of a field K. This element is said to be algebraic if it is a root of a polynomial with coefficients in K. Among the polynomials, of which x is a root, there is exactly one which is monic and of minimal degree, called the minimal polynomial of x. The minimal polynomial of an algebraic element x of L is irreducible, and is the unique monic irreducible polynomial of which x is a root. The minimal polynomial of x divides every polynomial which has x as a root (this is Abel's irreducibility theorem).
Conversely, if is a univariate polynomial over a field K, let be the quotient ring of the polynomial ring by the ideal generated by P. Then L is a field if and only if P is irreducible over K. In this case, if x is the image of X in L, the minimal polynomial of x is the quotient of P by its leading coefficient.
An example of what precedes is the standard definition of the complex numbers as 
If a polynomial P has an irreducible factor Q over K, which has a degree greater than one, one may apply to Q the preceding construction of an algebraic extension, for getting an extension in which P has at least one more root than in K. Iterating this construction, one gets eventually a field over which P factors into linear factors. This field, unique up to a field isomorphism, is called the splitting field of P.
== Over an integral domain ==
If R is an integral domain, an element f of R which is neither zero nor a unit is called irreducible if there are no non-units g and h with f = gh. One can show that every prime element is irreducible; the converse is not true in general but holds in unique factorization domains. The polynomial ring F[x] over a field F (or any unique-factorization domain) is again a unique factorization domain. Inductively, this means that the polynomial ring in n indeterminants (over a ring R) is a unique factorization domain if the same is true for R.
== See also ==
Gauss's lemma (polynomial)
Rational root theorem, a method of finding whether a polynomial has a linear factor with rational coefficients
Eisenstein's criterion
Perron method
Hilbert's irreducibility theorem
Cohn's irreducibility criterion
Irreducible component of a topological space
Factorization of polynomials over finite fields
Quartic function#Solving by factoring into quadratics
Cubic function#Factorization
Casus irreducibilis, the irreducible cubic with three real roots
Quadratic equation#Quadratic factorization
== Notes ==
== References ==
Gallian, Joseph (2012), Contemporary Abstract Algebra (8th ed.), Cengage Learning 
Lidl, Rudolf; Niederreiter, Harald (1997), Finite fields (2nd ed.), Cambridge University Press, ISBN 978-0-521-39231-0 , pp. 91.
Mac Lane, Saunders; Birkhoff, Garrett (1999), Algebra (3rd ed.), American Mathematical Society 
Menezes, Alfred J.; Van Oorschot, Paul C.; Vanstone, Scott A. (1997), Handbook of applied cryptography, CRC Press, ISBN 978-0-8493-8523-0 , pp. 154.
== External links ==
Weisstein, Eric W., "Irreducible Polynomial", MathWorld.
Irreducible Polynomial at PlanetMath.org.
Information on Primitive and Irreducible Polynomials, The (Combinatorial) Object Server.
PAGE	Line (geometry)	'Elementary geometry'
The notion of line or straight line was introduced by ancient mathematicians to represent straight objects with negligible width and depth. Lines are an idealization of such objects. Until the seventeenth century, lines were defined like this: "The line is the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which [ &#8230; ] will leave from its imaginary moving some vestige in length, exempt of any width. [ &#8230; ] The straight line is that which is equally extended between its points"
Euclid described a line as "breadthless length", and introduced several postulates as basic unprovable properties from which he constructed the geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of nineteenth century (such as non-Euclidean, projective and affine geometry).
In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.
When a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.
A line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points. Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment. Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these.
== Definitions versus descriptions ==
All definitions are ultimately circular in nature since they depend on concepts which must themselves have definitions, a dependence which can not be continued indefinitely without returning to the starting point. To avoid this vicious circle certain concepts must be taken as primitive concepts; terms which are given no definition. In geometry, it is frequently the case that the concept of line is taken as a primitive. In those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. When the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy.
In a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. In this circumstance it is possible that a description or mental image of a primitive notion is provided to give a foundation to build the notion on which would formally be based on the (unstated) axioms. Descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. These are not true definitions and could not be used in formal proofs of statements. The "definition" of line in Euclid's Elements falls into this category. Even in the case where a specific geometry is being considered (for example, Euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally.
== Ray ==
Given a line and any point A on it, we may consider A as decomposing this line into two parts. Each such part is called a ray (or half-line) and the point A is called its initial point. The point A is considered to be a member of the ray. Intuitively, a ray consists of those points on a line passing through A and proceeding indefinitely, starting at A, in one direction only along the line. However, in order to use this concept of a ray in proofs a more precise definition is required.
Given distinct points A and B, they determine a unique ray with initial point A. As two points define a unique line, this ray consists of all the points between A and B (including A and B) and all the points C on the line through A and B such that B is between A and C. This is, at times, also expressed as the set of all points C such that A is not between B and C. A point D, on the line determined by A and B but not in the ray with initial point A determined by B, will determine another ray with initial point A. With respect to the AB ray, the AD ray is called the opposite ray.
Thus, we would say that two different points, A and B, define a line and a decomposition of this line into the disjoint union of an open segment (A, &#8201; B) and two rays, BC and AD (the point D is not drawn in the diagram, but is to the left of A on the line AB). These are not opposite rays since they have different initial points.
In Euclidean geometry two rays with a common endpoint form an angle.
The definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.
In topology, a ray in a space X is a continuous embedding R+ &#8594; X. It is used to define the important concept of end of the space.
== Euclidean geometry ==
When geometry was first formalised by Euclid in the Elements, he defined a line to be "breadthless length" with a straight line being a line "which lies evenly with the points on itself". These definitions serve little purpose since they use terms which are not, themselves, defined. In fact, Euclid did not use these definitions in this work and probably included them just to make it clear to the reader what was being discussed. In modern geometry, a line is simply taken as an undefined object with properties given by axioms, but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined.
In an axiomatic formulation of Euclidean geometry, such as that of Hilbert (Euclid's original axioms contained various flaws which have been corrected by modern mathematicians), a line is stated to have certain properties which relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point. In two dimensions, i.e., the Euclidean plane, two lines which do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not.
Any collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.
=== Cartesian plane ===
Lines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations. In two dimensions, the equation for non-vertical lines is often given in the slope-intercept form:
where:
m is the slope or gradient of the line.
b is the y-intercept of the line.
x is the independent variable of the function y = f(x).
The slope of the line through points A(xa, ya) and B(xb, yb), when xa &#8800; xb, is given by m = (yb &#8722; ya)/(xb &#8722; xa) and the equation of this line can be written y = m(x &#8722; xa) + ya.
In R2, every line L (including vertical lines) is described by a linear equation of the form
with fixed real coefficients a, b and c such that a and b are not both zero. Using this form, vertical lines correspond to the equations with b = 0.
There are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. These forms (see Linear equation for other forms) are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.
The equation of the line passing through two different points and may be written as
.
If x0 &#8800; x1, this equation may be rewritten as
or
In three dimensions, lines can not be described by a single linear equation, so they are frequently described by parametric equations:
where:
x, y, and z are all functions of the independent variable t which ranges over the real numbers.
(x0, y0, z0) is any point on the line.
a, b, and c are related to the slope of the line, such that the vector (a, b, c) is parallel to the line.
They may also be described as the simultaneous solutions of two linear equations
such that and are not proportional (the relations imply t = 0). This follows since in three dimensions a single linear equation typically describes a plane and a line is what is common to two distinct intersecting planes.
==== Normal form ====
The normal segment for a given line is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin. The normal form of the equation of a straight line on the plane is given by:
where &#952; is the angle of inclination of the normal segment (the oriented angle from the unit vector of the x axis to this segment), and p is the (positive) length of the normal segment. The normal form can be derived from the general form by dividing all of the coefficients by
This form is also called the Hesse normal form, after the German mathematician Ludwig Otto Hesse.
Unlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, &#952; and p, to be specified. Note that if p > 0, then &#952; is uniquely defined modulo 2 &#960; . On the other hand, if the line is through the origin (c = 0, p = 0), one drops the |c|/( &#8722; c) term to compute sin &#952; and cos &#952; , and &#952; is only defined modulo &#960; .
=== Polar coordinates ===
In polar coordinates on the Euclidean plane the slope-intercept form of the equation of a line is expressed as:
where m is the slope of the line and b is the y-intercept. When &#952; = 0 the graph will be undefined. The equation can be rewritten to eliminate discontinuities in this manner:
In polar coordinates on the Euclidean plane, the intercept form of the equation of a line that is non-horizontal, non-vertical, and does not pass through pole may be expressed as,
where and represent the x and y intercepts respectively. The above equation is not applicable for vertical and horizontal lines because in these cases one of the intercepts does not exist. Moreover, it is not applicable on lines passing through the pole since in this case, both x and y intercepts are zero (which is not allowed here since and are denominators). A vertical line that doesn't pass through the pole is given by the equation
Similarly, a horizontal line that doesn't pass through the pole is given by the equation
The equation of a line which passes through the pole is simply given as:
where m is the slope of the line.
=== Vector equation ===
The vector equation of the line through points A and B is given by r = OA + &#955; AB (where &#955; is a scalar).
If a is vector OA and b is vector OB, then the equation of the line can be written: r = a + &#955; (b &#8722; a).
A ray starting at point A is described by limiting &#955; . One ray is obtained if &#955; &#8805; 0, and the opposite ray comes from &#955; &#8804; 0.
=== Euclidean space ===
In three-dimensional space, a first degree equation in the variables x, y, and z defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in n-dimensional space n-1 first-degree equations in the n coordinate variables define a line under suitable conditions.
In more general Euclidean space, Rn (and analogously in every other affine space), the line L passing through two different points a and b (considered as vectors) is the subset
The direction of the line is from a (t = 0) to b (t = 1), or in other words, in the direction of the vector b &#8722; a. Different choices of a and b can yield the same line.
==== Collinear points ====
Three points are said to be collinear if they lie on the same line. Three points usually determine a plane, but in the case of three collinear points this does not happen.
In affine coordinates, in n-dimensional space the points X=(x1, x2, ..., xn), Y=(y1, y2, ..., yn), and Z=(z1, z2, ..., zn) are collinear if the matrix
has a rank less than 3. In particular, for three points in the plane (n = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.
Equivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, k points in a plane are collinear if and only if any (k &#8211; 1) pairs of points have the same pairwise slopes.
In Euclidean geometry, the Euclidean distance d(a,b) between two points a and b may be used to express the collinearity between three points by:
The points a, b and c are collinear if and only if d(x,a) = d(c,a) and d(x,b) = d(c,b) implies x=c.
However there are other notions of distance (such as the Manhattan distance) for which this property is not true.
In the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.
=== Types of lines ===
In a sense, all lines in Euclidean geometry are equal, in that, without coordinates, one can not tell them apart from one another. However, lines may play special roles with respect to other objects in the geometry and be divided into types according to that relationship. For instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:
tangent lines, which touch the conic at a single point;
secant lines, which intersect the conic at two points and pass through its interior;
exterior lines, which do not meet the conic at any point of the Euclidean plane; or
a directrix, whose distance from a point helps to establish whether the point is on the conic.
In the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.
For more general algebraic curves, lines could also be:
i-secant lines, meeting the curve in i points counted without multiplicity, or
asymptotes, which a curve approaches arbitrarily closely without touching it.
With respect to triangles we have:
the Euler line,
the Simson lines, and
central lines.
For a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.
For a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.
Parallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other &#8212; every point that is on either one of them is also on the other.
Perpendicular lines are lines that intersect at right angles.
In three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.
== Projective geometry ==
In many models of projective geometry, the representation of a line rarely conforms to the notion of the "straight curve" as it is visualised in Euclidean geometry. In Elliptic geometry we see a typical example of this. In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.
== Geodesics ==
The "straightness" of a line, interpreted as the property that the distance along the line between any two of its points is minimized, can be generalized and leads to the concept of geodesics in metric spaces.
== See also ==
Line segment
Curve
Locus
Distance from a point to a line
Distance between two lines
Affine function
Incidence (geometry)
Plane (geometry)
== Notes ==
== References ==
Coxeter, H.S.M (1969), Introduction to Geometry (2nd ed.), New York: John Wiley & Sons, ISBN 0-471-18283-4 
Faber, Richard L. (1983). Foundations of Euclidean and Non-Euclidean Geometry. New York: Marcel Dekker. ISBN 0-8247-1748-1. 
Pedoe, Dan (1988), Geometry: A Comprehensive Course, Mineola, NY: Dover, ISBN 0-486-65812-0 
Wylie, Jr., C. R. (1964), Foundations of Geometry, New York: McGraw-Hill, ISBN 0-07-072191-2 
== External links ==
Hazewinkel, Michiel, ed. (2001), "Line (curve)", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Line", MathWorld.
Equations of the Straight Line at Cut-the-Knot
Citizendium
PAGE	Line segment	'Elementary geometry'
In geometry, a line segment is a part of a line that is bounded by two distinct end points, and contains every point on the line between its end points. A closed line segment includes both endpoints, while an open line segment excludes both endpoints; a half-open line segment includes exactly one of the endpoints.
Examples of line segments include the sides of a triangle or square. More generally, when both of the segment's end points are vertices of a polygon or polyhedron, the line segment is either an edge (of that polygon or polyhedron) if they are adjacent vertices, or otherwise a diagonal. When the end points both lie on a curve such as a circle, a line segment is called a chord (of that curve).
== In real or complex vector spaces ==
If V is a vector space over or , and L is a subset of V, then L is a line segment if L can be parameterized as
for some vectors , in which case the vectors u and u + v are called the end points of L.
Sometimes one needs to distinguish between "open" and "closed" line segments. Then one defines a closed line segment as above, and an open line segment as a subset L that can be parametrized as
for some vectors .
Equivalently, a line segment is the convex hull of two points. Thus, the line segment can be expressed as a convex combination of the segment's two end points.
In geometry, it is sometimes defined that a point B is between two other points A and C, if the distance AB added to the distance BC is equal to the distance AC. Thus in the line segment with endpoints A = (ax, ay) and C = (cx, cy) is the following collection of points:
.
== Properties ==
A line segment is a connected, non-empty set.
If V is a topological vector space, then a closed line segment is a closed set in V. However, an open line segment is an open set in V if and only if V is one-dimensional.
More generally than above, the concept of a line segment can be defined in an ordered geometry.
A pair of line segments can be any one of the following: intersecting, parallel, skew, or none of these.The last possibility is a way that line segments differ from lines: if two nonparallel lines are in the same Euclidean plane they must cross each other, but that need not be true of segments.
== In proofs ==
In an axiomatic treatment of geometry, the notion of betweenness is either assumed to satisfy a certain number of axioms, or else be defined in terms of an isometry of a line (used as a coordinate system).
Segments play an important role in other theories. For example, a set is convex if the segment that joins any two points of the set is contained in the set. This is important because it transforms some of the analysis of convex sets to the analysis of a line segment. The Segment Addition Postulate can be used to add congruent segment or segments with equal lengths and consequently substitute other segments into another statement to make segments congruent.
== As a degenerate ellipse ==
A line segment can be viewed as a degenerate case of an ellipse in which the semiminor axis goes to zero, the foci go to the endpoints, and the eccentricity goes to one. As a degenerate orbit this is a radial elliptic trajectory.
== In other geometric shapes ==
In addition to appearing as the sides and diagonals of polygons and polyhedra, line segments appear in numerous other locations relative to other geometric shapes.
=== Triangles ===
Some very frequently considered segments in a triangle include the three altitudes (each perpendicularly connecting a side or its extension to the opposite vertex), the three medians (each connecting a side's midpoint to the opposite vertex), the perpendicular bisectors of the sides (perpendicularly connecting the midpoint of a side to one of the other sides), and the internal angle bisectors (each connecting a vertex to the opposite side). In each case there are various equalities relating these segment lengths to others (discussed in the articles on the various types of segment) as well as various inequalities.
Other segments of interest in a triangle include those connecting various triangle centers to each other, most notably the incenter, the circumcenter, the nine-point center, the centroid, and the orthocenter.
=== Quadrilaterals ===
In addition to the sides and diagonals of a quadrilateral, some important segments are the two bimedians (connecting the midpoints of opposite sides) and the four maltitudes (each perpendicularly connecting one side to the midpoint of the opposite side).
=== Circles and ellipses ===
Any line segment connecting two points on a circle or ellipse is called a chord. Any chord in a circle which has no longer chord is called a diameter, and any segment connecting the circle's center (the midpoint of a diameter) to a point on the circle is called a radius.
In an ellipse, the longest chord is called the major axis, and a segment from the midpoint of the major axis (the ellipse's center) to either endpoint of the major axis is called a semi-major axis. Similarly, the shortest chord of an ellipse is called the minor axis, and the segment from its midpoint (the ellipse's center) to either of its endpoints is called a semi-minor axis. The chords of an ellipse which are perpendicular to the major axis and pass through one of its foci are called the latera recta of the ellipse.
== See also ==
Interval (mathematics)
Line (geometry)
Line segment intersection, the algorithmic problem of finding intersecting pairs in a collection of line segments
Spirangle
Segment addition postulate
== References ==
David Hilbert: The Foundations of Geometry. The Open Court Publishing Company 1950, p. 4
== External links ==
Line Segment at PlanetMath
Definition of line segment With interactive animation
Copying a line segment with compass and straightedge
Dividing a line segment into N equal parts with compass and straightedge Animated demonstration
This article incorporates material from Line segment on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
PAGE	Linear equation	'Elementary algebra'	'Equations'	'Polynomials'
A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable.
Linear equations can have one or more variables. Linear equations occur abundantly in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some "background" state. Linear equations do not include exponents.
This article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.
== One variable ==
A linear equation in one unknown x may always be rewritten
If a &#8800; 0, there is a unique solution
If a = 0, then either the equation does not have any solution, if b &#8800; 0 (it is inconsistent), or every number is a solution, if b is also zero.
== Two variables ==
A common form of a linear equation in the two variables x and y is
where m and b designate constants (parameters). The origin of the name "linear" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, otherwise known as the y-intercept.
Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.
=== Forms for two-dimensional linear equations ===
Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the "equations of the straight line." In what follows, x, y, t, and &#952; are variables; other letters represent constants (fixed numbers).
==== General (or standard) form ====
In the general (or standard) form the linear equation is written as:
where A and B are not both equal to zero. The equation is usually written so that A &#8805; 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is &#8722; A/B. The general form is sometimes written as:
where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.
==== Slope &#8211; intercept form ====
where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.
==== Point &#8211; slope form ====
where m is the slope of the line and (x1,y1) is any point on the line.
The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y &#8722; y1) is proportional to the difference in the x coordinate (that is, x &#8722; x1). The proportionality constant is m (the slope of the line).
==== Two-point form ====
where (x1, y1) and (x2, y2) are two points on the line with x2 &#8800; x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2 &#8722; y1)/(x2 &#8722; x1).
Multiplying both sides of this equation by (x2 &#8722; x1) yields a form of the line generally referred to as the symmetric form:
Expanding the products and regrouping the terms leads to the general form:
Using a determinant, one gets a determinant form, easy to remember:
==== Intercept form ====
where a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.
==== Matrix form ====
Using the order of the standard form
one can rewrite the equation in matrix form:
Further, this representation extends to systems of linear equations.
becomes:
Since this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.
==== Parametric form ====
and
Two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:
and
In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.
==== 2D vector determinant form ====
The equation of a line can also be written as the determinant of two vectors. If and are unique points on the line, then will also be a point on the line if the following is true:
One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.
To expand on this we can say that , and . Thus and , then the above equation becomes:
Thus,
Ergo,
Then dividing both side by would result in the &#8220; Two-point form &#8221; shown above, but leaving it here allows the equation to still be valid when .
==== Special cases ====
This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.
This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of line which is not the graph of a function (it obviously fails the vertical line test).
=== Connection with linear functions ===
A linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:
and
where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.
== More than two variables ==
A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewritten
where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.
If all the coefficients are zero, then either b &#8800; 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.
If at least one coefficient is nonzero, a permutation of the subscripts allows to suppose a1 &#8800; 0, and rewrite the equation
In other words, if ai &#8800; 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.
If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n &#8211; 1)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).
== See also ==
Line (geometry)
System of linear equations
Linear equation over a ring
Algebraic equation
Linear belief function
Linear inequality
== Notes ==
== References ==
Barnett, R.A.; Ziegler, M.R.; Byleen, K.E. (2008), College Mathematics for Business, Economics, Life Sciences and the Social Sciences (11th ed.), Upper Saddle River, N.J.: Pearson, ISBN 0-13-157225-3 
== External links ==
Linear Equations and Inequalities Open Elementary Algebra textbook chapter on linear equations and inequalities.
Hazewinkel, Michiel, ed. (2001), "Linear equation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4
PAGE	Nth root	'Elementary algebra'
In mathematics, the nth root of a number x is a number r which, when raised to the power n yields x
where n is the degree of the root. A root of degree 2 is called a square root and a root of degree 3, a cube root. Roots of higher degree are referred by using ordinal numbers, as in fourth root, twentieth root, etc.
For example:
2 is a square root of 4, since 22 = 4.
 &#8722; 2 is also a square root of 4, since ( &#8722; 2)2 = 4.
A real number or complex number has n roots of degree n. While the roots of 0 are not distinct (all equaling 0), the n nth roots of any other real or complex number are all distinct. If n is even and x is real and positive, one of its nth roots is positive, one is negative, and the rest are complex but not real; if n is even and x is real and negative, none of the nth roots is real. If n is odd and x is real, one nth root is real and has the same sign as the radicand , while the other roots are not real.
Roots are usually written using the radical symbol or radix or , with or denoting the square root, denoting the cube root, denoting the fourth root, and so on. In the expression , n is called the index, is the radical sign or radix, and x is called the radicand. When a number is presented under the radical symbol, it must return only one result like a function, so a non-negative real root, called the principal nth root, is preferred rather than others. An unresolved root, especially one using the radical symbol, is often referred to as a surd or a radical. Any expression containing a radical, whether it is a square root, a cube root, or a higher root, is called a radical expression, and if it contains no transcendental functions or transcendental numbers it is called an algebraic expression.
In calculus, roots are treated as special cases of exponentiation, where the exponent is a fraction:
Roots are particularly important in the theory of infinite series; the root test determines the radius of convergence of a power series. Nth roots can also be defined for complex numbers, and the complex roots of 1 (the roots of unity) play an important role in higher mathematics. Galois theory can be used to determine which algebraic numbers can be expressed using roots, and to prove the Abel-Ruffini theorem, which states that a general polynomial equation of degree five or higher cannot be solved using roots alone; this result is also known as "the insolubility of the quintic".
== History ==
The origin of the root symbol &#8730; is largely speculative. Some sources imply that the symbol was first used by Arabic mathematicians. One of those mathematicians was Ab &#363; al-Hasan ibn Al &#299; al-Qalas &#257; d &#299; (1421 &#8211; 1486). Legend has it that it was taken from the Arabic letter " &#1580; &#8206; " ( &#487; &#299; m, /d &#658; im/), which is the first letter in the Arabic word " &#1580; &#1584; &#1585; &#8206; " (jadhir, meaning "root"; / &#712; d &#658; &#593; &#720; &#240; ir/). However, many scholars, including Leonhard Euler, believe it originates from the letter "r", the first letter of the Latin word "radix" (meaning "root"), referring to the same mathematical operation. The symbol was first seen in print without the vinculum (the horizontal "bar" over the numbers inside the radical symbol) in the year 1525 in Die Coss by Christoff Rudolff, a German mathematician.
The term surd traces back to al-Khw &#257; rizm &#299; (c. 825), who referred to rational and irrational numbers as audible and inaudible, respectively. This later led to the Arabic word " &#1571; &#1589; &#1605; &#8206; " (asamm, meaning "deaf" or "dumb") for irrational number being translated into Latin as "surdus" (meaning "deaf" or "mute"). Gerard of Cremona (c. 1150), Fibonacci (1202), and then Robert Recorde (1551) all used the term to refer to unresolved irrational roots. In the U.S., an early contributor to its understanding was a little-known African American mathematician, Charles T. Gidiney who in 1843 proposed an innovative solution: "A Consice Method of Extracting the Fourth Root: Example.".
== Definition and notation ==
The nth root of a number x, where n is a positive integer, is a number r whose nth power is x:
Every positive real number x has a single positive nth root, which is written . For n equal to 2 this is called the square root and the n is omitted. The nth root can also be represented using exponentiation as x1/n.
For even values of n, positive numbers also have a negative nth root, while negative numbers do not have a real nth root. For odd values of n, every negative number x has a real negative nth root. For example, &#8722; 2 has a real 5th root, but &#8722; 2 does not have any real 6th roots.
Every non-zero number x, real or complex, has n different complex number nth roots including any positive or negative roots, see complex roots below. The nth root of 0 is 0.
The nth roots of almost all numbers (all integers except the nth powers, and all rationals except the quotients of two nth powers) are irrational. For example,
All nth roots of integers, or in fact of any algebraic number, are algebraic.
For the extension of powers and roots to indices that are not positive integers, see exponentiation.
The character codes for the radical symbols are
=== Square roots ===
A square root of a number x is a number r which, when squared, becomes x:
Every positive real number has two square roots, one positive and one negative. For example, the two square roots of 25 are 5 and &#8722; 5. The positive square root is also known as the principal square root, and is denoted with a radical sign:
Since the square of every real number is a positive real number, negative numbers do not have real square roots. However, every negative number has two imaginary square roots. For example, the square roots of &#8722; 25 are 5i and &#8722; 5i, where i represents a square root of &#8722; 1.
=== Cube roots ===
A cube root of a number x is a number r whose cube is x:
Every real number x has exactly one real cube root, written . For example,
Every real number has two additional complex cube roots (see complex roots below).
== Identities and properties ==
Every positive real number has a positive nth root and the rules for operations with such surds are straightforward:
Using the exponent form as in normally makes it easier to cancel out powers and roots.
Problems can occur when taking the nth roots of negative or complex numbers. For instance:
whereas
when taking the principal value of the roots. See failure of power and logarithm identities in the exponentiation article for more details.
== Simplified form of a radical expression ==
A radical expression is said to be in simplified form if
There is no factor of the radicand that can be written as a power greater than or equal to the index.
There are no fractions under the radical sign.
There are no radicals in the denominator.
For example, to write the radical expression in simplified form, we can proceed as follows. First, look for a perfect square under the square root sign and remove it:
Next, there is a fraction under the radical sign, which we change as follows:
Finally, we remove the radical from the denominator as follows:
When there is a denominator involving surds it may be possible to find a factor to multiply both numerator and denominator by to simplify the expression. For instance using the factorization of the sum of two cubes:
Simplifying radical expressions involving nested radicals can be quite difficult. It is not immediately obvious for instance that:
== Infinite series ==
The radical or root may be represented by the infinite series:
with . This expression can be derived from the binomial series.
== Computing principal roots ==
The nth root of an integer is not always an integer, and if it is not an integer then it is not a rational number. For instance, the fifth root of 34 is
where the dots signify that the decimal expression does not end after any finite number of digits. Since in this example the digits after the decimal never enter a repeating pattern, the number is irrational.
The nth root of a number A can be computed by the nth root algorithm, a special case of Newton's method. Start with an initial guess x0 and then iterate using the recurrence relation
until the desired precision is reached.
Depending on the application, it may be enough to use only the first Newton approximant:
For example, to find the fifth root of 34, note that 25 = 32 and thus take x = 2, n = 5 and y = 2 in the above formula. This yields
The error in the approximation is only about 0.03%.
Newton's method can be modified to produce a generalized continued fraction for the nth root which can be modified in various ways as described in that article. For example:
In the case of the fifth root of 34 above (after dividing out selected common factors):
=== Digit-by-digit calculation of principal roots of decimal (base 10) numbers ===
Building on the digit-by-digit calculation of a square root, it can be seen that the formula used there, , or , follows a pattern involving Pascal's triangle. For the nth root of a number is defined as the value of element in row of Pascal's Triangle such that , we can rewrite the expression as . For convenience, call the result of this expression . Using this more general expression, any positive principal root can be computed, digit-by-digit, as follows.
Write the original number in decimal form. The numbers are written similar to the long division algorithm, and, as in long division, the root will be written on the line above. Now separate the digits into groups of digits equating to the root being taken, starting from the decimal point and going both left and right. The decimal point of the root will be above the decimal point of the square. One digit of the root will appear above each group of digits of the original number.
Beginning with the left-most group of digits, do the following procedure for each group:
Starting on the left, bring down the most significant (leftmost) group of digits not yet used (if all the digits have been used, write "0" the number of times required to make a group) and write them to the right of the remainder from the previous step (on the first step, there will be no remainder). In other words, multiply the remainder by and add the digits from the next group. This will be the current value c.
Find p and x, as follows:
Let be the part of the root found so far, ignoring any decimal point. (For the first step, ).
Determine the greatest digit such that .
Place the digit as the next digit of the root, i.e., above the group of digits you just brought down. Thus the next p will be the old p times 10 plus x.
Subtract from to form a new remainder.
If the remainder is zero and there are no more digits to bring down, then the algorithm has terminated. Otherwise go back to step 1 for another iteration.
==== Examples ====
Find the square root of 152.2756.
 1 2. 3 4 
 /
 \/ 01 52.27 56
 01 100 &#183; 1 &#183; 00 &#183; 12 + 101 &#183; 2 &#183; 01 &#183; 11 &#8804; 1 < 100 &#183; 1 &#183; 00 &#183; 22 + 101 &#183; 2 &#183; 01 &#183; 21 x = 1
 01 y = 100 &#183; 1 &#183; 00 &#183; 12 + 101 &#183; 2 &#183; 01 &#183; 12 = 1 + 0 = 1
 00 52 100 &#183; 1 &#183; 10 &#183; 22 + 101 &#183; 2 &#183; 11 &#183; 21 &#8804; 52 < 100 &#183; 1 &#183; 10 &#183; 32 + 101 &#183; 2 &#183; 11 &#183; 31 x = 2
 00 44 y = 100 &#183; 1 &#183; 10 &#183; 22 + 101 &#183; 2 &#183; 11 &#183; 21 = 4 + 40 = 44
 08 27 100 &#183; 1 &#183; 120 &#183; 32 + 101 &#183; 2 &#183; 121 &#183; 31 &#8804; 827 < 100 &#183; 1 &#183; 120 &#183; 42 + 101 &#183; 2 &#183; 121 &#183; 41 x = 3
 07 29 y = 100 &#183; 1 &#183; 120 &#183; 32 + 101 &#183; 2 &#183; 121 &#183; 31 = 9 + 720 = 729
 98 56 100 &#183; 1 &#183; 1230 &#183; 42 + 101 &#183; 2 &#183; 1231 &#183; 41 &#8804; 9856 < 100 &#183; 1 &#183; 1230 &#183; 52 + 101 &#183; 2 &#183; 1231 &#183; 51 x = 4
 98 56 y = 100 &#183; 1 &#183; 1230 &#183; 42 + 101 &#183; 2 &#183; 1231 &#183; 41 = 16 + 9840 = 9856
 00 00 Algorithm terminates: Answer is 12.34
Find the cube root of 4192 to the nearest hundredth.
 1 6. 1 2 4
 3 /
 \/ 004 192.000 000 000
 004 100 &#183; 1 &#183; 00 &#183; 13 + 101 &#183; 3 &#183; 01 &#183; 12 + 102 &#183; 3 &#183; 02 &#183; 11 &#8804; 4 < 100 &#183; 1 &#183; 00 &#183; 23 + 101 &#183; 3 &#183; 01 &#183; 22 + 102 &#183; 3 &#183; 02 &#183; 21 x = 1
 001 y = 100 &#183; 1 &#183; 00 &#183; 13 + 101 &#183; 3 &#183; 01 &#183; 12 + 102 &#183; 3 &#183; 02 &#183; 11 = 1 + 0 + 0 = 1
 003 192 100 &#183; 1 &#183; 10 &#183; 63 + 101 &#183; 3 &#183; 11 &#183; 62 + 102 &#183; 3 &#183; 12 &#183; 61 &#8804; 3192 < 100 &#183; 1 &#183; 10 &#183; 73 + 101 &#183; 3 &#183; 11 &#183; 72 + 102 &#183; 3 &#183; 12 &#183; 71 x = 6
 003 096 y = 100 &#183; 1 &#183; 10 &#183; 63 + 101 &#183; 3 &#183; 11 &#183; 62 + 102 &#183; 3 &#183; 12 &#183; 61 = 216 + 1,080 + 1,800 = 3,096
 096 000 100 &#183; 1 &#183; 160 &#183; 13 + 101 &#183; 3 &#183; 161 &#183; 12 + 102 &#183; 3 &#183; 162 &#183; 11 &#8804; 96000 < 100 &#183; 1 &#183; 160 &#183; 23 + 101 &#183; 3 &#183; 161 &#183; 22 + 102 &#183; 3 &#183; 162 &#183; 21 x = 1
 077 281 y = 100 &#183; 1 &#183; 160 &#183; 13 + 101 &#183; 3 &#183; 161 &#183; 12 + 102 &#183; 3 &#183; 162 &#183; 11 = 1 + 480 + 76,800 = 77,281
 018 719 000 100 &#183; 1 &#183; 1610 &#183; 23 + 101 &#183; 3 &#183; 1611 &#183; 22 + 102 &#183; 3 &#183; 1612 &#183; 21 &#8804; 18719000 < 100 &#183; 1 &#183; 1610 &#183; 33 + 101 &#183; 3 &#183; 1611 &#183; 32 + 102 &#183; 3 &#183; 1612 &#183; 31 x = 2
 015 571 928 y = 100 &#183; 1 &#183; 1610 &#183; 23 + 101 &#183; 3 &#183; 1611 &#183; 22 + 102 &#183; 3 &#183; 1612 &#183; 21 = 8 + 19,320 + 15,552,600 = 15,571,928
 003 147 072 000 100 &#183; 1 &#183; 16120 &#183; 43 + 101 &#183; 3 &#183; 16121 &#183; 42 + 102 &#183; 3 &#183; 16122 &#183; 41 &#8804; 3147072000 < 100 &#183; 1 &#183; 16120 &#183; 53 + 101 &#183; 3 &#183; 16121 &#183; 52 + 102 &#183; 3 &#183; 16122 &#183; 51 x = 4
 The desired precision is achieved:
 The cube root of 4192 is about 16.12
== Complex roots ==
Every complex number other than 0 has n different nth roots.
=== Square roots ===
The two square roots of a complex number are always negatives of each other. For example, the square roots of &#8722; 4 are 2i and &#8722; 2i, and the square roots of i are
If we express a complex number in polar form, then the square root can be obtained by taking the square root of the radius and halving the angle:
A principal root of a complex number may be chosen in various ways, for example
which introduces a branch cut in the complex plane along the positive real axis with the condition 0 &#8804; &#952; < 2 &#960; , or along the negative real axis with &#8722; &#960; < &#952; &#8804; &#960; .
Using the first(last) branch cut the principal square root maps to the half plane with non-negative imaginary(real) part. The last branch cut is presupposed in mathematical software like Matlab or Scilab.
=== Roots of unity ===
The number 1 has n different nth roots in the complex plane, namely
where
These roots are evenly spaced around the unit circle in the complex plane, at angles which are multiples of . For example, the square roots of unity are 1 and &#8722; 1, and the fourth roots of unity are 1, , &#8722; 1, and .
=== nth roots ===
Every complex number has n different nth roots in the complex plane. These are
where &#951; is a single nth root, and 1, &#969; , &#969; 2, ... &#969; n &#8722; 1 are the nth roots of unity. For example, the four different fourth roots of 2 are
In polar form, a single nth root may be found by the formula
Here r is the magnitude (the modulus, also called the absolute value) of the number whose root is to be taken; if the number can be written as a+bi then . Also, is the angle formed as one pivots on the origin counterclockwise from the positive horizontal axis to a ray going from the origin to the number; it has the properties that and 
Thus finding nth roots in the complex plane can be segmented into two steps. First, the magnitude of all the nth roots is the nth root of the magnitude of the original number. Second, the angle between the positive horizontal axis and a ray from the origin to one of the nth roots is , where is the angle defined in the same way for the number whose root is being taken. Furthermore, all n of the nth roots are at equally spaced angles from each other.
As with square roots, the formula above does not define a continuous function over the entire complex plane, but instead has a branch cut at points where &#952; / n is discontinuous.
== Solving polynomials ==
It was once conjectured that all polynomial equations could be solved algebraically (that is, that all roots of a polynomial could be expressed in terms of a finite number of radicals and elementary operations). However, while this is true for third degree polynomials (cubics) and fourth degree polynomials (quartics), the Abel-Ruffini theorem (1824) shows that this is not true in general when the degree is 5 or greater. For example, the solutions of the equation
cannot be expressed in terms of radicals. (cf. quintic equation)
For solving any equation of the nth degree numerically, to obtain a result that is arbitrarily close to being exact, see Root-finding algorithm.
== See also ==
Nth root algorithm
Shifting nth-root algorithm
Irrational number
Algebraic number
Nested radical
Twelfth root of two
Super-root
== References ==
== External links ==
PAGE	Operation (mathematics)	'Elementary mathematics'
The general operation as explained on this page should not be confused with the more specific operators on vector spaces. For a notion in elementary mathematics, see arithmetic operation.
In its simplest meaning in mathematics and logic, an operation is an action or procedure which produces a new value from one or more input values, called "operands". There are two common types of operations: unary and binary. Unary operations involve only one value, such as negation and trigonometric functions. Binary operations, on the other hand, take two values, and include addition, subtraction, multiplication, division, and exponentiation.
Operations can involve mathematical objects other than numbers. The logical values true and false can be combined using logic operations, such as and, or, and not. Vectors can be added and subtracted. Rotations can be combined using the function composition operation, performing the first rotation and then the second. Operations on sets include the binary operations union and intersection and the unary operation of complementation. Operations on functions include composition and convolution.
Operations may not be defined for every possible value. For example, in the real numbers one cannot divide by zero or take square roots of negative numbers. The values for which an operation is defined form a set called its domain. The set which contains the values produced is called the codomain, but the set of actual values attained by the operation is its range. For example, in the real numbers, the squaring operation only produces nonnegative numbers; the codomain is the set of real numbers but the range is the nonnegative numbers.
Operations can involve dissimilar objects. A vector can be multiplied by a scalar to form another vector. And the inner product operation on two vectors produces a scalar. An operation may or may not have certain properties, for example it may be associative, commutative, anticommutative, idempotent, and so on.
The values combined are called operands, arguments, or inputs, and the value produced is called the value, result, or output. Operations can have fewer or more than two inputs.
An operation is like an operator, but the point of view is different. For instance, one often speaks of "the operation of addition" or "addition operation" when focusing on the operands and result, but one says "addition operator" (rarely "operator of addition") when focusing on the process, or from the more abstract viewpoint, the function +: S &#215; S &#8594; S.
== General description ==
An operation &#969; is a function of the form &#969; : V &#8594; Y, where V &#8834; X1 &#215; &#8230; &#215; Xk. The sets Xk are called the domains of the operation, the set Y is called the codomain of the operation, and the fixed non-negative integer k (the number of arguments) is called the type or arity of the operation. Thus a unary operation has arity one, and a binary operation has arity two. An operation of arity zero, called a nullary operation, is simply an element of the codomain Y. An operation of arity k is called a k-ary operation. Thus a k-ary operation is a (k+1)-ary relation that is functional on its first k domains.
The above describes what is usually called a finitary operation, referring to the finite number of arguments (the value k). There are obvious extensions where the arity is taken to be an infinite ordinal or cardinal, or even an arbitrary set indexing the arguments.
Often, use of the term operation implies that the domain of the function is a power of the codomain (i.e. the Cartesian product of one or more copies of the codomain), although this is by no means universal, as in the example of multiplying a vector by a scalar.
== See also ==
Algebra
Unicode mathematical operators
=== Special cases ===
Unary operation
Binary operation
=== Related topics ===
== Notes ==
^ See e.g. Chapter II, Definition 1.1 in: S. N. Burris and H. P. Sankappanavar, A Course in Universal Algebra, Springer, 1981. [1]
PAGE	Parallel (geometry)	'Elementary geometry'
In geometry, parallel lines are lines in a plane which do not meet; that is, two lines in a plane that do not intersect or touch at any point are said to be parallel. By extension, a line and a plane, or two planes, in three-dimensional Euclidean space that do not share a point are said to be parallel. However, two lines in three-dimensional space which do not meet must be in a common plane to be considered parallel; otherwise they are called skew lines. Parallel planes are planes in the same three-dimensional space that never meet.
Parallel lines are the subject of Euclid's parallel postulate. Parallelism is primarily a property of affine geometries and Euclidean space is a special instance of this type of geometry. Some other spaces, such as hyperbolic space, have analogous properties that are sometimes referred to as parallelism.
== Symbol ==
The parallel symbol is . For example, indicates that line AB is parallel to line CD.
In the Unicode character set, the "parallel" and "not parallel" signs have codepoints U+2225 ( &#8741; ) and U+2226 ( &#8742; ), respectively. In addition, U+22D5 ( &#8917; ) represents the relation "equal and parallel to".
== Euclidean parallelism ==
=== Two lines in a plane ===
==== Conditions for parallelism ====
Given parallel straight lines l and m in Euclidean space, the following properties are equivalent:
Every point on line m is located at exactly the same (minimum) distance from line l (equidistant lines).
Line m is in the same plane as line l but does not intersect l (recall that lines extend to infinity in either direction).
When lines m and l are both intersected by a third straight line (a transversal) in the same plane, the corresponding angles of intersection with the transversal are congruent.
Since these are equivalent properties, any one of them could be taken as the definition of parallel lines in Euclidean space, but the first and third properties involve measurement, and so, are "more complicated" than the second. Thus, the second property is the one usually chosen as the defining property of parallel lines in Euclidean geometry. The other properties are then consequences of Euclid's Parallel Postulate. Another property that also involves measurement is that lines parallel to each other have the same gradient (slope).
==== History ====
The definition of parallel lines as a pair of straight lines in a plane which do not meet appears as Definition 23 in Book I of Euclid's Elements. Alternative definitions were discussed by other Greeks, often as part of an attempt to prove the parallel postulate. Proclus attributes a definition of parallel lines as equidistant lines to Posidonius and quotes Geminus in a similar vein. Simplicius also mentions Posidonius' definition as well as its modification by the philosopher Aganis.
At the end of the nineteenth century, in England, Euclid's Elements was still the standard textbook in secondary schools. The traditional treatment of geometry was being pressured to change by the new developments in projective geometry and non-Euclidean geometry, so several new textbooks for the teaching of geometry were written at this time. A major difference between these reform texts, both between themselves and between them and Euclid, is the treatment of parallel lines. These reform texts were not without their critics and one of them, Charles Dodgson (a.k.a. Lewis Carroll), wrote a play, Euclid and His Modern Rivals, in which these texts are lambasted.
One of the early reform textbooks was James Maurice Wilson's Elementary Geometry of 1868. Wilson based his definition of parallel lines on the primitive notion of direction. According to Wilhelm Killing the idea may be traced back to Leibniz. Wilson, without defining direction since it is a primitive, uses the term in other definitions such as his sixth definition, "Two straight lines that meet one another have different directions, and the difference of their directions is the angle between them." Wilson (1868, p. 2) In definition 15 he introduces parallel lines in this way; "Straight lines which have the same direction, but are not parts of the same straight line, are called parallel lines." Wilson (1868, p. 12) Augustus De Morgan reviewed this text and declared it a failure, primarily on the basis of this definition and the way Wilson used it to prove things about parallel lines. Dodgson also devotes a large section of his play (Act II, Scene VI &#167; 1) to denouncing Wilson's treatment of parallels. Wilson edited this concept out of the third and higher editions of his text.
Other properties, proposed by other reformers, used as replacements for the definition of parallel lines, did not fare much better. The main difficulty, as pointed out by Dodgson, was that to use them in this way required additional axioms to be added to the system. The equidistant line definition of Posidonius, expounded by Francis Cuthbertson in his 1874 text Euclidean Geometry suffers from the problem that the points that are found at a fixed given distance on one side of a straight line must be shown to form a straight line. This can not be proved and must be assumed to be true. The corresponding angles formed by a transversal property, used by W. D. Cooley in his 1860 text, The Elements of Geometry, simplified and explained requires a proof of the fact that if one transversal meets a pair of lines in congruent corresponding angles then all transversals must do so. Again, a new axiom is needed to justify this statement.
==== Construction ====
The three properties above lead to three different methods of construction of parallel lines.
==== Distance between two parallel lines ====
Because parallel lines in a Euclidean plane are equidistant there is a unique distance between the two parallel lines. Given the equations of two non-vertical, non-horizontal parallel lines,
the distance between the two lines can be found by locating two points (one on each line) that lie on a common perpendicular to the parallel lines and calculating the distance between them. Since the lines have slope m, a common perpendicular would have slope &#8722; 1/m and we can take the line with equation y = &#8722; x/m as a common perpendicular. Solve the linear systems
and
to get the coordinates of the points. The solutions to the linear systems are the points
and
These formulas still give the correct point coordinates even if the parallel lines are horizontal (i.e., m = 0). The distance between the points is
which reduces to
When the lines are given by the general form of the equation of a line (horizontal and vertical lines are included):
their distance can be expressed as
=== Two lines in three-dimensional space ===
Two lines in the same three-dimensional space that do not intersect need not be parallel. Only if they are in a common plane are they called parallel; otherwise they are called skew lines.
Two distinct lines l and m in three-dimensional space are parallel if and only if the distance from a point P on line m to the nearest point on line l is independent of the location of P on line m. This never holds for skew lines.
=== A line and a plane ===
A line m and a plane q in three-dimensional space, the line not lying in that plane, are parallel if and only if they do not intersect.
Equivalently, they are parallel if and only if the distance from a point P on line m to the nearest point in plane q is independent of the location of P on line m.
=== Two planes ===
Similar to the fact that parallel lines must be located in the same plane, parallel planes must be situated in the same three-dimensional space and contain no point in common.
Two distinct planes q and r are parallel if and only if the distance from a point P in plane q to the nearest point in plane r is independent of the location of P in plane q. This will never hold if the two planes are not in the same three-dimensional space.
== Extension to non-Euclidean geometry ==
In non-Euclidean geometry, it is more common to talk about geodesics than (straight) lines. A geodesic is the shortest path between two points in a given geometry. In physics this may be interpreted as the path that a particle follows if no force is applied to it. In non-Euclidean geometry (elliptic or hyperbolic geometry) the three Euclidean properties mentioned above are not equivalent and only the second one, since it involves no metrics, is useful in non-Euclidean geometries. In general geometry the three properties above give three different types of curves, equidistant curves, parallel geodesics and geodesics sharing a common perpendicular, respectively.
While in Euclidean geometry two geodesics can either intersect or be parallel, in general, and in hyperbolic space in particular, there are three possibilities. Two geodesics can either be:
intersecting, if they intersect in a common point in the plane,
parallel, if they do not intersect in the plane, but have a common limit point at infinity, or
ultra parallel, if they do not have a common limit point at infinity.
In the literature ultra parallel geodesics are often called non-intersecting. Geodesics intersecting at infinity are then called limit geodesics.
=== Spherical ===
In spherical geometry, all geodesics are great circles. Great circles divide the sphere in two equal hemispheres and all great circles intersect each other. Thus, there are no parallel geodesics to a given geodesic, as all geodesics intersect. Equidistant curves on the sphere are called parallels of latitude analogous to the latitude lines on a globe. Parallels of latitude can be generated by the intersection of the sphere with a plane parallel to a plane through the center of the sphere.
== Reflexive variant ==
In synthetic, affine geometry the relation of two parallel lines is a fundamental concept that is modified from the usage in Euclidean geometry. It is clear that the relation of parallelism is a symmetric relation and a transitive relation. These are two properties of an equivalence relation. In Euclidean geometry a line is not considered to be parallel to itself, but in affine geometry it is convenient to hold a line as parallel to itself, thus yielding parallelism as an equivalence relation.
Another way of describing this type of parallelism is the requirement that their intersection is not a singleton. Two lines are then parallel when they have all or none of their points in common. It has been noted that Playfair's axiom used in affine and Euclidean geometry is then equivalent to the statement that parallelism forms a transitive relation on the set of lines in the plane.
== See also ==
Clifford parallel
Limiting parallel
Ultraparallel theorem
== Notes ==
== References ==
Heath, Thomas L. (1956), The Thirteen Books of Euclid's Elements (2nd ed. [Facsimile. Original publication: Cambridge University Press, 1925] ed.), New York: Dover Publications 
(3 vols.): ISBN 0-486-60088-2 (vol. 1), ISBN 0-486-60089-0 (vol. 2), ISBN 0-486-60090-4 (vol. 3). Heath's authoritative translation plus extensive historical research and detailed commentary throughout the text.
Richards, Joan L. (1988), Mathematical Visions: The Pursuit of Geometry in Victorian England, Boston: Academic Press, ISBN 0-12-587445-6 
Wilson, James Maurice (1868), Elementary Geometry (1st ed.), London: Macmillan and Co. 
Wylie, Jr., C.R. (1964), Foundations of Geometry, McGraw &#8211; Hill 
== Further reading ==
Papadopoulos, Athanase; Th &#233; ret, Guillaume (2014), La th &#233; orie des parall &#232; les de Johann Heinrich Lambert : Pr &#233; sentation, traduction et commentaires, Paris: Collection Sciences dans l'histoire, Librairie Albert Blanchard, ISBN 978-2-85367-266-5 
== External links ==
Constructing a parallel line through a given point with compass and straightedge
PAGE	Perimeter	'Elementary geometry'
A perimeter is a path that surrounds a two-dimensional shape. The word comes from the Greek peri (around) and meter (measure). The term may be used either for the path or its length - it can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference.
Calculating the perimeter has considerable practical applications. The perimeter can be used to calculate the length of fence required to surround a yard or garden. The perimeter of a wheel (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter.
== Formulae ==
The perimeter is the distance around a shape. Perimeters for more general shapes can be calculated as any path with where is the length of the path and is an infinitesimal line element. Both of these must be replaced with other algebraic forms in order to be solved: an advanced notion of perimeter, which includes hypersurfaces bounding volumes in -dimensional Euclidean spaces can be found in the theory of Caccioppoli sets.
== Polygons ==
Polygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.
The perimeter of a polygon equals the sum of the lengths of its edges. In particular, the perimeter of a rectangle which width is and length is equal to .
An equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.
A regular polygon may be defined by the number of its sides and by its radius, that is to say, the constant distance between its centre and each of its vertices. One can calculate the length of its sides using trigonometry. If R is a regular polygon's radius and n is the number of its sides, then its perimeter is
A splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. A cleaver is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths.
== Circumference of a circle ==
The perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, &#960; (the greek p for perimeter), such that if P is the circle's perimeter and D its diameter then:
In terms of the radius r of the circle, this formula becomes:
To calculate a circle's perimeter, knowledge of its radius or diameter and of the number &#960; is sufficient. The problem is that &#960; is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of &#960; is important for the calculation. The search for the digits of &#960; is relevant to many fields, such as mathematical analysis, algorithmics and computer science.
== Perception of perimeter ==
The perimeter and the area are the main two measures of geometric figures. Confusing them is frequent, as well as believing that the greater one of them is, the greater is the other. Indeed, an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/10,000 scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by 10,000. The real area is 10,0002 times the area of the shape on the map.
Nevertheless there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.
Proclus (5th century) reported that Greek peasants "fairly" parted fields relying on their perimeters. But a field's production is proportional to its area, not to its perimeter: many naive peasants may have got fields with long perimeters but low areas (thus, low crops).
If one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, some people may confuse perimeter with convex hull. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. On the animated picture on the left, all the figures have the same convex hull: the big, first hexagon.
== Isoperimetry ==
The isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive: it is the circle. In particular, that is why drops of fat on a broth surface are circular.
This problem may seem simple, but its mathematical proof needs sophisticated theorems. The isoperimetric problem is sometimes simplified: to find the quadrilateral, or the triangle or another particular figure, with the largest area amongst those having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with n sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is an irregular polygon.
== See also ==
Arclength
Area
Caccioppoli set
Circumference
Coastline paradox
Isoperimetric inequality
Pythagorean theorem
Volume
Wetted perimeter
== References ==
^ Heath, T. (1981). A History of Greek Mathematics 2. Dover Publications. p. 206. ISBN 0-486-24074-6. 
== External links ==
This article incorporates information from this version of the equivalent article on the French Wikipedia.
Weisstein, Eric W., "Perimeter", MathWorld.
Weisstein, Eric W., "Semiperimeter", MathWorld.
PAGE	Perpendicular	'Elementary geometry'
In elementary geometry, the property of being perpendicular (perpendicularity) is the relationship between two lines which meet at a right angle (90 degrees). The property extends to other related geometric objects.
A line is said to be perpendicular to another line if the two lines intersect at a right angle. Explicitly, a first line is perpendicular to a second line if (1) the two lines meet; and (2) at the point of intersection the straight angle on one side of the first line is cut by the second line into two congruent angles. Perpendicularity can be shown to be symmetric, meaning if a first line is perpendicular to a second line, then the second line is also perpendicular to the first. For this reason, we may speak of two lines as being perpendicular (to each other) without specifying an order.
Perpendicularity easily extends to segments and rays. For example, a line segment is perpendicular to a line segment if, when each is extended in both directions to form an infinite line, these two resulting lines are perpendicular in the sense above. In symbols, means line segment AB is perpendicular to line segment CD. The point B is called a foot of the perpendicular from A to segment , or simply, a foot of A on .
A line is said to be perpendicular to a plane if it is perpendicular to every line in the plane that it intersects. Note that this definition depends on the definition of perpendicularity between lines.
Two planes in space are said to be perpendicular if the dihedral angle at which they meet is a right angle (90 degrees).
Perpendicularity is one particular instance of the more general mathematical concept of orthogonality; perpendicularity is the orthogonality of classical geometric objects. Thus, in advanced mathematics, the word "perpendicular" is sometimes used to describe much more complicated geometric orthogonality conditions, such as that between a surface and its normal.
== Construction of the perpendicular ==
To make the perpendicular to the line AB through the point P using compass and straightedge, proceed as follows (see figure):
Step 1 (red): construct a circle with center at P to create points A' and B' on the line AB, which are equidistant from P.
Step 2 (green): construct circles centered at A' and B' having equal radius. Let Q and R be the points of intersection of these two circles.
Step 3 (blue): connect Q and R to construct the desired perpendicular PQ.
To prove that the PQ is perpendicular to AB, use the SSS congruence theorem for ' and QPB' to conclude that angles OPA' and OPB' are equal. Then use the SAS congruence theorem for triangles OPA' and OPB' to conclude that angles POA and POB are equal.
== In relationship to parallel lines ==
If two lines (a and b) are both perpendicular to a third line (c), all of the angles formed along the third line are right angles. Therefore, in Euclidean geometry, any two lines that are both perpendicular to a third line are parallel to each other, because of the parallel postulate. Conversely, if one line is perpendicular to a second line, it is also perpendicular to any line parallel to that second line.
In the figure at the right, all of the orange-shaded angles are congruent to each other and all of the green-shaded angles are congruent to each other, because vertical angles are congruent and alternate interior angles formed by a transversal cutting parallel lines are congruent. Therefore, if lines a and b are parallel, any of the following conclusions leads to all of the others:
One of the angles in the diagram is a right angle.
One of the orange-shaded angles is congruent to one of the green-shaded angles.
Line c is perpendicular to line a.
Line c is perpendicular to line b.
== In computing distances ==
The distance from a point to a line is the distance to the nearest point on that line. That is the point at which a segment from it to the given point is perpendicular to the line.
Likewise, the distance from a point to a curve is measured by a line segment that is perpendicular to a tangent line to the curve at the nearest point on the curve.
Perpendicular regression fits a line to data points by minimizing the sum of squared perpendicular distances from the data points to the line.
The distance from a point to a plane is measured as the length from the point along a segment that is perpendicular to the plane, meaning that it is perpendicular to all lines in the plane that pass through the nearest point in the plane to the given point.
== Graph of functions ==
In the two-dimensional plane, right angles can be formed by two intersected lines which the product of their slopes equals &#8722; 1. Thus defining two linear functions: y1 = a1x + b1 and y2 = a2x + b2, the graphs of the functions will be perpendicular and will make four right angles where the lines intersect if and only if a1a2 = &#8722; 1. However, this method cannot be used if the slope is zero or undefined (the line is parallel to an axis).
For another method, let the two linear functions: a1x + b1y + c1 = 0 and a2x + b2y + c2 = 0. The lines will be perpendicular if and only if a1a2 + b1b2 = 0. This method is simplified from the dot product (or, more generally, the inner product) of vectors. In particular, two vectors are considered orthogonal if their inner product is zero.
== In circles and other conics ==
=== Circles ===
Each diameter of a circle is perpendicular to the tangent line to that circle at the point where the diameter intersects the circle.
A line segment through a circle's center bisecting a chord is perpendicular to the chord.
If the intersection of any two perpendicular chords divides one chord into lengths a and b and divides the other chord into lengths c and d, then a2 + b2 + c2 + d 2 equals the square of the diameter.
The sum of the squared lengths of any two perpendicular chords intersecting at a given point is the same as that of any other two perpendicular chords intersecting at the same point, and is given by 8r 2 &#8211; 4p 2 (where r is the circle's radius and p is the distance from the center point to the point of intersection).
Thales' theorem states that two lines both through the same point on a circle but going through opposite endpoints of a diameter are perpendicular.
=== Ellipses ===
The major and minor axes of an ellipse are perpendicular to each other and to the tangent lines to the ellipse at the points where the axes intersect the ellipse.
The major axis of an ellipse is perpendicular to the directrix and to each latus rectum.
=== Parabolas ===
In a parabola, the axis of symmetry is perpendicular to each of the latus rectum, the directrix, and the tangent line at the point where the axis intersects the parabola.
From a point on the tangent line to a parabola's vertex, the other tangent line to the parabola is perpendicular to the line from that point through the parabola's focus.
The orthoptic property of a parabola is that If two tangents to the parabola are perpendicular to each other, then they intersect on the directrix. Conversely, two tangents which intersect on the directrix are perpendicular.
=== Hyperbolas ===
The transverse axis of a hyperbola is perpendicular to the conjugate axis and to each directrix.
The product of the perpendicular distances from a point P on a hyperbola or on its conjugate hyperbola to the asymptotes is a constant independent of the location of P.
A rectangular hyperbola has asymptotes that are perpendicular to each other. It has an eccentricity equal to 
== In polygons ==
=== Triangles ===
The legs of a right triangle are perpendicular to each other.
The altitudes of a triangle are perpendicular to their respective bases. The perpendicular bisectors of the sides also play a prominent role in triangle geometry.
The Euler line of an isosceles triangle is perpendicular to the triangle's base.
The Droz-Farny line theorem concerns a property of two perpendicular lines intersecting at a triangle's orthocenter.
Harcourt's theorem concerns the relationship of line segments through a vertex and perpendicular to any line tangent to the triangle's incircle.
=== Quadrilaterals ===
In a square or other rectangle, all pairs of adjacent sides are perpendicular. A right trapezoid is a trapezoid that has two pairs of adjacent sides that are perpendicular.
Each of the four maltitudes of a quadrilateral is a perpendicular to a side through the midpoint of the opposite side.
An orthodiagonal quadrilateral is a quadrilateral whose diagonals are perpendicular. These include the square, the rhombus, and the kite. By Brahmagupta's theorem, in an orthodiagonal quadrilateral that is also cyclic, a line through the midpoint of one side and through the intersection point of the diagonals is perpendicular to the opposite side.
By van Aubel's theorem, if squares are constructed externally on the sides of a quadrilateral, the line segments connecting the centers of opposite squares are perpendicular and equal in length.
== Lines in three dimensions ==
Up to three lines in three-dimensional space can be pairwise perpendicular, as exemplified by the x, y, and z axes of a three-dimensional Cartesian coordinate system.
== See also ==
Tangential and normal components
== Notes ==
== References ==
Altshiller-Court, Nathan (1925), College Geometry: An Introduction to the Modern Geometry of the Triangle and the Circle (2nd ed.), New York: Barnes & Noble, LCCN 52-13504 
Kay, David C. (1969), College Geometry, New York: Holt, Rinehart and Winston, LCCN 69-12075 
== External links ==
Definition: perpendicular with interactive animation.
How to draw a perpendicular bisector of a line with compass and straight edge (animated demonstration).
How to draw a perpendicular at the endpoint of a ray with compass and straight edge (animated demonstration).
PAGE	Polynomial arithmetic	'Polynomials'
Polynomial arithmetic is a branch of algebra dealing with some properties of polynomials which share strong analogies with properties of number theory relative to integers. It includes basic mathematical operations such as addition, subtraction, and multiplication, as well as more elaborate operations like Euclidean division, and properties related to roots of polynomials. The latter are essentially connected to the fact that the set K[X] of univariate polynomials with coefficients in a field K is a commutative ring, such as the ring of integers .
== Elementary operations on polynomials ==
Addition and subtraction of two polynomials are performed by adding or subtracting corresponding coefficients. If
then addition is defined as
 where m > n
Multiplication is performed much the same way as addition and subtraction, but instead by multiplying the corresponding coefficients. If then multiplication is defined as where . Note that we treat as zero for and that the degree of the product is equal to the sum of the degrees of the two polynomials.
== Advanced polynomial arithmetics and comparison with number theory ==
Many fascinating properties of polynomials can be found when, thanks to the basic operations that can be performed on two polynomials and the underlying commutative ring structure of the set they live in, one tries to apply reasonings similar to those known from number theory.
To see this, one first needs to introduce two concepts: the notion of root of a polynomial and that of divisibility for pairs of polynomials.
If one considers a polynom of a single variable X in a field K (typically or ), and with coefficients in that field, a root of is an element of K such that
The second concept, divisibility of polynomials, allows to see a first analogy with number theory: a polynomial is said to divide another polynomial when the latter can be written as
with C being ALSO a polynomial. This definition is similar to divisibility for integers, and the fact that divides is also denoted .
The relation between both concepts above arises when noticing the following property: is a root of if and only if . Whereas one logical inclusion ("if") is obvious, the other ("only if") relies on a more elaborate concept, the Euclidean division of polynomials, here again strongly reminding of the Euclidean division of integers.
From this it follows that one can define prime polynomials, as polynomials that cannot be divided by any other polynomials but 1 and themselves (up to an overall constant factor) - here again the analogously with prime integers is manifest, and allows that some of the main definitions and theorems related to prime numbers and number theory have their counterpart in polynomial algebra. The most important result is the fundamental theorem of algebra, allowing for factorization of any polynomial as a product of prime ones. Worth mentioning is also the B &#233; zout's identity in the context of polynomials. It states that two given polynomials P and Q have as greatest common divisor (GCD) a third polynomial D (D is then unique as GCD of P and Q up to a finite constant factor), if and only if there exists polynomials U and V such that
.
== See also ==
Polynomial long division
Polynomial greatest common divisor
== References ==
Stallings, William; : "Cryptography And Network Security: Principles and Practice", pages 121-126. Prentice Hall, 1999.
== External links ==
J.A. Beachy and W.D. Blair; : "Polynomials", from "Abstract algebra", 2nd edition, 1996.
PAGE	Polynomial long division	'Polynomials'
In algebra, polynomial long division is an algorithm for dividing a polynomial by another polynomial of the same or lower degree, a generalised version of the familiar arithmetic technique called long division. It can be done easily by hand, because it separates an otherwise complex division problem into smaller ones. Sometimes using a shorthand version called synthetic division is faster, with less writing and fewer calculations.
Polynomial long division is an algorithm that implements the Euclidean division of polynomials, which starting from two polynomials A (the dividend) and B (the divisor) produces, if B is not zero, a quotient Q and a remainder R such that
A = BQ + R,
and either R = 0 or the degree of R is lower than the degree of B. These conditions define uniquely Q and R, which means that Q and R do not depend on the method used to compute them.
== Example ==
Find the quotient and the remainder of the division of the dividend, by the divisor.
The dividend is first rewritten like this:
The quotient and remainder can then be determined as follows:
The polynomial above the bar is the quotient q(x), and the number left over ( 5) is the remainder r(x).
The long division algorithm for arithmetic is very similar to the above algorithm, in which the variable x is replaced by the specific number 10.
== Pseudo-code ==
The algorithm can be represented in pseudo-code as follows, where +, -, and &#215; represent polynomial arithmetic, and / represents simple division of two terms:
function n / d:
 require d &#8800; 0
 (q, r) &#8592; (0, n) # At each step n = d &#215; q + r
 while r &#8800; 0 AND degree(r) &#8805; degree(d):
 t &#8592; lead(r)/lead(d) # Divide the leading terms
 (q, r) &#8592; (q + t, r - (t * d))
 return (q, r)
Note that this works equally well when degree(n) < degree(d); in that case the result is just the trivial (0, n).
This algorithm describes exactly above paper and pencil method: d is written on the left of the ")"; q is written, term after term, above the horizontal line, the last term being the value of t; the region under the horizontal line is used to compute and write down the successive values of r.
== Euclidean division ==
For every pair of polynomials (A, B) such that B &#8800; 0, polynomial division provides a quotient Q and a remainder R such that
and either R=0 or degree(R) < degree(B). Moreover (Q, R) is the unique pair of polynomials having this property.
The process of getting, from A and B, the uniquely defined polynomials Q and R is called Euclidean division (sometimes division transformation). The polynomial long division is thus an algorithm for Euclidean division.
== Applications ==
=== Factoring polynomials ===
Sometimes one or more roots of a polynomial are known, perhaps having been found using the rational root theorem. If one root r of a polynomial P(x) of degree n is known then polynomial long division can be used to factor P(x) into the form (x - r)(Q(x)) where Q(x) is a polynomial of degree n &#8211; 1. Q(x) is simply the quotient obtained from the division process; since r is known to be a root of P(x), it is known that the remainder must be zero.
Likewise, if more than one root is known, a linear factor (x &#8211; r) in one of them (r) can be divided out to obtain Q(x), and then a linear term in another root, s, can be divided out of Q(x), etc. Alternatively, they can all be divided out at once: for example the linear factors x &#8211; r and x &#8211; s can be multiplied together to obtain the quadratic factor x2 &#8211; (r + s)x + rs, which can then be divided into the original polynomial P(x) to obtain a quotient of degree n &#8211; 2.
In this way, sometimes all the roots of a polynomial of degree greater than four can be obtained, even though that is not always possible. For example, if the rational root theorem can be used to obtain a single (rational) root of a quintic polynomial, it can be factored out to obtain a quartic (fourth degree) quotient; the explicit formula for the roots of a quartic polynomial can then be used to find the other four roots of the quintic.
=== Finding tangents to polynomial functions ===
Polynomial long division can be used to find the equation of the line that is tangent to the graph of the function defined by the polynomial P(x) at a particular point x = r. If R(x) is the remainder of the division of P(x) divided by (x &#8211; r )2, then the equation of the tangent line at x = r to the graph of the function y = P(x) is y = R(x), regardless of whether or not r is a root of the polynomial.
Example
Find the equation of the line that is tangent to the following curve at 
Begin by dividing the equation of the curve by 
The tangent is 
== See also ==
Polynomial remainder theorem
Synthetic division, a more concise method of performing polynomial long division
Ruffini's rule
Euclidean domain
Gr &#246; bner basis
Greatest common divisor of two polynomials
== Notes ==
^ S. Barnard (2008). Higher Algebra. READ BOOKS. p. 24. ISBN 1-4437-3086-6. 
^ Strickland-Constable, Charles, "A simple method for finding tangents to polynomial graphs", Mathematical Gazette 89, November 2005: 466-467.
Roe,Spencer and Taylor (2014) http://leicesteripsc.com/index.php?title=Group_3#References
PAGE	Pythagorean theorem	'Equations'
In mathematics, the Pythagorean theorem, also known as Pythagoras's theorem, is a relation in Euclidean geometry among the three sides of a right triangle. It states that the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. The theorem can be written as an equation relating the lengths of the sides a, b and c, often called the "Pythagorean equation":
... where c represents the length of the hypotenuse and a and b the lengths of the triangle's other two sides.
Although it is often argued that knowledge of the theorem predates him, the theorem is named after the ancient Greek mathematician Pythagoras (c. 570 &#8211; c. 495 BC) as it is he who, by tradition, is credited with its first recorded proof. There is some evidence that Babylonian mathematicians understood the formula, although little of it indicates an application within a mathematical framework. Mesopotamian, Indian and Chinese mathematicians are all known to have discovered the theorem independently and, in some cases, provide proofs for special cases.
The theorem has been given numerous proofs &#8211; possibly the most for any mathematical theorem. They are very diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years. The theorem can be generalized in various ways, including higher-dimensional spaces, to spaces that are not Euclidean, to objects that are not right triangles, and indeed, to objects that are not triangles at all, but n-dimensional solids. The Pythagorean theorem has attracted interest outside mathematics as a symbol of mathematical abstruseness, mystique, or intellectual power; popular references in literature, plays, musicals, songs, stamps and cartoons abound.
== Pythagorean proof ==
The Pythagorean Theorem was known long before Pythagoras, but he may well have been the first to prove it. In any event, the proof attributed to him is very simple, and is called a proof by rearrangement.
The two large squares shown in the figure each contain four identical triangles, and the only difference between the two large squares is that the triangles are arranged differently. Therefore, the white space within each of the two large squares must have equal area. Equating the area of the white space yields the Pythagorean Theorem, Q.E.D.
That Pythagoras originated this very simple proof is sometimes inferred from the writings of the later Greek philosopher and mathematician Proclus. Several other proofs of this theorem are described below, but this is known as the Pythagorean one.
== Other forms of the theorem ==
As pointed out in the introduction, if c denotes the length of the hypotenuse and a and b denote the lengths of the other two sides, the Pythagorean theorem can be expressed as the Pythagorean equation:
If the length of both a and b are known, then c can be calculated as
If the length of the hypotenuse c and of one side (a or b) are known, then the length of the other side can be calculated as
or
The Pythagorean equation relates the sides of a right triangle in a simple way, so that if the lengths of any two sides are known the length of the third side can be found. Another corollary of the theorem is that in any right triangle, the hypotenuse is greater than any one of the other sides, but less than their sum.
A generalization of this theorem is the law of cosines, which allows the computation of the length of any side of any triangle, given the lengths of the other two sides and the angle between them. If the angle between the other sides is a right angle, the law of cosines reduces to the Pythagorean equation.
== Other proofs of the theorem ==
This theorem may have more known proofs than any other (the law of quadratic reciprocity being another contender for that distinction); the book The Pythagorean Proposition contains 370 proofs.
=== Proof using similar triangles ===
This proof is based on the proportionality of the sides of two similar triangles, that is, upon the fact that the ratio of any two corresponding sides of similar triangles is the same regardless of the size of the triangles.
Let ABC represent a right triangle, with the right angle located at C, as shown on the figure. Draw the altitude from point C, and call H its intersection with the side AB. Point H divides the length of the hypotenuse c into parts d and e. The new triangle ACH is similar to triangle ABC, because they both have a right angle (by definition of the altitude), and they share the angle at A, meaning that the third angle will be the same in both triangles as well, marked as &#952; in the figure. By a similar reasoning, the triangle CBH is also similar to ABC. The proof of similarity of the triangles requires the Triangle postulate: the sum of the angles in a triangle is two right angles, and is equivalent to the parallel postulate. Similarity of the triangles leads to the equality of ratios of corresponding sides:
The first result equates the cosines of the angles &#952; , whereas the second result equates their sines.
These ratios can be written as
Summing these two equalities results in
which, after simplification, expresses the Pythagorean theorem:
The role of this proof in history is the subject of much speculation. The underlying question is why Euclid did not use this proof, but invented another. One conjecture is that the proof by similar triangles involved a theory of proportions, a topic not discussed until later in the Elements, and that the theory of proportions needed further development at that time.
=== Euclid's proof ===
In outline, here is how the proof in Euclid's Elements proceeds. The large square is divided into a left and right rectangle. A triangle is constructed that has half the area of the left rectangle. Then another triangle is constructed that has half the area of the square on the left-most side. These two triangles are shown to be congruent, proving this square has the same area as the left rectangle. This argument is followed by a similar version for the right rectangle and the remaining square. Putting the two rectangles together to reform the square on the hypotenuse, its area is the same as the sum of the area of the other two squares. The details follow.
Let A, B, C be the vertices of a right triangle, with a right angle at A. Drop a perpendicular from A to the side opposite the hypotenuse in the square on the hypotenuse. That line divides the square on the hypotenuse into two rectangles, each having the same area as one of the two squares on the legs.
For the formal proof, we require four elementary lemmata:
If two triangles have two sides of the one equal to two sides of the other, each to each, and the angles included by those sides equal, then the triangles are congruent (side-angle-side).
The area of a triangle is half the area of any parallelogram on the same base and having the same altitude.
The area of a rectangle is equal to the product of two adjacent sides.
The area of a square is equal to the product of two of its sides (follows from 3).
Next, each top square is related to a triangle congruent with another triangle related in turn to one of two rectangles making up the lower square.
The proof is as follows:
Let ACB be a right-angled triangle with right angle CAB.
On each of the sides BC, AB, and CA, squares are drawn, CBDE, BAGF, and ACIH, in that order. The construction of squares requires the immediately preceding theorems in Euclid, and depends upon the parallel postulate.
From A, draw a line parallel to BD and CE. It will perpendicularly intersect BC and DE at K and L, respectively.
Join CF and AD, to form the triangles BCF and BDA.
Angles CAB and BAG are both right angles; therefore C, A, and G are collinear. Similarly for B, A, and H.
Angles CBD and FBA are both right angles; therefore angle ABD equals angle FBC, since both are the sum of a right angle and angle ABC.
Since AB is equal to FB and BD is equal to BC, triangle ABD must be congruent to triangle FBC.
Since A-K-L is a straight line, parallel to BD, then rectangle BDLK has twice the area of triangle ABD because they share the base BD and have the same altitude BK, i.e., a line normal to their common base, connecting the parallel lines BD and AL. (lemma 2)
Since C is collinear with A and G, square BAGF must be twice in area to triangle FBC.
Therefore rectangle BDLK must have the same area as square BAGF = AB2.
Similarly, it can be shown that rectangle CKLE must have the same area as square ACIH = AC2.
Adding these two results, AB2 + AC2 = BD &#215; BK + KL &#215; KC
Since BD = KL, BD &#215; BK + KL &#215; KC = BD(BK + KC) = BD &#215; BC
Therefore AB2 + AC2 = BC2, since CBDE is a square.
This proof, which appears in Euclid's Elements as that of Proposition 47 in Book 1, demonstrates that the area of the square on the hypotenuse is the sum of the areas of the other two squares. This is quite distinct from the proof by similarity of triangles, which is conjectured to be the proof that Pythagoras used.
=== Proof by rearrangement ===
We have already discussed the Pythagorean proof, which was a proof by rearrangement. The same idea is conveyed by the leftmost animation below, which consists of a large square, side a + b, containing four identical right triangles. The triangles are shown in two arrangements, the first of which leaves two squares a2 and b2 uncovered, the second of which leaves square c2 uncovered. The area encompassed by the outer square never changes, and the area of the four triangles is the same at the beginning and the end, so the black square areas must be equal, therefore a2 + b2 = c2.
A second proof by rearrangement is given by the middle animation. A large square is formed with area c2, from four identical right triangles with sides a, b and c, fitted around a small central square. Then two rectangles are formed with sides a and b by moving the triangles. Combining the smaller square with these rectangles produces two squares of areas a2 and b2, which must have the same area as the initial large square.
The third, rightmost image also gives a proof. The upper two squares are divided as shown by the blue and green shading, into pieces that when rearranged can be made to fit in the lower square on the hypotenuse &#8211; or conversely the large square can be divided as shown into pieces that fill the other two. This shows the area of the large square equals that of the two smaller ones.
=== Algebraic proofs ===
The theorem can be proved algebraically using four copies of a right triangle with sides a, b and c, arranged inside a square with side c as in the top half of the diagram. The triangles are similar with area , while the small square has side b &#8722; a and area (b &#8722; a)2. The area of the large square is therefore
But this is a square with side c and area c2, so
A similar proof uses four copies of the same triangle arranged symmetrically around a square with side c, as shown in the lower part of the diagram. This results in a larger square, with side a + b and area (a + b)2. The four triangles and the square side c must have the same area as the larger square,
giving
A related proof was published by future U.S. President James A. Garfield (then a U.S. Representative). Instead of a square it uses a trapezoid, which can be constructed from the square in the second of the above proofs by bisecting along a diagonal of the inner square, to give the trapezoid as shown in the diagram. The area of the trapezoid can be calculated to be half the area of the square, that is
The inner square is similarly halved, and there are only two triangles so the proof proceeds as above except for a factor of , which is removed by multiplying by two to give the result.
=== Proof using differentials ===
One can arrive at the Pythagorean theorem by studying how changes in a side produce a change in the hypotenuse and employing calculus.
The triangle ABC is a right triangle, as shown in the upper part of the diagram, with BC the hypotenuse. At the same time the triangle lengths are measured as shown, with the hypotenuse of length y, the side AC of length x and the side AB of length a, as seen in the lower diagram part.
If x is increased by a small amount dx by extending the side AC slightly to D, then y also increases by dy. These form two sides of a triangle, CDE, which (with E chosen so CE is perpendicular to the hypotenuse) is a right triangle approximately similar to ABC. Therefore the ratios of their sides must be the same, that is:
This can be rewritten as follows:
This is a differential equation which is solved to give
And the constant can be deduced from x = 0, y = a to give the equation
This is more of an intuitive proof than a formal one: it can be made more rigorous if proper limits are used in place of dx and dy.
== Converse ==
The converse of the theorem is also true:
For any three positive numbers a, b, and c such that a2 + b2 = c2, there exists a triangle with sides a, b and c, and every such triangle has a right angle between the sides of lengths a and b.
An alternative statement is:
For any triangle with sides a, b, c, if a2 + b2 = c2, then the angle between a and b measures 90 &#176; .
This converse also appears in Euclid's Elements (Book I, Proposition 48):
"If in a triangle the square on one of the sides equals the sum of the squares on the remaining two sides of the triangle, then the angle contained by the remaining two sides of the triangle is right."
It can be proven using the law of cosines or as follows:
Let ABC be a triangle with side lengths a, b, and c, with a2 + b2 = c2. Construct a second triangle with sides of length a and b containing a right angle. By the Pythagorean theorem, it follows that the hypotenuse of this triangle has length c = &#8730; a2 + b2, the same as the hypotenuse of the first triangle. Since both triangles' sides are the same lengths a, b and c, the triangles are congruent and must have the same angles. Therefore, the angle between the side of lengths a and b in the original triangle is a right angle.
The above proof of the converse makes use of the Pythagorean Theorem itself. The converse can also be proven without assuming the Pythagorean Theorem.
A corollary of the Pythagorean theorem's converse is a simple means of determining whether a triangle is right, obtuse, or acute, as follows. Let c be chosen to be the longest of the three sides and a + b > c (otherwise there is no triangle according to the triangle inequality). The following statements apply:
If a2 + b2 = c2, then the triangle is right.
If a2 + b2 > c2, then the triangle is acute.
If a2 + b2 < c2, then the triangle is obtuse.
Edsger Dijkstra has stated this proposition about acute, right, and obtuse triangles in this language:
sgn( &#945; + &#946; &#8722; &#947; ) = sgn(a2 + b2 &#8722; c2),
where &#945; is the angle opposite to side a, &#946; is the angle opposite to side b, &#947; is the angle opposite to side c, and sgn is the sign function.
== Consequences and uses of the theorem ==
=== Pythagorean triples ===
A Pythagorean triple has three positive integers a, b, and c, such that a2 + b2 = c2. In other words, a Pythagorean triple represents the lengths of the sides of a right triangle where all three sides have integer lengths. Evidence from megalithic monuments in Northern Europe shows that such triples were known before the discovery of writing. Such a triple is commonly written (a, b, c). Some well-known examples are (3, 4, 5) and (5, 12, 13).
A primitive Pythagorean triple is one in which a, b and c are coprime (the greatest common divisor of a, b and c is 1).
The following is a list of primitive Pythagorean triples with values less than 100:
(3, 4, 5), (5, 12, 13), (7, 24, 25), (8, 15, 17), (9, 40, 41), (11, 60, 61), (12, 35, 37), (13, 84, 85), (16, 63, 65), (20, 21, 29), (28, 45, 53), (33, 56, 65), (36, 77, 85), (39, 80, 89), (48, 55, 73), (65, 72, 97)
=== Incommensurable lengths ===
One of the consequences of the Pythagorean theorem is that line segments whose lengths are incommensurable (so the ratio of which is not a rational number) can be constructed using a straightedge and compass. Pythagoras's theorem enables construction of incommensurable lengths because the hypotenuse of a triangle is related to the sides by the square root operation.
The figure on the right shows how to construct line segments whose lengths are in the ratio of the square root of any positive integer. Each triangle has a side (labeled "1") that is the chosen unit for measurement. In each right triangle, Pythagoras's theorem establishes the length of the hypotenuse in terms of this unit. If a hypotenuse is related to the unit by the square root of a positive integer that is not a perfect square, it is a realization of a length incommensurable with the unit, such as &#8730; 2, &#8730; 3, &#8730; 5 . For more detail, see Quadratic irrational.
Incommensurable lengths conflicted with the Pythagorean school's concept of numbers as only whole numbers. The Pythagorean school dealt with proportions by comparison of integer multiples of a common subunit. According to one legend, Hippasus of Metapontum (ca. 470 B.C.) was drowned at sea for making known the existence of the irrational or incommensurable.
=== Complex numbers ===
For any complex number
the absolute value or modulus is given by
So the three quantities, r, x and y are related by the Pythagorean equation,
Note that r is defined to be a positive number or zero but x and y can be negative as well as positive. Geometrically r is the distance of the z from zero or the origin O in the complex plane.
This can be generalised to find the distance between two points, z1 and z2 say. The required distance is given by
so again they are related by a version of the Pythagorean equation,
=== Euclidean distance in various coordinate systems ===
The distance formula in Cartesian coordinates is derived from the Pythagorean theorem. If (x1, y1) and (x2, y2) are points in the plane, then the distance between them, also called the Euclidean distance, is given by
More generally, in Euclidean n-space, the Euclidean distance between two points, and , is defined, by generalization of the Pythagorean theorem, as:
If Cartesian coordinates are not used, for example, if polar coordinates are used in two dimensions or, in more general terms, if curvilinear coordinates are used, the formulas expressing the Euclidean distance are more complicated than the Pythagorean theorem, but can be derived from it. A typical example where the straight-line distance between two points is converted to curvilinear coordinates can be found in the applications of Legendre polynomials in physics. The formulas can be discovered by using Pythagoras's theorem with the equations relating the curvilinear coordinates to Cartesian coordinates. For example, the polar coordinates (r, &#952; ) can be introduced as:
Then two points with locations (r1, &#952; 1) and (r2, &#952; 2) are separated by a distance s:
Performing the squares and combining terms, the Pythagorean formula for distance in Cartesian coordinates produces the separation in polar coordinates as:
using the trigonometric product-to-sum formulas. This formula is the law of cosines, sometimes called the Generalized Pythagorean Theorem. From this result, for the case where the radii to the two locations are at right angles, the enclosed angle &#916; &#952; = &#960; /2, and the form corresponding to Pythagoras's theorem is regained: The Pythagorean theorem, valid for right triangles, therefore is a special case of the more general law of cosines, valid for arbitrary triangles.
=== Pythagorean trigonometric identity ===
In a right triangle with sides a, b and hypotenuse c, trigonometry determines the sine and cosine of the angle &#952; between side a and the hypotenuse as:
From that it follows:
where the last step applies Pythagoras's theorem. This relation between sine and cosine sometimes is called the fundamental Pythagorean trigonometric identity. In similar triangles, the ratios of the sides are the same regardless of the size of the triangles, and depend upon the angles. Consequently, in the figure, the triangle with hypotenuse of unit size has opposite side of size sin &#8201; &#952; and adjacent side of size cos &#8201; &#952; in units of the hypotenuse.
=== Relation to the cross product ===
The Pythagorean theorem relates the cross product and dot product in a similar way:
This can be seen from the definitions of the cross product and dot product, as
with n a unit vector normal to both a and b. The relationship follows from these definitions and the Pythagorean trigonometric identity.
This can also be used to define the cross product. By rearranging the following equation is obtained
This can be considered as a condition on the cross product and so part of its definition, for example in seven dimensions.
== Generalizations ==
=== Similar figures on the three sides ===
A generalization of the Pythagorean theorem extending beyond the areas of squares on the three sides to similar figures was known by Hippocrates of Chios in the fifth century BC, and was included by Euclid in his Elements:
If one erects similar figures (see Euclidean geometry) with corresponding sides on the sides of a right triangle, then the sum of the areas of the ones on the two smaller sides equals the area of the one on the larger side.
This extension assumes that the sides of the original triangle are the corresponding sides of the three congruent figures (so the common ratios of sides between the similar figures are a:b:c). While Euclid's proof only applied to convex polygons, the theorem also applies to concave polygons and even to similar figures that have curved boundaries (but still with part of a figure's boundary being the side of the original triangle).
The basic idea behind this generalization is that the area of a plane figure is proportional to the square of any linear dimension, and in particular is proportional to the square of the length of any side. Thus, if similar figures with areas A, B and C are erected on sides with corresponding lengths a, b and c then:
But, by the Pythagorean theorem, a2 + b2 = c2, so A + B = C.
Conversely, if we can prove that A + B = C for three similar figures without using the Pythagorean theorem, then we can work backwards to construct a proof of the theorem. For example, the starting center triangle can be replicated and used as a triangle C on its hypotenuse, and two similar right triangles (A and B ) constructed on the other two sides, formed by dividing the central triangle by its altitude. The sum of the areas of the two smaller triangles therefore is that of the third, thus A + B = C and reversing the above logic leads to the Pythagorean theorem a2 + b2 = c2.
=== Law of cosines ===
The Pythagorean theorem is a special case of the more general theorem relating the lengths of sides in any triangle, the law of cosines:
where &#952; is the angle between sides a and b.
When &#952; is 90 degrees, then cos &#952; = 0, and the formula reduces to the usual Pythagorean theorem.
=== Arbitrary triangle ===
At any selected angle of a general triangle of sides a, b, c, inscribe an isosceles triangle such that the equal angles at its base &#952; are the same as the selected angle. Suppose the selected angle &#952; is opposite the side labeled c. Inscribing the isosceles triangle forms triangle ABD with angle &#952; opposite side a and with side r along c. A second triangle is formed with angle &#952; opposite side b and a side with length s along c, as shown in the figure. T &#226; bit ibn Qorra stated that the sides of the three triangles were related as:
As the angle &#952; approaches &#960; /2, the base of the isosceles triangle narrows, and lengths r and s overlap less and less. When &#952; = &#960; /2, ADB becomes a right triangle, r + s = c, and the original Pythagoras's theorem is regained.
One proof observes that triangle ABC has the same angles as triangle ABD, but in opposite order. (The two triangles share the angle at vertex B, both contain the angle &#952; , and so also have the same third angle by the triangle postulate.) Consequently, ABC is similar to the reflection of ABD, the triangle DBA in the lower panel. Taking the ratio of sides opposite and adjacent to &#952; ,
Likewise, for the reflection of the other triangle,
Clearing fractions and adding these two relations:
the required result.
=== General triangles using parallelograms ===
A further generalization applies to triangles that are not right triangles, using parallelograms on the three sides in place of squares. (Squares are a special case, of course.) The upper figure shows that for a scalene triangle, the area of the parallelogram on the longest side is the sum of the areas of the parallelograms on the other two sides, provided the parallelogram on the long side is constructed as indicated (the dimensions labeled with arrows are the same, and determine the sides of the bottom parallelogram). This replacement of squares with parallelograms bears a clear resemblance to the original Pythagoras's theorem, and was considered a generalization by Pappus of Alexandria in 4 A.D.
The lower figure shows the elements of the proof. Focus on the left side of the figure. The left green parallelogram has the same area as the left, blue portion of the bottom parallelogram because both have the same base b and height h. However, the left green parallelogram also has the same area as the left green parallelogram of the upper figure, because they have the same base (the upper left side of the triangle) and the same height normal to that side of the triangle. Repeating the argument for the right side of the figure, the bottom parallelogram has the same area as the sum of the two green parallelograms.
=== Solid geometry ===
In terms of solid geometry, Pythagoras's theorem can be applied to three dimensions as follows. Consider a rectangular solid as shown in the figure. The length of diagonal BD is found from Pythagoras's theorem as:
where these three sides form a right triangle. Using horizontal diagonal BD and the vertical edge AB, the length of diagonal AD then is found by a second application of Pythagoras's theorem as:
or, doing it all in one step:
This result is the three-dimensional expression for the magnitude of a vector v (the diagonal AD) in terms of its orthogonal components {vk} (the three mutually perpendicular sides):
This one-step formulation may be viewed as a generalization of Pythagoras's theorem to higher dimensions. However, this result is really just the repeated application of the original Pythagoras's theorem to a succession of right triangles in a sequence of orthogonal planes.
A substantial generalization of the Pythagorean theorem to three dimensions is de Gua's theorem, named for Jean Paul de Gua de Malves: If a tetrahedron has a right angle corner (like a corner of a cube), then the square of the area of the face opposite the right angle corner is the sum of the squares of the areas of the other three faces. This result can be generalized as in the "n-dimensional Pythagorean theorem":
Let be orthogonal vectors in &#8477; n. Consider the n-dimensional simplex S with vertices . (Think of the (n &#8722; 1)-dimensional simplex with vertices not including the origin as the "hypotenuse" of S and the remaining (n &#8722; 1)-dimensional faces of S as its "legs".) Then the square of the volume of the hypotenuse of S is the sum of the squares of the volumes of the n legs.
This statement is illustrated in three dimensions by the tetrahedron in the figure. The "hypotenuse" is the base of the tetrahedron at the back of the figure, and the "legs" are the three sides emanating from the vertex in the foreground. As the depth of the base from the vertex increases, the area of the "legs" increases, while that of the base is fixed. The theorem suggests that when this depth is at the value creating a right vertex, the generalization of Pythagoras's theorem applies. In a different wording:
Given an n-rectangular n-dimensional simplex, the square of the (n &#8722; 1)-content of the facet opposing the right vertex will equal the sum of the squares of the (n &#8722; 1)-contents of the remaining facets.
=== Inner product spaces ===
The Pythagorean theorem can be generalized to inner product spaces, which are generalizations of the familiar 2-dimensional and 3-dimensional Euclidean spaces. For example, a function may be considered as a vector with infinitely many components in an inner product space, as in functional analysis.
In an inner product space, the concept of perpendicularity is replaced by the concept of orthogonality: two vectors v and w are orthogonal if their inner product is zero. The inner product is a generalization of the dot product of vectors. The dot product is called the standard inner product or the Euclidean inner product. However, other inner products are possible.
The concept of length is replaced by the concept of the norm ||v|| of a vector v, defined as:
In an inner-product space, the Pythagorean theorem states that for any two orthogonal vectors v and w we have
Here the vectors v and w are akin to the sides of a right triangle with hypotenuse given by the vector sum v + w. This form of the Pythagorean theorem is a consequence of the properties of the inner product:
where the inner products of the cross terms are zero, because of orthogonality.
A further generalization of the Pythagorean theorem in an inner product space to non-orthogonal vectors is the parallelogram law :
which says that twice the sum of the squares of the lengths of the sides of a parallelogram is the sum of the squares of the lengths of the diagonals. Any norm that satisfies this equality is ipso facto a norm corresponding to an inner product.
 The Pythagorean identity can be extended to sums of more than two orthogonal vectors. If v1, v2, ..., vn are pairwise-orthogonal vectors in an inner-product space, then application of the Pythagorean theorem to successive pairs of these vectors (as described for 3-dimensions in the section on solid geometry) results in the equation
=== Non-Euclidean geometry ===
The Pythagorean theorem is derived from the axioms of Euclidean geometry, and in fact, the Pythagorean theorem given above does not hold in a non-Euclidean geometry. (The Pythagorean theorem has been shown, in fact, to be equivalent to Euclid's Parallel (Fifth) Postulate.) In other words, in non-Euclidean geometry, the relation between the sides of a triangle must necessarily take a non-Pythagorean form. For example, in spherical geometry, all three sides of the right triangle (say a, b, and c) bounding an octant of the unit sphere have length equal to &#960; /2, and all its angles are right angles, which violates the Pythagorean theorem because a2 + b2 &#8800; c2.
Here two cases of non-Euclidean geometry are considered &#8212; spherical geometry and hyperbolic plane geometry; in each case, as in the Euclidean case for non-right triangles, the result replacing the Pythagorean theorem follows from the appropriate law of cosines.
However, the Pythagorean theorem remains true in hyperbolic geometry and elliptic geometry if the condition that the triangle be right is replaced with the condition that two of the angles sum to the third, say A+B = C. The sides are then related as follows: the sum of the areas of the circles with diameters a and b equals the area of the circle with diameter c.
==== Spherical geometry ====
For any right triangle on a sphere of radius R (for example, if &#947; in the figure is a right angle), with sides a, b, c, the relation between the sides takes the form:
This equation can be derived as a special case of the spherical law of cosines that applies to all spherical triangles:
By expressing the Maclaurin series for the cosine function as an asymptotic expansion with the remainder term in big O notation,
it can be shown that as the radius R approaches infinity and the arguments a/R, b/R, and c/R tend to zero, the spherical relation between the sides of a right triangle approaches the Euclidean form of the Pythagorean theorem. Substituting the asymptotic expansion for each of the cosines into the spherical relation for a right triangle yields
The constants a4, b4, and c4 have been absorbed into the big O remainder terms since they are independent of the radius R. This asymptotic relationship can be further simplified by multiplying out the bracketed quantities, cancelling the ones, multiplying through by &#8722; 2, and collecting all of the error terms together:
After multiplying through by R2, the Euclidean Pythagorean relationship c2 = a2 + b2 is recovered in the limit as the radius R approaches infinity (since the remainder term tends to zero):
==== Hyperbolic geometry ====
For a right triangle in hyperbolic geometry with sides a, b, c and with side c opposite a right angle, the relation between the sides takes the form:
where cosh is the hyperbolic cosine. This formula is a special form of the hyperbolic law of cosines that applies to all hyperbolic triangles:
with &#947; the angle at the vertex opposite the side c.
By using the Maclaurin series for the hyperbolic cosine, cosh x &#8776; 1 + x2/2, it can be shown that as a hyperbolic triangle becomes very small (that is, as a, b, and c all approach zero), the hyperbolic relation for a right triangle approaches the form of Pythagoras's theorem.
=== Differential geometry ===
On an infinitesimal level, in three dimensional space, Pythagoras's theorem describes the distance between two infinitesimally separated points as:
with ds the element of distance and (dx, dy, dz) the components of the vector separating the two points. Such a space is called a Euclidean space. However, a generalization of this expression useful for general coordinates (not just Cartesian) and general spaces (not just Euclidean) takes the form:
where gij is called the metric tensor. It may be a function of position. Such curved spaces include Riemannian geometry as a general example. This formulation also applies to a Euclidean space when using curvilinear coordinates. For example, in polar coordinates:
== History ==
There is debate whether the Pythagorean theorem was discovered once, or many times in many places.
The history of the theorem can be divided into four parts: knowledge of Pythagorean triples, knowledge of the relationship among the sides of a right triangle, knowledge of the relationships among adjacent angles, and proofs of the theorem within some deductive system.
Bartel Leendert van der Waerden (1903 &#8211; 1996) conjectured that Pythagorean triples were discovered algebraically by the Babylonians. Written between 2000 and 1786 BC, the Middle Kingdom Egyptian Berlin Papyrus 6619 includes a problem whose solution is the Pythagorean triple 6:8:10, but the problem does not mention a triangle. The Mesopotamian tablet Plimpton 322, written between 1790 and 1750 BC during the reign of Hammurabi the Great, contains many entries closely related to Pythagorean triples.
In India, the Baudhayana Sulba Sutra, the dates of which are given variously as between the 8th century BC and the 2nd century BC, contains a list of Pythagorean triples discovered algebraically, a statement of the Pythagorean theorem, and a geometrical proof of the Pythagorean theorem for an isosceles right triangle. The Apastamba Sulba Sutra (ca. 600 BC) contains a numerical proof of the general Pythagorean theorem, using an area computation. Van der Waerden believed that "it was certainly based on earlier traditions". Boyer (1991) thinks the elements found in the &#346; ulba-s &#361; tram may be of Mesopotamian derivation.
With contents known much earlier, but in surviving texts dating from roughly the first century BC, the Chinese text Zhou Bi Suan Jing ( &#21608; &#39616; &#31639; &#32463; ), (The Arithmetical Classic of the Gnomon and the Circular Paths of Heaven) gives a reasoning for the Pythagorean theorem for the (3, 4, 5) triangle &#8212; in China it is called the "Gougu Theorem" ( &#21246; &#32929; &#23450; &#29702; ). During the Han Dynasty (202 BC to 220 AD), Pythagorean triples appear in The Nine Chapters on the Mathematical Art, together with a mention of right triangles. Some believe the theorem arose first in China, where it is alternatively known as the "Shang Gao Theorem" ( &#21830; &#39640; &#23450; &#29702; ), named after the Duke of Zhou's astronomer and mathematician, whose reasoning composed most of what was in the Zhou Bi Suan Jing.
Pythagoras, whose dates are commonly given as 569 &#8211; 475 BC, used algebraic methods to construct Pythagorean triples, according to Proclus's commentary on Euclid. Proclus, however, wrote between 410 and 485 AD. According to Thomas L. Heath (1861 &#8211; 1940), no specific attribution of the theorem to Pythagoras exists in the surviving Greek literature from the five centuries after Pythagoras lived. However, when authors such as Plutarch and Cicero attributed the theorem to Pythagoras, they did so in a way which suggests that the attribution was widely known and undoubted. "Whether this formula is rightly attributed to Pythagoras personally, [...] one can safely assume that it belongs to the very oldest period of Pythagorean mathematics."
Around 400 BC, according to Proclus, Plato gave a method for finding Pythagorean triples that combined algebra and geometry. Around 300 BC, in Euclid's Elements, the oldest extant axiomatic proof of the theorem is presented.
== In popular culture ==
The Pythagorean theorem has arisen in popular culture in a variety of ways.
Hans Christian Andersen wrote in 1831 a poem about the Pythagorean theorem: Formens Evige Magie (Et poetisk Spilf &#230; gteri).
A verse of the Major-General's Song in the Gilbert and Sullivan comic opera The Pirates of Penzance, "About binomial theorem I'm teeming with a lot o' news, With many cheerful facts about the square of the hypotenuse", makes an oblique reference to the theorem.
The Scarecrow in the film The Wizard of Oz makes a more specific reference to the theorem. Upon receiving his diploma from the Wizard, he immediately exhibits his "knowledge" by reciting a mangled and incorrect version of the theorem: "The sum of the square roots of any two sides of an isosceles triangle is equal to the square root of the remaining side. Oh, joy! Oh, rapture! I've got a brain!"
In 2000, Uganda released a coin with the shape of an isosceles right triangle. The coin's tail has an image of Pythagoras and the equation &#945; 2 + &#946; 2 = &#947; 2, accompanied with the mention "PYTHAGORAS MILLENNIUM".
Greece, Japan, San Marino, Sierra Leone, and Suriname have issued postage stamps depicting Pythagoras and the Pythagorean theorem.
In Neal Stephenson's speculative fiction Anathem, the Pythagorean theorem is referred to as 'the Adrakhonic theorem'. A geometric proof of the theorem is displayed on the side of an alien ship to demonstrate the aliens' understanding of mathematics.
== See also ==
British flag theorem
Dulcarnon
Fermat's Last Theorem
Linear algebra
List of triangle topics
Lp space
Nonhypotenuse number
Parallelogram law
Ptolemy's theorem
Pythagorean expectation
Pythagorean tiling
Rational trigonometry#Pythagoras's theorem
== Notes ==
== References ==
== External links ==
Pythagorean Theorem (more than 70 proofs from cut-the-knot)
Interactive links:
Interactive proof in Java of The Pythagorean Theorem
Another interactive proof in Java of The Pythagorean Theorem
Pythagorean theorem with interactive animation
Animated, Non-Algebraic, and User-Paced Pythagorean Theorem
History topic: Pythagoras's theorem in Babylonian mathematics
Hazewinkel, Michiel, ed. (2001), "Pythagorean theorem", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Pythagorean theorem", MathWorld.
Euclid (David E. Joyce, ed. 1997) [c. 300 BC]. Elements. Retrieved 2006-08-30. In HTML with Java-based interactive figures.
PAGE	Quadratic equation	'Elementary algebra'	'Equations'	'Polynomials'
In elementary algebra, a quadratic equation (from the Latin quadratus for "square") is any equation having the form
where x represents an unknown, and a, b, and c represent numbers such that a is not equal to 0. If a = 0, then the equation is linear, not quadratic. The numbers a, b, and c are the coefficients of the equation, and may be distinguished by calling them, respectively, the quadratic coefficient, the linear coefficient and the constant or free term.
Because the quadratic equation involves only one unknown, it is called "univariate". The quadratic equation only contains powers of x that are non-negative integers, and therefore it is a polynomial equation, and in particular it is a second degree polynomial equation since the greatest power is two.
Quadratic equations can be solved by factoring, by completing the square, by using the quadratic formula, or by graphing. Solutions to problems equivalent to the quadratic equation were known as early as 2000 BC.
== Solving the quadratic equation ==
A quadratic equation with real or complex coefficients has two solutions, called roots. These two solutions may or may not be distinct, and they may or may not be real.
=== Factoring by inspection ===
It may be possible to express a quadratic equation ax2 + bx + c = 0 as a product (px + q)(rx + s) = 0. In some cases, it is possible, by simple inspection, to determine values of p, q, r, and s that make the two forms equivalent to one another. If the quadratic equation is written in the second form, then the "Zero Factor Property" states that the quadratic equation is satisfied if px + q = 0 or rx + s = 0. Solving these two linear equations provides the roots of the quadratic.
For most students, factoring by inspection is the first method of solving quadratic equations to which they are exposed. If one is given a quadratic equation in the form x2 + bx + c = 0, the sought factorization has the form (x + q)(x + s), and one has to find two numbers q and s that add up to b and whose product is c (this is sometimes called "Vieta's rule" and is related to Vieta's formulas). The more general case where a does not equal 1 can require a considerable effort in trial and error guess-and-check, assuming that it can be factored at all by inspection.
Except for special cases such as where b = 0 or c = 0, factoring by inspection only works for quadratic equations that have rational roots. This means that the great majority of quadratic equations that arise in practical applications cannot be solved by factoring by inspection.
=== Completing the square ===
The process of completing the square makes use of the algebraic identity
which represents a well-defined algorithm that can be used to solve any quadratic equation. Starting with a quadratic equation in standard form, ax2 + bx + c = 0
Divide each side by a, the coefficient of the squared term.
Rearrange the equation so that the constant term c/a is on the right side.
Add the square of one-half of b/a, the coefficient of x, to both sides. This "completes the square", converting the left side into a perfect square.
Write the left side as a square and simplify the right side if necessary.
Produce two linear equations by equating the square root of the left side with the positive and negative square roots of the right side.
Solve the two linear equations.
We illustrate use of this algorithm by solving 2x2 + 4x &#8722; 4 = 0
The plus-minus symbol " &#177; " indicates that both x = &#8722; 1 + &#8730; 3 and x = &#8722; 1 &#8722; &#8730; 3 are solutions of the quadratic equation.
=== Quadratic formula and its derivation ===
Completing the square can be used to derive a general formula for solving quadratic equations, called the quadratic formula. The mathematical proof will now be briefly summarized. It can easily be seen, by polynomial expansion, that the following equation is equivalent to the quadratic equation:
Taking the square root of both sides, and isolating x, gives:
Some sources, particularly older ones, use alternative parameterizations of the quadratic equation such as ax2 &#8722; 2bx + c = 0 , where b has a magnitude one half of the more common one, possibly with opposite sign. These result in slightly different forms for the solution, but are otherwise equivalent.
A number of alternative derivations can be found in the literature. These proofs are simpler than the standard completing the square method, represent interesting applications of other frequently used techniques in algebra, or offer insight into other areas of mathematics.
=== Reduced quadratic equation ===
It is sometimes convenient to reduce a quadratic equation to an equation involving two instead of three constant coefficients. This is done by simply dividing both sides by a, which is possible because a is non-zero. This produces the reduced quadratic equation:
Here p = b/a and q = c/a are the only coefficients in the reduced equation, which is also called a monic equation.
It follows from the quadratic formula that the solution to the reduced quadratic equation is
=== Discriminant ===
In the quadratic formula, the expression underneath the square root sign is called the discriminant of the quadratic equation, and is often represented using an upper case D or an upper case Greek delta:
A quadratic equation with real coefficients can have either one or two distinct real roots, or two distinct complex roots. In this case the discriminant determines the number and nature of the roots. There are three cases:
If the discriminant is positive, then there are two distinct roots
both of which are real numbers. For quadratic equations with rational coefficients, if the discriminant is a square number, then the roots are rational &#8212; in other cases they may be quadratic irrationals.
If the discriminant is zero, then there is exactly one real root
sometimes called a repeated or double root.
If the discriminant is negative, then there are no real roots. Rather, there are two distinct (non-real) complex roots
which are complex conjugates of each other. In these expressions i is the imaginary unit.
Thus the roots are distinct if and only if the discriminant is non-zero, and the roots are real if and only if the discriminant is non-negative.
=== Geometric interpretation ===
The function f(x) = ax2 + bx + c is the quadratic function. The graph of any quadratic function has the same general shape, which is called a parabola. The location and size of the parabola, and how it opens, depends on the values of a, b, and c. As shown in Figure 1, if a > 0, the parabola has a minimum point and opens upward. If a < 0, the parabola has a maximum point and opens downward. The extreme point of the parabola, whether minimum or maximum, corresponds to its vertex. The x-coordinate of the vertex will be located at , and the y-coordinate of the vertex may be found by substituting this x-value into the function. The y-intercept is located at the point (0, c).
The solutions of the quadratic equation ax2 + bx + c = 0 correspond to the roots of the function f(x) = ax2 + bx + c, since they are the values of x for which f(x) = 0. As shown in Figure 2, if a, b, and c are real numbers and the domain of f is the set of real numbers, then the roots of f are exactly the x-coordinates of the points where the graph touches the x-axis. As shown in Figure 3, if the discriminant is positive, the graph touches the x-axis at two points; if zero, the graph touches at one point; and if negative, the graph does not touch the x-axis.
=== Quadratic factorization ===
The term
is a factor of the polynomial
if and only if r is a root of the quadratic equation
It follows from the quadratic formula that
In the special case b2 = 4ac where the quadratic has only one distinct root (i.e. the discriminant is zero), the quadratic polynomial can be factored as
=== Graphing for real roots ===
For most of the 20th century, graphing was rarely mentioned as a method for solving quadratic equations in high school or college algebra texts. Students learned to solve quadratic equations by factoring, completing the square, and applying the quadratic formula. Recently, graphing calculators have become common in schools and graphical methods have started to appear in textbooks, but they are generally not highly emphasized.
Being able to use a graphing calculator to solve a quadratic equation requires the ability to produce a graph of y = f(x), the ability to scale the graph appropriately to the dimensions of the graphing surface, and the recognition that when f(x) = 0, x is a solution to the equation. The skills required to solve a quadratic equation on a calculator are in fact applicable to finding the real roots of any arbitrary function.
Since an arbitrary function may cross the x-axis at multiple points, graphing calculators generally require one to identify the desired root by positioning a cursor at a "guessed" value for the root. (Some graphing calculators require bracketing the root on both sides of the zero.) The calculator then proceeds, by an iterative algorithm, to refine the estimated position of the root to the limit of calculator accuracy.
=== Avoiding loss of significance ===
Although the quadratic formula provides what in principle should be an exact solution, it does not, from a numerical analysis standpoint, provide a completely stable method for evaluating the roots of a quadratic equation. If the two roots of the quadratic equation vary greatly in absolute magnitude, b will be very close in magnitude to , and the subtraction of two nearly equal numbers will cause loss of significance or catastrophic cancellation. A second form of cancellation can occur between the terms b2 and &#8722; 4ac of the discriminant, which can lead to loss of up to half of correct significant figures.
== History ==
Babylonian mathematicians, as early as 2000 BC (displayed on Old Babylonian clay tablets) could solve problems relating the areas and sides of rectangles. There is evidence dating this algorithm as far back as the Third Dynasty of Ur. In modern notation, the problems typically involved solving a pair of simultaneous equations of the form:
which are equivalent to the equation:
The steps given by Babylonian scribes for solving the above rectangle problem were as follows:
Compute half of p.
Square the result.
Subtract q.
Find the square root using a table of squares.
Add together the results of steps (1) and (4) to give x. This is essentially equivalent to calculating 
Geometric methods were used to solve quadratic equations in Babylonia, Egypt, Greece, China, and India. The Egyptian Berlin Papyrus, dating back to the Middle Kingdom (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation. In the Indian Sulba Sutras, circa 8th century BC, quadratic equations of the form ax2 = c and ax2 + bx = c were explored using geometric methods. Babylonian mathematicians from circa 400 BC and Chinese mathematicians from circa 200 BC used geometric methods of dissection to solve quadratic equations with positive roots. Rules for quadratic equations were given in the The Nine Chapters on the Mathematical Art, a Chinese treatise on mathematics. These early geometric methods do not appear to have had a general formula. Euclid, the Greek mathematician, produced a more abstract geometrical method around 300 BC. With a purely geometric approach Pythagoras and Euclid created a general procedure to find solutions of the quadratic equation. In his work Arithmetica, the Greek mathematician Diophantus solved the quadratic equation, but giving only one root, even when both roots were positive.
In 628 AD, Brahmagupta, an Indian mathematician, gave the first explicit (although still not completely general) solution of the quadratic equation ax2 + bx = c as follows: "To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value." (Brahmasphutasiddhanta, Colebrook translation, 1817, page 346) This is equivalent to:
The Bakhshali Manuscript written in India in the 7th century AD contained an algebraic formula for solving quadratic equations, as well as quadratic indeterminate equations (originally of type ax/c = y Muhammad ibn Musa al-Khwarizmi (Persia, 9th century), inspired by Brahmagupta, developed a set of formulas that worked for positive solutions. Al-Khwarizmi goes further in providing a full solution to the general quadratic equation, accepting one or two numerical answers for every quadratic equation, while providing geometric proofs in the process. He also described the method of completing the square and recognized that the discriminant must be positive, which was proven by his contemporary 'Abd al-Ham &#299; d ibn Turk (Central Asia, 9th century) who gave geometric figures to prove that if the discriminant is negative, a quadratic equation has no solution. While al-Khwarizmi himself did not accept negative solutions, later Islamic mathematicians that succeeded him accepted negative solutions, as well as irrational numbers as solutions. Ab &#363; K &#257; mil Shuj &#257; ibn Aslam (Egypt, 10th century) in particular was the first to accept irrational numbers (often in the form of a square root, cube root or fourth root) as solutions to quadratic equations or as coefficients in an equation. The 9th century Indian mathematician Sridhara wrote down rules for solving quadratic equations.
The Jewish mathematician Abraham bar Hiyya Ha-Nasi (12th century, Spain) authored the first European book to include the full solution to the general quadratic equation. His solution was largely based on Al-Khwarizmi's work. The writing of the Chinese mathematician Yang Hui (1238 &#8211; 1298 AD) is the first known one in which quadratic equations with negative coefficients of 'x' appear, although he attributes this to the earlier Liu Yi. By 1545 Gerolamo Cardano compiled the works related to the quadratic equations. The quadratic formula covering all cases was first obtained by Simon Stevin in 1594. In 1637 Ren &#233; Descartes published La G &#233; om &#233; trie containing the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by Henry Heaton.
== Advanced topics ==
=== Alternative methods of root calculation ===
==== Vieta's formulas ====
Vieta's formulas give a simple relation between the roots of a polynomial and its coefficients. In the case of the quadratic polynomial, they take the following form:
and
These results follow immediately from the relation:
which can be compared term by term with
The first formula above yields a convenient expression when graphing a quadratic function. Since the graph is symmetric with respect to a vertical line through the vertex, when there are two real roots the vertex's x-coordinate is located at the average of the roots (or intercepts). Thus the x-coordinate of the vertex is given by the expression
The y-coordinate can be obtained by substituting the above result into the given quadratic equation, giving
As a practical matter, Vieta's formulas provide a useful method for finding the roots of a quadratic in the case where one root is much smaller than the other. If | &#8239; x &#8201; 2| << | &#8239; x &#8201; 1|, then x &#8201; 1 + x &#8201; 2 &#8776; x &#8201; 1, and we have the estimate:
The second Vieta's formula then provides:
These formulas are much easier to evaluate than the quadratic formula under the condition of one large and one small root, because the quadratic formula evaluates the small root as the difference of two very nearly equal numbers (the case of large b), which causes round-off error in a numerical evaluation. Figure 5 shows the difference between (i) a direct evaluation using the quadratic formula (accurate when the roots are near each other in value) and (ii) an evaluation based upon the above approximation of Vieta's formulas (accurate when the roots are widely spaced). As the linear coefficient b increases, initially the quadratic formula is accurate, and the approximate formula improves in accuracy, leading to a smaller difference between the methods as b increases. However, at some point the quadratic formula begins to lose accuracy because of round off error, while the approximate method continues to improve. Consequently the difference between the methods begins to increase as the quadratic formula becomes worse and worse.
This situation arises commonly in amplifier design, where widely separated roots are desired to ensure a stable operation (see step response).
==== Trigonometric solution ====
In the days before calculators, people would use mathematical tables &#8212; lists of numbers showing the results of calculation with varying arguments &#8212; to simplify and speed up computation. Tables of logarithms and trigonometric functions were common in math and science textbooks. Specialized tables were published for applications such as astronomy, celestial navigation and statistics. Methods of numerical approximation existed, called prosthaphaeresis, that offered shortcuts around time-consuming operations such as multiplication and taking powers and roots. Astronomers, especially, were concerned with methods that could speed up the long series of computations involved in celestial mechanics calculations.
It is within this context that we may understand the development of means of solving quadratic equations by the aid of trigonometric substitution. Consider the following alternate form of the quadratic equation,
[1] 
where the sign of the &#177; symbol is chosen so that a and c may both be positive. By substituting
[2] 
and then multiplying through by cos2 &#952; , we obtain
[3] 
Introducing functions of 2 &#952; and rearranging, we obtain
[4] 
[5] 
where the subscripts n and p correspond, respectively, to the use of a negative or positive sign in equation [1]. Substituting the two values of &#952; n or &#952; p found from equations [4] or [5] into [2] gives the required roots of [1]. Complex roots occur in the solution based on equation [5] if the absolute value of sin 2 &#952; p exceeds unity. The amount of effort involved in solving quadratic equations using this mixed trigonometric and logarithmic table look-up strategy was two-thirds the effort using logarithmic tables alone. Calculating complex roots would require using a different trigonometric form.
To illustrate, let us assume we had available seven-place logarithm and trigonometric tables, and wished to solve the following to six-significant-figure accuracy:
A seven-place lookup table might have only 100,000 entries, and computing intermediate results to seven places would generally require interpolation between adjacent entries.
 (rounded to six significant figures)
==== Geometric solution ====
The quadratic equation may be solved geometrically in a number of ways. One way is via Lill's method. The three coefficients a, b, c are drawn with right angles between them as in SA, AB, and BC in Figure 6. A circle is drawn with the start and end point SC as a diameter. If this cuts the middle line AB of the three then the equation has a solution, and the solutions are given by negative of the distance along this line from A divided by the first coefficient a or SA. If a is 1 the coefficients may be read off directly. Thus the solutions in the diagram are &#8722; AX1/SA and &#8722; AX2/SA.
=== Generalization of quadratic equation ===
The formula and its derivation remain correct if the coefficients a, b and c are complex numbers, or more generally members of any field whose characteristic is not 2. (In a field of characteristic 2, the element 2a is zero and it is impossible to divide by it.)
The symbol
in the formula should be understood as "either of the two elements whose square is b2 &#8722; 4ac, if such elements exist". In some fields, some elements have no square roots and some have two; only zero has just one square root, except in fields of characteristic 2. Even if a field does not contain a square root of some number, there is always a quadratic extension field which does, so the quadratic formula will always make sense as a formula in that extension field.
==== Characteristic 2 ====
In a field of characteristic 2, the quadratic formula, which relies on 2 being a unit, does not hold. Consider the monic quadratic polynomial
over a field of characteristic 2. If b = 0, then the solution reduces to extracting a square root, so the solution is
and there is only one root since
In summary,
See quadratic residue for more information about extracting square roots in finite fields.
In the case that b &#8800; 0, there are two distinct roots, but if the polynomial is irreducible, they cannot be expressed in terms of square roots of numbers in the coefficient field. Instead, define the 2-root R(c) of c to be a root of the polynomial x2 + x + c, an element of the splitting field of that polynomial. One verifies that R(c) + 1 is also a root. In terms of the 2-root operation, the two roots of the (non-monic) quadratic ax2 + bx + c are
and
For example, let a denote a multiplicative generator of the group of units of F4, the Galois field of order four (thus a and a + 1 are roots of x2 + x + 1 over F4. Because (a + 1)2 = a, a + 1 is the unique solution of the quadratic equation x2 + a = 0. On the other hand, the polynomial x2 + ax + 1 is irreducible over F4, but it splits over F16, where it has the two roots ab and ab + a, where b is a root of x2 + x + a in F16.
This is a special case of Artin &#8211; Schreier theory.
== See also ==
== References ==
== External links ==
Hazewinkel, Michiel, ed. (2001), "Quadratic equation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Quadratic equations", MathWorld.
101 uses of a quadratic equation
101 uses of a quadratic equation: Part II
Step-by-step instructions on using the quadratic formula for any input
PAGE	Quadratic formula	'Elementary algebra'	'Equations'	'Polynomials'
In basic algebra, the quadratic formula is the solution of the quadratic equation. There are other ways to solve the quadratic equation instead of using the quadratic formula, such as factoring, completing the square, or graphing. Using the quadratic formula is often the most convenient way.
The general quadratic equation is
Here x represents an unknown, and a, b, and c are constants with a not equal to 0. One can verify that the quadratic formula satisfies the quadratic equation, by inserting the former into the latter. Each of the solutions given by the quadratic formula is called a root of the quadratic equation.
== Derivation of the formula ==
Once a student understands how to complete the square, they can then derive the quadratic formula. For that reason, the derivation is sometimes left as an exercise for the student, who can thereby experience rediscovery of this important formula. The explicit derivation is as follows.
Divide the quadratic equation by a, which is allowed because a is non-zero:
Subtract c/a from both sides of the equation, transforming it into the form
The quadratic equation is now in a form to which the method of completing the square can be applied. To "complete the square", add a constant to both sides of the equation such that the left hand side becomes a complete square:
which produces
or (after rearranging the terms on the right hand side to have a common denominator)
The square has thus been completed, as shown in the figure. Taking the square root of both sides yields
Isolating x gives the quadratic formula:
The plus-minus symbol " &#177; " indicates that both
are solutions of the quadratic equation. There are many alternatives of this derivation with minor differences, mostly concerning the manipulation of .
Some sources, particularly older ones, use alternative parameterizations of the quadratic equation such as or , where b has a magnitude one half of the more common one. These result in slightly different forms for the solution, but are otherwise equivalent.
== Historical development ==
The earliest methods for solving quadratic equations were geometric. Babylonian cuneiform tablets contain problems reducible to solving quadratic equations. The Egyptian Berlin Papyrus, dating back to the Middle Kingdom (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation.
The Greek mathematician Euclid (circa 300 BC) used geometric methods to solve quadratic equations in Book 2 of his Elements, an influential mathematical treatise. Rules for quadratic equations appear in the Chinese The Nine Chapters on the Mathematical Art circa 200 BC. In his work Arithmetica, the Greek mathematician Diophantus solved quadratic equations with a method more recognizably algebraic than the geometric algebra of Euclid. His solution gives only one root, even when both roots are positive. The Indian mathematician Brahmagupta (597 &#8211; 668 AD) explicitly described the quadratic formula in his treatise Br &#257; hmasphu &#7789; asiddh &#257; nta published in 628 AD, but written in words instead of symbols. His solution of the quadratic equation was as follows: "To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value." This is equivalent to:
The 9th century Persian mathematician al-Khw &#257; rizm &#299; , influenced by earlier Greek and Indian mathematicians, solved quadratic equations algebraically. The quadratic formula covering all cases was first obtained by Simon Stevin in 1594. In 1637 Ren &#233; Descartes published La G &#233; om &#233; trie containing the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by Henry Heaton.
== Importance of this solution ==
Among the many equations that one encounters while studying algebra, the quadratic formula is one of the most important, and is considered the most useful method of solving quadratic equations. Unlike some other solution methods such as factoring, the quadratic formula can be used to solve any quadratic equation. Many equations that do not initially appear to be quadratic can be put into quadratic form, and solved using the quadratic formula. For these reasons, it is often memorized.
Completing the square also allows for the solution of all quadratics, as it is mathematically equivalent, but the quadratic formula gives a result without the need for so much algebraic manipulation. As such, it is generally considered more practical to use the formula. Completing the square is very useful for other purposes, such as putting the equations for conic sections into standard form.
== Other derivations ==
A number of alternative derivations of the quadratic formula can be found in the literature. These derivations either (a) are simpler than the standard completing the square method, (b) represent interesting applications of other frequently used techniques in algebra, or (c) offer insight into other areas of mathematics.
=== Alternate method of completing the square ===
The great majority of algebra texts published over the last several decades teach completing the square using the sequence presented earlier: (1) divide each side by a, (2) rearrange, (3) then add the square of one-half of b/a.
As pointed out by Larry Hoehn in 1975, completing the square can be accomplished by a different sequence that leads to a simpler sequence of intermediate terms: (1) multiply each side by 4a, (2) rearrange, (3) then add .
In other words, the quadratic formula can be derived as follows:
This actually represents an ancient derivation of the quadratic formula, and was known to the Hindus at least as far back as 1025 AD. Compared with the derivation in standard usage, this alternate derivation is shorter, involves fewer computations with literal coefficients, avoids fractions until the last step, has simpler expressions, and uses simpler math. As Hoehn states, "it is easier 'to add the square of b' than it is 'to add the square of half the coefficient of the x term'".
=== By substitution ===
Another technique is solution by substitution. In this technique, we substitute into the quadratic to get:
Expanding the result and then collecting the powers of produces:
We have not yet imposed a second condition on and , so we now choose m so that the middle term vanishes. That is, or . Subtracting the constant term from both sides of the equation (to move it to the right hand side) and then dividing by a gives:
Substituting for gives:
Therefore ; substituting provides the quadratic formula.
=== By using algebraic identities ===
Let the roots of the standard quadratic equation be and . At this point, we recall the identity:
Taking square root on both sides, we get
Since the coefficient a &#8800; 0, we can divide the standard equation by a to obtain a quadratic polynomial having the same roots. Namely,
From this we can see that the sum of the roots of the standard quadratic equation is given by , and the product of those roots is given by 
Hence the identity can be rewritten as:
Now,
Since, , if we take then we obtain and if we instead take then we calculate that Combining these results by using the standard shorthand, we have that the solutions of the quadratic equation are given by:
=== By Lagrange resolvents ===
An alternative way of deriving the quadratic formula is via the method of Lagrange resolvents, which is an early part of Galois theory. This method can be generalized to give the roots of cubic polynomials and quartic polynomials, and leads to Galois theory, which allows one to understand the solution of algebraic equations of any degree in terms of the symmetry group of their roots, the Galois group.
This approach focuses on the roots more than on rearranging the original equation. Given a monic quadratic polynomial
assume that it factors as
Expanding yields
where and .
Since the order of multiplication does not matter, one can switch and and the values of p and q will not change: one says that p and q are symmetric polynomials in and . In fact, they are the elementary symmetric polynomials &#8211; any symmetric polynomial in and can be expressed in terms of and The Galois theory approach to analyzing and solving polynomials is: given the coefficients of a polynomial, which are symmetric functions in the roots, can one "break the symmetry" and recover the roots? Thus solving a polynomial of degree n is related to the ways of rearranging ("permuting") n terms, which is called the symmetric group on n letters, and denoted For the quadratic polynomial, the only way to rearrange two terms is to swap them ("transpose" them), and thus solving a quadratic polynomial is simple.
To find the roots and consider their sum and difference:
These are called the Lagrange resolvents of the polynomial; notice that one of these depends on the order of the roots, which is the key point. One can recover the roots from the resolvents by inverting the above equations:
Thus, solving for the resolvents gives the original roots.
Formally, the resolvents are called the discrete Fourier transform (DFT) of order 2, and the transform can be expressed by the matrix with inverse matrix The transform matrix is also called the DFT matrix or Vandermonde matrix.
Now is a symmetric function in and so it can be expressed in terms of p and q, and in fact as noted above. But is not symmetric, since switching and yields (formally, this is termed a group action of the symmetric group of the roots). Since is not symmetric, it cannot be expressed in terms of the polynomials p and q, as these are symmetric in the roots and thus so is any polynomial expression involving them. Changing the order of the roots only changes by a factor of and thus the square is symmetric in the roots, and thus expressible in terms of p and q. Using the equation
yields
and thus
If one takes the positive root, breaking symmetry, one obtains:
and thus
Thus the roots are
which is the quadratic formula. Substituting yields the usual form for when a quadratic is not monic. The resolvents can be recognized as being the vertex, and is the discriminant (of a monic polynomial).
A similar but more complicated method works for cubic equations, where one has three resolvents and a quadratic equation (the "resolving polynomial") relating and which one can solve by the quadratic equation, and similarly for a quartic (degree 4) equation, whose resolving polynomial is a cubic, which can in turn be solved. The same method for a quintic equation yields a polynomial of degree 24, which does not simplify the problem, and in fact solutions to quintic equations in general cannot be expressed using only roots.
== See also ==
Discriminant
Fundamental theorem of algebra
== References ==
== External links ==
Quadratic formula calculator
Quadratic formula calculator Online
Alternative formula (Wolfram)
PAGE	Quadratic function	'Polynomials'
In mathematics, a quadratic function, a quadratic polynomial, a polynomial of degree 2, or simply a quadratic, is a polynomial function in one or more variables in which the highest-degree term is of the second degree. For example, a quadratic function in three variables x, y, and z contains exclusively terms x2, y2, z2, xy, xz, yz, x, y, z, and a constant:
with at least one of the coefficients a, b, c, d, e, or f of the second-degree terms being non-zero.
A univariate (single-variable) quadratic function has the form
in the single variable x.The graph of a univariate quadratic function is a parabola whose axis of symmetry is parallel to the y-axis, as shown at right.
If the quadratic function is set equal to zero, then the result is a quadratic equation. The solutions to the univariate equation are called the roots of the univariate function.
The bivariate case in terms of variables x and y has the form
with at least one of a, b, c not equal to zero, and an equation setting this function equal to zero gives rise to a conic section (a circle or other ellipse, a parabola, or a hyperbola).
In general there can be an arbitrarily large number of variables, in which case the resulting surface is called a quadric, but the highest degree term must be of degree 2, such as x2, xy, yz, etc.
== Origin of word ==
The adjective quadratic comes from the Latin word quadr &#257; tum ("square"). A term like x2 is called a square in algebra because it is the area of a square with side x.
In general, a prefix quadr(i)- indicates the number 4. Examples are quadrilateral and quadrant. Quadratum is the Latin word for square because a square has four sides.
== Terminology ==
=== Coefficients ===
The coefficients of a polynomial are often taken to be real or complex numbers, but in fact, a polynomial may be defined over any ring.
=== Degree ===
When using the term "quadratic polynomial", authors sometimes mean "having degree exactly 2", and sometimes "having degree at most 2". If the degree is less than 2, this may be called a "degenerate case". Usually the context will establish which of the two is meant.
Sometimes the word "order" is used with the meaning of "degree", e.g. a second-order polynomial.
=== Variables ===
A quadratic polynomial may involve a single variable x (the univariate case), or multiple variables such as x, y, and z (the multivariate case).
==== The one-variable case ====
Any single-variable quadratic polynomial may be written as
where x is the variable, and a, b, and c represent the coefficients. In elementary algebra, such polynomials often arise in the form of a quadratic equation . The solutions to this equation are called the roots of the quadratic polynomial, and may be found through factorization, completing the square, graphing, Newton's method, or through the use of the quadratic formula. Each quadratic polynomial has an associated quadratic function, whose graph is a parabola.
==== Bivariate case ====
Any quadratic polynomial with two variables may be written as
where x and y are the variables and a, b, c, d, e, and f are the coefficients. Such polynomials are fundamental to the study of conic sections. Similarly, quadratic polynomials with three or more variables correspond to quadric surfaces and hypersurfaces. In linear algebra, quadratic polynomials can be generalized to the notion of a quadratic form on a vector space.
== Forms of a univariate quadratic function ==
A univariate quadratic function can be expressed in three formats:
 is called the standard form,
 is called the factored form, where x1 and x2 are the roots of the quadratic function and the solutions of the corresponding quadratic equation.
 is called the vertex form, where h and k are the x and y coordinates of the vertex, respectively.
To convert the standard form to factored form, one needs only the quadratic formula to determine the two roots x1 and x2. To convert the standard form to vertex form, one needs a process called completing the square. To convert the factored form (or vertex form) to standard form, one needs to multiply, expand and/or distribute the factors.
== Graph of the univariate function ==
Regardless of the format, the graph of a univariate quadratic function f(x)=ax2+bx+c is a parabola (as shown at the right). Equivalently, this is the graph of the bivariate quadratic equation y = ax2+bx+c.
If a > 0, (or is a positive number), the parabola opens upward.
If a < 0, (or is a negative number), the parabola opens downward.
The coefficient a controls the speed of increase (or decrease) of the quadratic function from the vertex, greater positive a values makes the function increase faster and the graph appears more closed.
The coefficients b and a together control the axis of symmetry of the parabola (also the x-coordinate of the vertex) which is at .
The coefficient b alone is the declivity of the parabola as y-axis intercepts.
The coefficient c controls the height of the parabola, more specifically, it is the point where the parabola intercept the y-axis.
=== Vertex ===
The vertex of a parabola is the place where it turns; hence, it is also called the turning point. If the quadratic function is in vertex form, the vertex is (h, k). By the method of completing the square, one can turn the standard form
into
so the vertex of the parabola in standard form is
If the quadratic function is in factored form
the average of the two roots, i.e.,
is the x-coordinate of the vertex, and hence the vertex is
The vertex is also the maximum point if a < 0, or the minimum point if a > 0.
The vertical line
that passes through the vertex is also the axis of symmetry of the parabola.
==== Maximum and minimum points ====
Using calculus, the vertex point, being a maximum or minimum of the function, can be obtained by finding the roots of the derivative:
giving
with the corresponding function value
so again the vertex point coordinates can be expressed as
== Roots of the univariate function ==
The roots (zeros) of the univariate quadratic function
are the values of x for which f(x) = 0.
When the coefficients a, b, and c, are real or complex, the roots are
where the discriminant is defined as
== The square root of a univariate quadratic function ==
The square root of a univariate quadratic function gives rise to one of the four conic sections, almost always either to an ellipse or to a hyperbola.
If then the equation describes a hyperbola, as can be seen by squaring both sides. The directions of the axes of the hyperbola are determined by the ordinate of the minimum point of the corresponding parabola . If the ordinate is negative, then the hyperbola's major axis (through its vertices) is horizontal, while if the ordinate is positive then the hyperbola's major axis is vertical.
If then the equation describes either a circle or other ellipse or nothing at all. If the ordinate of the maximum point of the corresponding parabola is positive, then its square root describes an ellipse, but if the ordinate is negative then it describes an empty locus of points.
== Iteration ==
To iterate a function , one applies the function repeatedly, using the output from one iteration as the input to the next.
One cannot always deduce the analytic form of , which means the nth iteration of . (The superscript can be extended to negative numbers, referring to the iteration of the inverse of if the inverse exists.) But there are some analytically tractable cases.
For example, for the iterative equation
one has
where
 and 
So by induction,
can be obtained, where can be easily computed as
Finally, we have
as the solution.
See Topological conjugacy for more detail about the relationship between f and g. And see Complex quadratic polynomial for the chaotic behavior in the general iteration.
The logistic map
with parameter 2<r<4 can be solved in certain cases, one of which is chaotic and one of which is not. In the chaotic case r=4 the solution is
where the initial condition parameter is given by . For rational , after a finite number of iterations maps into a periodic sequence. But almost all are irrational, and, for irrational , never repeats itself &#8211; it is non-periodic and exhibits sensitive dependence on initial conditions, so it is said to be chaotic.
The solution of the logistic map when r=2 is
for . Since for any value of other than the unstable fixed point 0, the term goes to 0 as n goes to infinity, so goes to the stable fixed point 
== Bivariate (two variable) quadratic function ==
A bivariate quadratic function is a second-degree polynomial of the form
where A, B, C, D, and E are fixed coefficients and F is the constant term. Such a function describes a quadratic surface. Setting equal to zero describes the intersection of the surface with the plane , which is a locus of points equivalent to a conic section.
=== Minimum/maximum ===
If the function has no maximum or minimum, its graph forms an hyperbolic paraboloid.
If the function has a minimum if A>0, and a maximum if A<0, its graph forms an elliptic paraboloid. In this case the minimum or maximum occurs at where:
If and the function has no maximum or minimum, its graph forms a parabolic cylinder.
If and the function achieves the maximum/minimum at a line. Similarly, a minimum if A>0 and a maximum if A<0, its graph forms a parabolic cylinder.
== See also ==
Quadratic form
Quadratic equation
Matrix representation of conic sections
Quadric
Periodic points of complex quadratic mappings
List of mathematical functions
== References ==
Algebra 1, Glencoe, ISBN 0-07-825083-8
Algebra 2, Saxon, ISBN 0-939798-62-X
== External links ==
Weisstein, Eric W., "Quadratic", MathWorld.
PAGE	Quadratics	'Elementary algebra'	'Equations'	'Polynomials'
Quadratics is a six-part Canadian instructional television series produced by TVOntario in 1993. The miniseries is part of the Concepts in Mathematics series. The program uses computer animation to demonstrate quadratic equations and their corresponding functions in the Cartesian coordinate system.
== Synopsis ==
Each program involves two robots, Edie and Charon, who work on an assembly line in a high-tech factory. The robots discuss their desire to learn about quadratic equations, and they are subsequently provided with lessons that further their education.
== Episodes ==
== References ==
== External links ==
Quadratics at TV.com
PAGE	Quartic function	'Elementary algebra'	'Equations'	'Polynomials'
In mathematics, a quartic function, is a function of the form
where a is nonzero, which is defined by a polynomial of degree four, called quartic polynomial.
Sometimes the term biquadratic is used instead of quartic, but, usually, biquadratic function refers to a quadratic function of a square (or, equivalently, to the function defined by a quartic polynomial without terms of odd degree), having the form
A quartic equation, or equation of the fourth degree, is an equation consisting in equating to zero a quartic polynomial, of the form
where a &#8800; 0.
The derivative of a quartic function is a cubic function.
Since a quartic function is defined by a polynomial of even degree, it has the same infinite limit when the argument goes to positive or negative infinity. If a is positive, then the function increases to positive infinity at both ends; and thus the function has a global minimum. Likewise, if a is negative, it decreases to negative infinity and has a global maximum. In both cases it may have, but not always, another local maximum and another local minimum.
The degree four (quartic case) is the highest degree such that every polynomial equation can be solved by radicals.
== History ==
Lodovico Ferrari is credited with the discovery of the solution to the quartic in 1540, but since this solution, like all algebraic solutions of the quartic, requires the solution of a cubic to be found, it could not be published immediately. The solution of the quartic was published together with that of the cubic by Ferrari's mentor Gerolamo Cardano in the book Ars Magna (1545).
The Soviet historian I. Y. Depman claimed that even earlier, in 1486, Spanish mathematician Valmes was burned at the stake for claiming to have solved the quartic equation. Inquisitor General Tom &#225; s de Torquemada allegedly told Valmes that it was the will of God that such a solution be inaccessible to human understanding. However Beckmann, who popularized this story of Depman in the west, said that it was unreliable and hinted that it may have been invented as Soviet antireligious propaganda. Beckmann's version of this story has been widely copied in several books and internet sites, usually without his reservations and sometimes with fanciful embellishments. Several attempts to find corroborating evidence for this story, or even for the existence of Valmes, have failed.
The proof that four is the highest degree of a general polynomial for which such solutions can be found was first given in the Abel &#8211; Ruffini theorem in 1824, proving that all attempts at solving the higher order polynomials would be futile. The notes left by &#201; variste Galois prior to dying in a duel in 1832 later led to an elegant complete theory of the roots of polynomials, of which this theorem was one result.
== Applications ==
Polynomials of high degrees often appear in problems involving optimization, and sometimes these polynomials happen to be quartics, but this is a coincidence.
Quartics often arise in computer graphics, for example when computing the intersection of two conic sections. Another example is ray-tracing against quartic surfaces such as tori.
In computer-aided manufacturing, the torus is a common shape associated with the endmill cutter. To calculate its location relative to a triangulated surface, the position of a horizontal torus on the Z-axis must be found where it is tangent to a fixed line, and this requires the solution of a general quartic equation to be calculated. Over 10% of the computational time in a CAM system can be consumed simply calculating the solution to millions of quartic equations.
== Solving a quartic equation ==
=== Nature of the roots ===
Given the general quartic equation
with real coefficients and the nature of its roots is mainly determined by the sign of its discriminant
This may be refined by considering the signs of three other polynomials:
such that is the second degree coefficient of the associated depressed quartic (see below);
which is 0 if the quartic has a triple root; and
which is 0 if the quartic has two double roots.
The possible cases for the nature of the roots are as follows:
If then the equation has two real roots and two complex conjugate roots.
If then the equation's four roots are either all real or all complex.
If P < 0 and D < 0 then all four roots are real and distinct.
If P > 0 or D > 0 then there are two pairs of complex conjugate roots.
If then either the polynomial has a multiple root, or it is the square of a quadratic polynomial. Here are the different cases that can occur:
If P < 0 and D < 0 and &#8800; 0, there is a real double root and two real simple roots.
If (P > 0 and D &#8800; 0) or D > 0, there is a real double root and two complex conjugate roots.
If = 0 and D &#8800; 0, there is a triple root and a simple root, all real.
If D = 0, then:
If P < 0, there are two real double roots.
If P > 0, there are two complex conjugate double roots.
If = 0, all four roots are equal to 
There are some cases that do not seem to be covered, but they can not occur. For example > 0, P = 0 and D &#8804; 0 is not one of the cases. However if > 0 and P = 0 then D > 0 so this combination is not possible.
=== General formula for roots ===
The four roots () for the general quartic equation
with a &#8800; 0 are given in the following formula, which is deduced from the one in the section Solving by factoring into quadratics by back changing the variables (see section Converting to a depressed quartic) and using the formulas for the quadratic and cubic equations.
where p and q are the coefficients of the second and of the first degree respectively in the associated depressed quartic
and where
with
and
 where is the aforementioned discriminant. The mathematical expressions of these last four terms are very similar to those of their cubic counterparts.
==== Special cases of the formula ====
If using may prove inconvenient, since its value is now a complex number. However, if all four roots are real, the value of is also real, and it would be simpler to express it with the aid of trigonometric functions, as follows:
where
If and the sign of has to be chosen to have that is one should define as maintaining the sign of 
If then one must change the choice of the cubic root in for having This is always possible except if the quartic may be factored into The result is then correct, but misleading hiding the fact that no cubic root is needed in this case. In fact this case may occur only if the numerator of is zero, and the associated depressed quartic is biquadratic; it may thus be solved by the method described below.
If and and thus also at least three roots are equal, and the roots are rational functions of the coefficients.
If and the above expression for the roots is correct but misleading, hiding the fact that the polynomial is reducible and no cubic root is needed to represent the roots.
=== Simpler cases ===
==== Reducible quartics ====
Consider the general quartic
It is reducible if Q=RS, where R and S are non-constant polynomials with rational coefficients (or more generally with coefficients in the same field as the coefficients of Q). There are two ways to write such a factorization: Either
or
In either case, the roots of Q are the roots of the factors, which may be computed by solving quadratic or cubic equations.
Detecting such factorizations can be done by using the factor function of every computer algebra system. But, in many cases, it may be done by hand-written computation. In the preceding section, we have already seen that the polynomial is always reducible if its discriminant is zero (this is true for polynomials of every degree).
A very special case of the first case of factorization is when a0=0. This implies that x1=0 is a first root, b3=a4, b2=a3, b1=a2, b0=a1, and the other roots may be computed by solving a cubic equation.
If then and we have a factorization of the first kind with x1=1. Similarly, if then and we have a factorization of the first kind with x1=-1.
Once a root x1 is known, the second factor of the factorization of the first kind is the quotient of the Euclidean division of Q by x-x1. It is
If are small integers a factorization of the first kind is easy to detect: if with p and q coprime integers, then q divides evenly a4, and p divides evenly a0. Thus, computing for every possible values of p and q allows to find the rational roots, if any.
In the case of two quadratic factors or of large integer coefficients, the factorization is harder to compute, and, in general, it is better to use the factor function of a computer algebra system (see polynomial factorization for a description of the algorithms that are involved).
==== Biquadratic equations ====
If then the biquadratic function
defines a biquadratic equation, which is easy to solve.
Let Then Q becomes a quadratic q in 
Let and be the roots of q. Then the roots of our quartic Q are
==== Quasi-palindromic equation ====
The polynomial
is almost palindromic, as
(it is palindromic if m = 1).
The change of variables in produces the quadratic equation As x2 - xz + m= 0, the quartic equation
may be solved by applying twice the quadratic formula.
=== Converting to a depressed quartic ===
For solving purpose, it is generally better to convert the quartic into a depressed quartic by the following simple change of variable. All formulas are simpler and some methods work only in this case. The roots of the original quartic are easily recovered from that of the depressed quartic by the reverse change of variable.
Let
be the general quartic equation we want to solve.
Dividing by a4, provides the equivalent equation
with
Substituting x by gives, after a simple term regrouping, the equation
where
If y1, y2, y3, y4 are the roots of this depressed quartic, then the roots of the original quartic are 
=== Ferrari's solution ===
As explained in the preceding section, we may start with a depressed quartic equation
This depressed quartic can be solved by means of a method discovered by Lodovico Ferrari. The depressed equation may be rewritten (this is easily verified by expanding the square and regrouping all terms in the left-hand side)
Then, we introduce a variable y into the factor on the left-hand side by adding to both sides. After regrouping the coefficients of the power of u in the right-hand side, this gives the equation
which is equivalent to the original equation, whichever value is given to y.
As the value of y may be arbitrarily chosen, we will choose it in order to get a perfect square in the right-hand side. This implies that the discriminant in u of this quadratic equation is zero, that is y is a root of the equation
which may be rewritten
The value of y may thus be obtained from the formulas provided in the article Cubic equation.
When y is a root of equation (4), the right-hand side of equation (3) the square of
However, this induces a division by zero if This implies and thus that the depressed equation is bi-quadratic, and may be solved by an easier method (see above). This was not a problem at the time of Ferrari, when one solved only explicitly given equations with numeric coefficients. For a general formula that is always true, one thus need to choose a root of the cubic equation such that This is always possible unless for the depressed equation x4=0.
Now, if y is a root of the cubic equation such that equation (3) may be rewritten
and the equation is easily solved by applying to each factor the formula for quadratic equations. Solving them we may write the four roots as
where and denote either + or -. As the two occurrences of must denote the same sign, this leave four possibilities, one for each root.
Therefore the solutions of the original quartic equation are
=== Solving by factoring into quadratics ===
One can solve a quartic by factoring it into a product of two quadratics. Let
By equating coefficients, this results in the following set of simultaneous equations:
This can be simplified by starting again with a depressed quartic where , which can be obtained by substituting for , then , and:
It's now easy to eliminate both and by doing the following:
If we set , then this equation turns into the resolvent cubic equation
which is solved elsewhere. Then, if p is a square root of a non-zero root of this resolvent (such a non zero root exists except for the quartic x4, which is trivially factored),
The symmetries in this solution are easy to see. There are three roots of the cubic, corresponding to the three ways that a quartic can be factored into two quadratics, and choosing positive or negative values of for the square root of merely exchanges the two quadratics with one another.
The above solution shows that the quartic polynomial with a zero coefficient on the cubic term is factorable into quadratics with rational coefficients if and only if either the resolvent cubic has a non-zero root which is the square of a rational, or is the square of rational and c=0; this can readily be checked using the rational root test.
=== Solving by Lagrange resolvent ===
The symmetric group S4 on four elements has the Klein four-group as a normal subgroup. This suggests using a resolvent cubic whose roots may be variously described as a discrete Fourier transform or a Hadamard matrix transform of the roots; see Lagrange resolvents for the general method. Denote by xi, for i from 0 to 3, the four roots of
If we set
then since the transformation is an involution we may express the roots in terms of the four si in exactly the same way. Since we know the value s0 = -a/2, we only need the values for s1, s2 and s3. These are the roots of the polynomial
Substituting the si by their values in term of the xi, this polynomial may be expanded in a polynomial in s whose coefficients are symmetric polynomials in the xi. By the fundamental theorem of symmetric polynomials, these coefficients may be expressed as polynomials in the coefficients of the monic quartic. If, for simplification, we suppose that the quartic is depressed, that is a=0, this results in the polynomial
This polynomial is of degree six, but only of degree three in s2, and so the corresponding equation is solvable by the method described in the article Cubic function. By substituting the roots in the expression of the xi in terms of the si, we obtain expression for the roots. In fact we obtain, apparently, several expressions, depending on the numbering of the roots of the cubic polynomial and of the signs given to their square roots. All these different expressions may be deduced from one of them by simply changing the numbering of the xi.
These expressions are unnecessarily complicated, involving the cubic roots of unity, which can be avoided as follows. If s is any non-zero root of (3), and if we set
then
We therefore can solve the quartic by solving for s and then solving for the roots of the two factors using the quadratic formula.
Note that this gives exactly the same formula for the roots as the preceding section.
=== Solving with algebraic geometry ===
An alternative solution using algebraic geometry is given in (Faucette 1996), and proceeds as follows (more detailed discussion in reference). In brief, one interprets the roots as the intersection of two quadratic curves, then finds the three reducible quadratic curves (pairs of lines) that pass through these points (this corresponds to the resolvent cubic, the pairs of lines being the Lagrange resolvents), and then use these linear equations to solve the quadratic.
The four roots of the depressed quartic may also be expressed as the x coordinates of the intersections of the two quadratic equations i.e., using the substitution that two quadratics intersect in four points is an instance of B &#233; zout's theorem. Explicitly, the four points are for the four roots of the quartic.
These four points are not collinear because they lie on the irreducible quadratic and thus there is a 1-parameter family of quadratics (a pencil of curves) passing through these points. Writing the projectivization of the two quadratics as quadratic forms in three variables:
the pencil is given by the forms for any point in the projective line &#8211; in other words, where and are not both zero, and multiplying a quadratic form by a constant does not change its quadratic curve of zeros.
This pencil contains three reducible quadratics, each corresponding to a pair of lines, each passing through two of the four points, which can be done different ways. Denote these Given any two of these, their intersection is exactly the four points.
The reducible quadratics, in turn, may be determined by expressing the quadratic form as a 3 &#215; 3 matrix: reducible quadratics correspond to this matrix being singular, which is equivalent to its determinant being zero, and the determinant is a homogeneous degree three polynomial in and and corresponds to the resolvent cubic.
== See also ==
Linear function
Quadratic function
Cubic function
Quintic function
Polynomial
Newton's method
== References ==
== Further reading ==
Cardano, Gerolamo (1545), Ars magna or The Rules of Algebra, Dover (published 1993), ISBN 0-486-67811-3 
Faucette, William Mark (1996), "A Geometric Interpretation of the Solution of the General Quartic Polynomial", The American Mathematical Monthly 103 (1): 51 &#8211; 57, doi:10.2307/2975214, CiteSeerX: 10.1.1.111.5574 
Nickalls, R. W. D. (2009). "The quartic equation: invariants and Euler's solution revealed". Mathematical Gazette 93: 66 &#8211; 75. 
Carpenter, W. (1966). "On the solution of the real quartic". Mathematics Magazine 39: 28 &#8211; 30. doi:10.2307/2688990. 
Shmakov, S.L. (2011). "A Universal Method of Solving Quartic Equations". International Journal of Pure and Applied Mathematics 71: 251 &#8211; 259. 
== External links ==
Quartic formula as four single equations at PlanetMath.org.
Ferrari's achievement
Calculator for solving Quartics (also solves Cubics and Quadratics)
PAGE	Ratio	'Elementary mathematics'
In mathematics, a ratio is a relationship between two numbers of the same kind (e.g., objects, persons, students, spoonfuls, units of whatever identical dimension), expressed as "a to b" or a:b, sometimes expressed arithmetically as a dimensionless quotient of the two that explicitly indicates how many times the first number contains the second (not necessarily an integer).
In layman's terms a ratio represents, for every amount of one thing, how much there is of another thing. For example, supposing one has 8 oranges and 6 lemons in a bowl of fruit, the ratio of oranges to lemons would be 4:3 (which is equivalent to 8:6) while the ratio of lemons to oranges would be 3:4. Additionally, the ratio of oranges to the total amount of fruit is 4:7 (equivalent to 8:14). The 4:7 ratio can be further converted to a fraction of 4/7 to represent how much of the fruit is oranges.
== Notation and terminology ==
The ratio of numbers A and B can be expressed as:
the ratio of A to B
A is to B (often followed by "as ...")
A:B
A fraction (rational number) that is the quotient A divided by B: 
The numbers A and B are sometimes called terms with A being the antecedent and B being the consequent.
The proportion expressing the equality of the ratios A:B and C:D is written A:B = C:D or A:B::C:D. This latter form, when spoken or written in the English language, is often expressed as
A is to B as C is to D.
A, B, C and D are called the terms of the proportion. A and D are called the extremes, and B and C are called the means. The equality of three or more proportions is called a continued proportion.
Ratios are sometimes used with three or more terms. The ratio of the dimensions of a "two by four" that is ten inches long is 2:4:10. A good concrete mix is sometimes quoted as 1:2:4 for the ratio of cement to sand to gravel.
For a mixture of 4/1 cement to water, it could be said that the ratio of cement to water is 4:1, that there is 4 times as much cement as water, or that there is a quarter (1/4) as much water as cement..
Older televisions have a 4:3 aspect ratio, which means that the width is 4/3 of the height; modern widescreen TVs have a 16:9 aspect ratio.
== History and etymology ==
It is impossible to trace the origin of the concept of ratio, because the ideas from which it developed would have been familiar to preliterate cultures. For example, the idea of one village being twice as large as another is so basic that it would have been understood in prehistoric society. However, it is possible to trace the origin of the word "ratio" to the Ancient Greek &#955; &#972; &#947; &#959; &#962; (logos). Early translators rendered this into Latin as ratio ("reason"; as in the word "rational"). (A rational number may be expressed as the quotient of two integers.) A more modern interpretation of Euclid's meaning is more akin to computation or reckoning. Medieval writers used the word proportio ("proportion") to indicate ratio and proportionalitas ("proportionality") for the equality of ratios.
Euclid collected the results appearing in the Elements from earlier sources. The Pythagoreans developed a theory of ratio and proportion as applied to numbers. The Pythagoreans' conception of number included only what would today be called rational numbers, casting doubt on the validity of the theory in geometry where, as the Pythagoreans also discovered, incommensurable ratios (corresponding to irrational numbers) exist. The discovery of a theory of ratios that does not assume commensurability is probably due to Eudoxus. The exposition of the theory of proportions that appears in Book VII of The Elements reflects the earlier theory of ratios of commensurables.
The existence of multiple theories seems unnecessarily complex to modern sensibility since ratios are, to a large extent, identified with quotients. This is a comparatively recent development however, as can be seen from the fact that modern geometry textbooks still use distinct terminology and notation for ratios and quotients. The reasons for this are twofold. First, there was the previously mentioned reluctance to accept irrational numbers as true numbers. Second, the lack of a widely used symbolism to replace the already established terminology of ratios delayed the full acceptance of fractions as alternative until the 16th century.
=== Euclid's definitions ===
Book V of Euclid's Elements has 18 definitions, all of which relate to ratios. In addition, Euclid uses ideas that were in such common usage that he did not include definitions for them. The first two definitions say that a part of a quantity is another quantity that "measures" it and conversely, a multiple of a quantity is another quantity that it measures. In modern terminology, this means that a multiple of a quantity is that quantity multiplied by an integer greater than one &#8212; and a part of a quantity (meaning aliquot part) is a part that, when multiplied by an integer greater than one, gives the quantity.
Euclid does not define the term "measure" as used here, However, one may infer that if a quantity is taken as a unit of measurement, and a second quantity is given as an integral number of these units, then the first quantity measures the second. Note that these definitions are repeated, nearly word for word, as definitions 3 and 5 in book VII.
Definition 3 describes what a ratio is in a general way. It is not rigorous in a mathematical sense and some have ascribed it to Euclid's editors rather than Euclid himself. Euclid defines a ratio as between two quantities of the same type, so by this definition the ratios of two lengths or of two areas are defined, but not the ratio of a length and an area. Definition 4 makes this more rigorous. It states that a ratio of two quantities exists when there is a multiple of each that exceeds the other. In modern notation, a ratio exists between quantities p and q if there exist integers m and n so that mp>q and nq>p. This condition is known as the Archimedean property.
Definition 5 is the most complex and difficult. It defines what it means for two ratios to be equal. Today, this can be done by simply stating that ratios are equal when the quotients of the terms are equal, but Euclid did not accept the existence of the quotients of incommensurables, so such a definition would have been meaningless to him. Thus, a more subtle definition is needed where quantities involved are not measured directly to one another. Though it may not be possible to assign a rational value to a ratio, it is possible to compare a ratio with a rational number. Specifically, given two quantities, p and q, and a rational number m/n we can say that the ratio of p to q is less than, equal to, or greater than m/n when np is less than, equal to, or greater than mq respectively. Euclid's definition of equality can be stated as that two ratios are equal when they behave identically with respect to being less than, equal to, or greater than any rational number. In modern notation this says that given quantities p, q, r and s, then p:q::r:s if for any positive integers m and n, np<mq, np=mq, np>mq according as nr<ms, nr=ms, nr>ms respectively. There is a remarkable similarity between this definition and the theory of Dedekind cuts used in the modern definition of irrational numbers.
Definition 6 says that quantities that have the same ratio are proportional or in proportion. Euclid uses the Greek &#7936; &#957; &#945; &#955; &#972; &#947; &#959; &#957; (analogon), this has the same root as &#955; &#972; &#947; &#959; &#962; and is related to the English word "analog".
Definition 7 defines what it means for one ratio to be less than or greater than another and is based on the ideas present in definition 5. In modern notation it says that given quantities p, q, r and s, then p:q>r:s if there are positive integers m and n so that np>mq and nr &#8804; ms.
As with definition 3, definition 8 is regarded by some as being a later insertion by Euclid's editors. It defines three terms p, q and r to be in proportion when p:q::q:r. This is extended to 4 terms p, q, r and s as p:q::q:r::r:s, and so on. Sequences that have the property that the ratios of consecutive terms are equal are called Geometric progressions. Definitions 9 and 10 apply this, saying that if p, q and r are in proportion then p:r is the duplicate ratio of p:q and if p, q, r and s are in proportion then p:s is the triplicate ratio of p:q. If p, q and r are in proportion then q is called a mean proportional to (or the geometric mean of) p and r. Similarly, if p, q, r and s are in proportion then q and r are called two mean proportionals to p and s.
=== Fraction ===
If there are 2 oranges and 3 apples, the ratio of oranges to apples is 2:3, and the ratio of oranges to the total number of pieces of fruit is 2:5. These ratios can also be expressed in fraction form: there are 2/3 as many oranges as apples, and 2/5 of the pieces of fruit are oranges. If orange juice concentrate is to be diluted with water in the ratio 1:4, then one part of concentrate is mixed with four parts of water, giving five parts total; the amount of orange juice concentrate is 1/4 the amount of water, while the amount of orange juice concentrate is 1/5 of the total liquid. In both ratios and fractions, it is important to be clear what is being compared to what, and beginners often make mistakes for this reason.
== Number of terms ==
In general, when comparing the quantities of a two-quantity ratio, this can be expressed as a fraction derived from the ratio. For example, in a ratio of 2:3, the amount/size/volume/number of the first quantity is that of the second quantity. This pattern also works with ratios with more than two terms. However, a ratio with more than two terms cannot be completely converted into a single fraction; a single fraction represents only one part of the ratio since a fraction can only compare two numbers. If the ratio deals with objects or amounts of objects, this is often expressed as "for every two parts of the first quantity there are three parts of the second quantity".
=== Percentage ratio ===
If we multiply all quantities involved in a ratio by the same number, the ratio remains valid. For example, a ratio of 3:2 is the same as 12:8. It is usual either to reduce terms to the lowest common denominator, or to express them in parts per hundred (percent).
If a mixture contains substances A, B, C & D in the ratio 5:9:4:2 then there are 5 parts of A for every 9 parts of B, 4 parts of C and 2 parts of D. As 5+9+4+2=20, the total mixture contains 5/20 of A (5 parts out of 20), 9/20 of B, 4/20 of C, and 2/20 of D. If we divide all numbers by the total and multiply by 100, this is converted to percentages: 25% A, 45% B, 20% C, and 10% D (equivalent to writing the ratio as 25:45:20:10).
== Proportions ==
If the two or more ratio quantities encompass all of the quantities in a particular situation, for example two apples and three oranges in a fruit basket containing no other types of fruit, it could be said that "the whole" contains five parts, made up of two parts apples and three parts oranges. In this case, , or 40% of the whole are apples and , or 60% of the whole are oranges. This comparison of a specific quantity to "the whole" is sometimes called a proportion. Proportions are sometimes expressed as percentages as demonstrated above.
== Reduction ==
Note that ratios can be reduced (as fractions are) by dividing each quantity by the common factors of all the quantities. This is often called "cancelling." As for fractions, the simplest form is considered that in which the numbers in the ratio are the smallest possible integers.
Thus, the ratio 40:60 may be considered equivalent in meaning to the ratio 2:3 within contexts concerned only with relative quantities.
Mathematically, we write: "40:60" = "2:3" (dividing both quantities by 20).
Grammatically, we would say, "40 to 60 equals 2 to 3."
An alternative representation is: "40:60::2:3"
Grammatically, we would say, "40 is to 60 as 2 is to 3."
A ratio that has integers for both quantities and that cannot be reduced any further (using integers) is said to be in simplest form or lowest terms.
Sometimes it is useful to write a ratio in the form 1:n or n:1 to enable comparisons of different ratios.
For example, the ratio 4:5 can be written as 1:1.25 (dividing both sides by 4)
Alternatively, 4:5 can be written as 0.8:1 (dividing both sides by 5)
Where the context makes the meaning clear, a ratio in this form is sometimes written without the 1 and the colon, though, mathematically, this makes it a factor or multiplier.
=== Dilution ratio ===
Ratios are often used for simple dilutions applied in chemistry and biology. A simple dilution is one in which a unit volume of a liquid material of interest is combined with an appropriate volume of a solvent liquid to achieve the desired concentration. The dilution factor is the total number of unit volumes in which your material is dissolved. The diluted material must then be thoroughly mixed to achieve the true dilution. For example, a 1:5 dilution (verbalize as "1 to 5" dilution) entails combining 1 unit volume of solute (the material to be diluted) + 4 unit volumes (approximately) of the solvent to give 5 units of the total volume. (Some solutions and mixtures take up slightly less volume than their components.)
The dilution factor is frequently expressed using exponents: 1:5 would be 5e &#8722; 1 (5 &#8722; 1 i.e. one-fifth:one); 1:100 would be 10e &#8722; 2 (10 &#8722; 2 i.e. one hundredth:one), and so on.
There is often confusion between dilution ratio (1:n meaning 1 part solute to n parts solvent) and dilution factor (1:n+1) where the second number (n+1) represents the total volume of solute + solvent. In scientific and serial dilutions, the given ratio (or factor) often means the ratio to the final volume, not to just the solvent. The factors then can easily be multiplied to give an overall dilution factor.
In other areas of science such as pharmacy, and in non-scientific usage, a dilution is normally given as a plain ratio of solvent to solute.
== Odds ==
Odds (as in gambling) are expressed as a ratio. For example, odds of "7 to 3 against" (7:3) mean that there are seven chances that the event will not happen to every three chances that it will happen. The probability of success is 30%. In every ten trials, there are expected to be three wins and seven losses.
== Different units ==
Ratios are unitless when they relate quantities in units of the same dimension.
For example, the ratio 1 minute : 40 seconds can be reduced by changing the first value to 60 seconds. Once the units are the same, they can be omitted, and the ratio can be reduced to 3:2.
In chemistry, mass concentration "ratios" are usually expressed as w/v percentages, and are really proportions.
For example, a concentration of 3% w/v usually means 3g of substance in every 100mL of solution. This cannot easily be converted to a pure ratio because of density considerations, and the second figure is the total amount, not the volume of solvent.
== Financial ratios ==
Various financial ratios are used in the fundamental analysis of a business, for example the price &#8211; earnings ratio is commonly quoted for shares.
== Triangular coordinates ==
The locations of points relative to a triangle with vertices A, B, and C and sides AB, BC, and CA are often expressed in extended ratio form as triangular coordinates.
In barycentric coordinates, a point with coordinates is the point upon which a thin sheet of uniform-density metal in the shape and size of the triangle would exactly balance if weights were put on the vertices, with the ratio of the weights at A and B being the ratio of the weights at B and C being and therefore the ratio of weights at A and C being 
In trilinear coordinates, a point with coordinates x:y:z has perpendicular distances to side BC (across from vertex A) and side CA (across from vertex B) in the ratio x:y, distances to side CA and side AB (across from C) in the ratio y:z, and therefore distances to sides BC and AB in the ratio x:z.
Since all information is expressed in terms of ratios (the individual numbers denoted by x, y, and z have no meaning by themselves), a triangle analysis using barycentric or trilinear coordinates applies regardless of the size of the triangle.
== See also ==
Aspect ratio
Fraction (mathematics)
Golden ratio
Interval (music)
Parts-per notation
Price &#8211; performance ratio
Proportionality (mathematics)
Ratio distribution
Ratio estimator
Rule of three (mathematics)
Sex ratio
Silver ratio
Slope
== References ==
^ Wentworth, p. 55
^ New International Encyclopedia
^ Penny Cyclopedia, p. 307
^ New International Encyclopedia
^ New International Encyclopedia
^ Belle Group concrete mixing hints
^ Smith, p. 477
^ Penny Cyclopedia, p. 307
^ Smith, p. 478
^ Heath, p. 112
^ Heath, p. 113
^ Smith, p. 480
^ Heath, reference for section
^ "Geometry, Euclidean" Encyclop &#230; dia Britannica Eleventh Edition p682.
^ Heath p. 125
== Further reading ==
"Ratio" The Penny Cyclop &#230; dia vol. 19, The Society for the Diffusion of Useful Knowledge (1841) Charles Knight and Co., London pp. 307ff
"Proportion" New International Encyclopedia, Vol. 19 2nd ed. (1916) Dodd Mead & Co. pp270-271
"Ratio and Proportion" Fundamentals of practical mathematics, George Wentworth, David Eugene Smith, Herbert Druery Harper (1922) Ginn and Co. pp. 55ff
The thirteen books of Euclid's Elements, vol 2. trans. Sir Thomas Little Heath (1908). Cambridge Univ. Press. pp. 112ff. 
D.E. Smith, History of Mathematics, vol 2 Dover (1958) pp. 477ff
== External links ==
PAGE	Rational number	'Elementary mathematics'
In mathematics, a rational number is any number that can be expressed as the quotient or fraction p/q of two integers, p and q, with the denominator q not equal to zero. Since q may be equal to 1, every integer is a rational number. The set of all rational numbers is usually denoted by a boldface Q (or blackboard bold , Unicode &#8474; ); it was thus denoted in 1895 by Peano after quoziente, Italian for "quotient".
The decimal expansion of a rational number always either terminates after a finite number of digits or begins to repeat the same finite sequence of digits over and over. Moreover, any repeating or terminating decimal represents a rational number. These statements hold true not just for base 10, but also for binary, hexadecimal, or any other integer base.
A real number that is not rational is called irrational. Irrational numbers include &#8730; 2, &#960; , e, and &#966; . The decimal expansion of an irrational number continues without repeating. Since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational.
The rational numbers can be formally defined as the equivalence classes of the quotient set (Z &#215; (Z \ {0})) / ~, where the cartesian product Z &#215; (Z \ {0}) is the set of all ordered pairs (m,n) where m and n are integers, n is not 0 (n &#8800; 0), and "~" is the equivalence relation defined by (m1,n1) ~ (m2,n2) if, and only if, m1n2 &#8722; m2n1 = 0.
In abstract algebra, the rational numbers together with certain operations of addition and multiplication form a field. This is the archetypical field of characteristic zero, and is the field of fractions for the ring of integers. Finite extensions of Q are called algebraic number fields, and the algebraic closure of Q is the field of algebraic numbers.
In mathematical analysis, the rational numbers form a dense subset of the real numbers. The real numbers can be constructed from the rational numbers by completion, using Cauchy sequences, Dedekind cuts, or infinite decimals.
Zero divided by any other integer equals zero; therefore, zero is a rational number (but division by zero is undefined).
== Terminology ==
The term rational in reference to the set Q refers to the fact that a rational number represents a ratio of two integers. In mathematics, the adjective rational often means that the underlying field considered is the field Q of rational numbers. Rational polynomial usually, and most correctly, means a polynomial with rational coefficients, also called a "polynomial over the rationals". However, rational function does not mean the underlying field is the rational numbers, and a rational algebraic curve is not an algebraic curve with rational coefficients.
== Arithmetic ==
=== Embedding of integers ===
Any integer n can be expressed as the rational number n/1.
=== Equality ===
 if and only if 
=== Ordering ===
Where both denominators are positive:
 if and only if 
If either denominator is negative, the fractions must first be converted into equivalent forms with positive denominators, through the equations:
and
=== Addition ===
Two fractions are added as follows:
=== Subtraction ===
=== Multiplication ===
The rule for multiplication is:
=== Division ===
Where c &#8800; 0:
Note that division is equivalent to multiplying by the reciprocal of the divisor fraction:
=== Inverse ===
Additive and multiplicative inverses exist in the rational numbers:
=== Exponentiation to integer power ===
If n is a non-negative integer, then
and (if a &#8800; 0):
== Continued fraction representation ==
A finite continued fraction is an expression such as
where an are integers. Every rational number a/b has two closely related expressions as a finite continued fraction, whose coefficients an can be determined by applying the Euclidean algorithm to (a,b).
== Formal construction ==
Mathematically we may construct the rational numbers as equivalence classes of ordered pairs of integers (m,n), with n &#8800; 0. This space of equivalence classes is the quotient space (Z &#215; (Z \ {0})) / ~, where (m1,n1) ~ (m2,n2) if, and only if, m1n2 &#8722; m2n1 = 0. We can define addition and multiplication of these pairs with the following rules:
and, if m2 &#8800; 0, division by
The equivalence relation (m1,n1) ~ (m2,n2) if, and only if, m1n2 &#8722; m2n1 = 0 is a congruence relation, i.e. it is compatible with the addition and multiplication defined above, and we may define Q to be the quotient set (Z &#215; (Z \ {0})) / ~, i.e. we identify two pairs (m1,n1) and (m2,n2) if they are equivalent in the above sense. (This construction can be carried out in any integral domain: see field of fractions.) We denote by [(m1,n1)] the equivalence class containing (m1,n1). If (m1,n1) ~ (m2,n2) then, by definition, (m1,n1) belongs to [(m2,n2)] and (m2,n2) belongs to [(m1,n1)]; in this case we can write [(m1,n1)] = [(m2,n2)]. Given any equivalence class [(m,n)] there are a countably infinite number of representation, since
The canonical choice for [(m,n)] is chosen so that n is positive and gcd(m,n) = 1, i.e. m and n share no common factors, i.e. m and n are coprime. For example, we would write [(1,2)] instead of [(2,4)] or [( &#8722; 12, &#8722; 24)], even though [(1,2)] = [(2,4)] = [( &#8722; 12, &#8722; 24)].
We can also define a total order on Q. Let &#8743; be the and-symbol and &#8744; be the or-symbol. We say that [(m1,n1)] &#8804; [(m2,n2)] if:
The integers may be considered to be rational numbers by the embedding that maps m to [(m,1)].
== Properties ==
The set Q, together with the addition and multiplication operations shown above, forms a field, the field of fractions of the integers Z.
The rationals are the smallest field with characteristic zero: every other field of characteristic zero contains a copy of Q. The rational numbers are therefore the prime field for characteristic zero.
The algebraic closure of Q, i.e. the field of roots of rational polynomials, is the algebraic numbers.
The set of all rational numbers is countable. Since the set of all real numbers is uncountable, we say that almost all real numbers are irrational, in the sense of Lebesgue measure, i.e. the set of rational numbers is a null set.
The rationals are a densely ordered set: between any two rationals, there sits another one, and, therefore, infinitely many other ones. For example, for any two fractions such that
(where are positive), we have
Any totally ordered set which is countable, dense (in the above sense), and has no least or greatest element is order isomorphic to the rational numbers.
== Real numbers and topological properties ==
The rationals are a dense subset of the real numbers: every real number has rational numbers arbitrarily close to it. A related property is that rational numbers are the only numbers with finite expansions as regular continued fractions.
By virtue of their order, the rationals carry an order topology. The rational numbers, as a subspace of the real numbers, also carry a subspace topology. The rational numbers form a metric space by using the absolute difference metric d(x,y) = |x &#8722; y|, and this yields a third topology on Q. All three topologies coincide and turn the rationals into a topological field. The rational numbers are an important example of a space which is not locally compact. The rationals are characterized topologically as the unique countable metrizable space without isolated points. The space is also totally disconnected. The rational numbers do not form a complete metric space; the real numbers are the completion of Q under the metric d(x,y) = |x &#8722; y|, above.
== p-adic numbers ==
In addition to the absolute value metric mentioned above, there are other metrics which turn Q into a topological field:
Let p be a prime number and for any non-zero integer a, let |a|p = p &#8722; n, where pn is the highest power of p dividing a.
In addition set |0|p = 0. For any rational number a/b, we set |a/b|p = |a|p / |b|p.
Then dp(x,y) = |x &#8722; y|p defines a metric on Q.
The metric space (Q,dp) is not complete, and its completion is the p-adic number field Qp. Ostrowski's theorem states that any non-trivial absolute value on the rational numbers Q is equivalent to either the usual real absolute value or a p-adic absolute value.
== See also ==
Floating point
Ford circles
Niven's theorem
Rational data type
== References ==
^ a b Rosen, Kenneth (2007). Discrete Mathematics and its Applications (6th ed.). New York, NY: McGraw-Hill. pp. 105,158 &#8211; 160. ISBN 978-0-07-288008-3. 
^ Gilbert, Jimmie; Linda, Gilbert (2005). Elements of Modern Algebra (6th ed.). Belmont, CA: Thomson Brooks/Cole. pp. 243 &#8211; 244. ISBN 0-534-40264-X. 
== External links ==
Hazewinkel, Michiel, ed. (2001), "Rational number", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
"Rational Number" From MathWorld &#8211; A Wolfram Web Resource
PAGE	Rationalisation (mathematics)	'Elementary algebra'
In elementary algebra, root rationalisation is a process by which surds in the denominator of an irrational fraction are eliminated.
These surds may be monomials or binomials involving square roots, in simple examples. There are wide extensions to the technique.
== Rationalisation of a monomial square root and cube root ==
For the fundamental technique, the numerator and denominator must be multiplied by the same factor.
Example 1:
To rationalise this kind of monomial, bring in the factor :
The square root disappears from the denominator, because it is squared:
This gives the result, after simplification:
Example 2:
To rationalise this radical, bring in the factor :
The cube root disappears from the denominator, because it is cubed:
This gives the result, after simplification:
== Dealing with more square roots ==
For a denominator that is:
Rationalisation can be achieved by multiplying by the Conjugate:
and applying the difference of two squares identity, which here will yield &#8722; 1. To get this result, the entire fraction should be multiplied by
This technique works much more generally. It can easily be adapted to remove one square root at a time, i.e. to rationalise
by multiplication by
Example:
The fraction must be multiplied by a quotient containing .
Now, we can proceed to remove the square roots in the denominator:
== Generalisations ==
Rationalisation can be extended to all algebraic numbers and algebraic functions (as an application of norm forms). For example, to rationalise a cube root, two linear factors involving cube roots of unity should be used, or equivalently a quadratic factor.
== See also ==
Conjugate (algebra)
Sum of two squares
== References ==
This material is carried in classic algebra texts. For example:
George Chrystal, Introduction to Algebra: For the Use of Secondary Schools and Technical Colleges is a nineteenth-century text, first edition 1889, in print (ISBN 1402159072); a trinomial example with square roots is on p. 256, while a general theory of rationalising factors for surds is on pp. 189 &#8211; 199.
PAGE	Real number	'Elementary mathematics'
In mathematics, a real number is a value that represents a quantity along a continuous line. The real numbers include all the rational numbers, such as the integer &#8722; 5 and the fraction 4/3, and all the irrational numbers such as &#8730; 2 (1.41421356 &#8230; , the square root of two, an irrational algebraic number) and &#960; (3.14159265 &#8230; , a transcendental number). Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation such as that of 8.632, where each consecutive digit is measured in units one tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and complex numbers include real numbers.
These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers &#8211; indeed, the realization that a better definition was needed &#8211; was one of the most important developments of 19th century mathematics. The currently standard axiomatic definition is that real numbers form the unique Archimedean complete totally ordered field (R ; + ; &#183; ; <), up to an isomorphism, whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences of rational numbers, Dedekind cuts, or certain infinite "decimal representations", together with precise interpretations for the arithmetic operations and the order relation. These definitions are equivalent in the realm of classical mathematics.
The reals are uncountable; that is, while both the set of all natural numbers and the set of all real numbers are infinite sets, there can be no one-to-one function from the real numbers to the natural numbers: the cardinality of the set of all real numbers (denoted and called cardinality of the continuum) is strictly greater than the cardinality of the set of all natural numbers (denoted ). The statement that there is no subset of the reals with cardinality strictly greater than and strictly smaller than is known as the continuum hypothesis. It is known to be neither provable nor refutable using the axioms of Zermelo &#8211; Fraenkel set theory, the standard foundation of modern mathematics, provided ZF set theory is consistent.
== History ==
Simple fractions have been used by the Egyptians around 1000 BC; the Vedic "Sulba Sutras" ("The rules of chords") in, c. 600 BC, include what may be the first "use" of irrational numbers. The concept of irrationality was implicitly accepted by early Indian mathematicians since Manava (c. 750 &#8211; 690 BC), who were aware that the square roots of certain numbers such as 2 and 61 could not be exactly determined. Around 500 BC, the Greek mathematicians led by Pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of 2.
The Middle Ages brought the acceptance of zero, negative, integral, and fractional numbers, first by Indian and Chinese mathematicians, and then by Arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects, which was made possible by the development of algebra. Arabic mathematicians merged the concepts of "number" and "magnitude" into a more general idea of real numbers. The Egyptian mathematician Ab &#363; K &#257; mil Shuj &#257; ibn Aslam (c. 850 &#8211; 930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.
In the 16th century, Simon Stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard.
In the 17th century, Descartes introduced the term "real" to describe roots of a polynomial, distinguishing them from "imaginary" ones.
In the 18th and 19th centuries there was much work on irrational and transcendental numbers. Johann Heinrich Lambert (1761) gave the first flawed proof that &#960; cannot be rational; Adrien-Marie Legendre (1794) completed the proof, and showed that &#960; is not the square root of a rational number. Paolo Ruffini (1799) and Niels Henrik Abel (1842) both constructed proofs of the Abel &#8211; Ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots.
 &#201; variste Galois (1832) developed techniques for determining whether a given equation could be solved by radicals, which gave rise to the field of Galois theory. Joseph Liouville (1840) showed that neither e nor e2 can be a root of an integer quadratic equation, and then established the existence of transcendental numbers, the proof being subsequently displaced by Georg Cantor (1873). Charles Hermite (1873) first proved that e is transcendental, and Ferdinand von Lindemann (1882), showed that &#960; is transcendental. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and has finally been made elementary by Adolf Hurwitz and Paul Gordan.
The development of calculus in the 18th century used the entire set of real numbers without having defined them cleanly. The first rigorous definition was given by Georg Cantor in 1871. In 1874 he showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite. Contrary to widely held beliefs, his first method was not his famous diagonal argument, which he published in 1891. See Cantor's first uncountability proof.
== Definition ==
The real number system can be defined axiomatically up to an isomorphism, which is described below. There are also many ways to construct "the" real number system, for example, starting from natural numbers, then defining rational numbers algebraically, and finally defining real numbers as equivalence classes of their Cauchy sequences or as Dedekind cuts, which are certain subsets of rational numbers. Another possibility is to start from some rigorous axiomatization of Euclidean geometry (Hilbert, Tarski etc.) and then define the real number system geometrically. From the structuralist point of view all these constructions are on equal footing.
=== Axiomatic approach ===
Let &#8477; denote the set of all real numbers. Then:
The set &#8477; is a field, meaning that addition and multiplication are defined and have the usual properties.
The field &#8477; is ordered, meaning that there is a total order &#8805; such that, for all real numbers x, y and z:
if x &#8805; y then x + z &#8805; y + z;
if x &#8805; 0 and y &#8805; 0 then xy &#8805; 0.
The order is Dedekind-complete; that is, every non-empty subset S of &#8477; with an upper bound in &#8477; has a least upper bound (also called supremum) in &#8477; .
The last property is what differentiates the reals from the rationals. For example, the set of rationals with square less than 2 has a rational upper bound (e.g., 1.5) but no rational least upper bound, because the square root of 2 is not rational.
The real numbers are uniquely specified by the above properties. More precisely, given any two Dedekind-complete ordered fields &#8477; 1 and &#8477; 2, there exists a unique field isomorphism from &#8477; 1 to &#8477; 2, allowing us to think of them as essentially the same mathematical object.
For another axiomatization of &#8477; , see Tarski's axiomatization of the reals.
=== Construction from the rational numbers ===
The real numbers can be constructed as a completion of the rational numbers in such a way that a sequence defined by a decimal or binary expansion like (3; 3.1; 3.14; 3.141; 3.1415; &#8230; ) converges to a unique real number, in this case &#960; . For details and other constructions of real numbers, see construction of the real numbers.
== Properties ==
=== Basic properties ===
A real number may be either rational or irrational; either algebraic or transcendental; and either positive, negative, or zero. Real numbers are used to measure continuous quantities. They may be expressed by decimal representations that have an infinite sequence of digits to the right of the decimal point; these are often represented in the same form as 324.823122147 &#8230; The ellipsis (three dots) indicates that there would still be more digits to come.
More formally, real numbers have the two basic properties of being an ordered field, and having the least upper bound property. The first says that real numbers comprise a field, with addition and multiplication as well as division by non-zero numbers, which can be totally ordered on a number line in a way compatible with addition and multiplication. The second says that, if a non-empty set of real numbers has an upper bound, then it has a real least upper bound. The second condition distinguishes the real numbers from the rational numbers: for example, the set of rational numbers whose square is less than 2 is a set with an upper bound (e.g. 1.5) but no (rational) least upper bound: hence the rational numbers do not satisfy the least upper bound property.
=== Completeness ===
A main reason for using real numbers is that the reals contain all limits. More precisely, every sequence of real numbers having the property that consecutive terms of the sequence become arbitrarily close to each other necessarily has the property that after some term in the sequence the remaining terms are arbitrarily close to some specific real number. In mathematical terminology, this means that the reals are complete (in the sense of metric spaces or uniform spaces, which is a different sense than the Dedekind completeness of the order in the previous section). This is formally defined in the following way:
A sequence (xn) of real numbers is called a Cauchy sequence if for any &#949; > 0 there exists an integer N (possibly depending on &#949; ) such that the distance |xn &#8722; xm| is less than &#949; for all n and m that are both greater than N. In other words, a sequence is a Cauchy sequence if its elements xn eventually come and remain arbitrarily close to each other.
A sequence (xn) converges to the limit x if for any &#949; > 0 there exists an integer N (possibly depending on &#949; ) such that the distance |xn &#8722; x| is less than &#949; provided that n is greater than N. In other words, a sequence has limit x if its elements eventually come and remain arbitrarily close to x.
Notice that every convergent sequence is a Cauchy sequence. The converse is also true:
Every Cauchy sequence of real numbers is convergent to a real number.
That is, the reals are complete.
Note that the rationals are not complete. For example, the sequence (1; 1.4; 1.41; 1.414; 1.4142; 1.41421 &#8230; ), where each term adds a digit of the decimal expansion of the positive square root of 2, is Cauchy but it does not converge to a rational number. (In the real numbers, in contrast, it converges to the positive square root of 2.)
The existence of limits of Cauchy sequences is what makes calculus work and is of great practical use. The standard numerical test to determine if a sequence has a limit is to test if it is a Cauchy sequence, as the limit is typically not known in advance.
For example, the standard series of the exponential function
converges to a real number because for every x the sums
can be made arbitrarily small by choosing N sufficiently large. This proves that the sequence is Cauchy, so we know that the sequence converges even if the limit is not known in advance.
=== "The complete ordered field" ===
The real numbers are often described as "the complete ordered field", a phrase that can be interpreted in several ways.
First, an order can be lattice-complete. It is easy to see that no ordered field can be lattice-complete, because it can have no largest element (given any element z, z + 1 is larger), so this is not the sense that is meant.
Additionally, an order can be Dedekind-complete, as defined in the section Axioms. The uniqueness result at the end of that section justifies using the word "the" in the phrase "complete ordered field" when this is the sense of "complete" that is meant. This sense of completeness is most closely related to the construction of the reals from Dedekind cuts, since that construction starts from an ordered field (the rationals) and then forms the Dedekind-completion of it in a standard way.
These two notions of completeness ignore the field structure. However, an ordered group (in this case, the additive group of the field) defines a uniform structure, and uniform structures have a notion of completeness (topology); the description in the previous section Completeness is a special case. (We refer to the notion of completeness in uniform spaces rather than the related and better known notion for metric spaces, since the definition of metric space relies on already having a characterization of the real numbers.) It is not true that R is the only uniformly complete ordered field, but it is the only uniformly complete Archimedean field, and indeed one often hears the phrase "complete Archimedean field" instead of "complete ordered field". Every uniformly complete Archimedean field must also be Dedekind-complete (and vice versa, of course), justifying using "the" in the phrase "the complete Archimedean field". This sense of completeness is most closely related to the construction of the reals from Cauchy sequences (the construction carried out in full in this article), since it starts with an Archimedean field (the rationals) and forms the uniform completion of it in a standard way.
But the original use of the phrase "complete Archimedean field" was by David Hilbert, who meant still something else by it. He meant that the real numbers form the largest Archimedean field in the sense that every other Archimedean field is a subfield of R. Thus R is "complete" in the sense that nothing further can be added to it without making it no longer an Archimedean field. This sense of completeness is most closely related to the construction of the reals from surreal numbers, since that construction starts with a proper class that contains every ordered field (the surreals) and then selects from it the largest Archimedean subfield.
=== Advanced properties ===
The reals are uncountable; that is, there are strictly more real numbers than natural numbers, even though both sets are infinite. In fact, the cardinality of the reals equals that of the set of subsets (i.e. the power set) of the natural numbers, and Cantor's diagonal argument states that the latter set's cardinality is strictly greater than the cardinality of N. Since the set of algebraic numbers is countable, almost all real numbers are transcendental. The non-existence of a subset of the reals with cardinality strictly between that of the integers and the reals is known as the continuum hypothesis. The continuum hypothesis can neither be proved nor be disproved; it is independent from the axioms of set theory.
As a topological space, the real numbers are separable. This is because the set of rationals, which is countable, is dense in the real numbers. The irrational numbers are also dense in the real numbers, however they are uncountable and have the same cardinality as the reals.
The real numbers form a metric space: the distance between x and y is defined as the absolute value |x &#8722; y|. By virtue of being a totally ordered set, they also carry an order topology; the topology arising from the metric and the one arising from the order are identical, but yield different presentations for the topology &#8211; in the order topology as ordered intervals, in the metric topology as epsilon-balls. The Dedekind cuts construction uses the order topology presentation, while the Cauchy sequences construction uses the metric topology presentation. The reals are a contractible (hence connected and simply connected), separable and complete metric space of Hausdorff dimension 1. The real numbers are locally compact but not compact. There are various properties that uniquely specify them; for instance, all unbounded, connected, and separable order topologies are necessarily homeomorphic to the reals.
Every nonnegative real number has a square root in R, although no negative number does. This shows that the order on R is determined by its algebraic structure. Also, every polynomial of odd degree admits at least one real root: these two properties make R the premier example of a real closed field. Proving this is the first half of one proof of the fundamental theorem of algebra.
The reals carry a canonical measure, the Lebesgue measure, which is the Haar measure on their structure as a topological group normalized such that the unit interval [0;1] has measure 1. There exist sets of real numbers that are not Lebesgue measurable, e.g. Vitali sets.
The supremum axiom of the reals refers to subsets of the reals and is therefore a second-order logical statement. It is not possible to characterize the reals with first-order logic alone: the L &#246; wenheim &#8211; Skolem theorem implies that there exists a countable dense subset of the real numbers satisfying exactly the same sentences in first-order logic as the real numbers themselves. The set of hyperreal numbers satisfies the same first order sentences as R. Ordered fields that satisfy the same first-order sentences as R are called nonstandard models of R. This is what makes nonstandard analysis work; by proving a first-order statement in some nonstandard model (which may be easier than proving it in R), we know that the same statement must also be true of R.
The field R of real numbers is an extension field of the field Q of rational numbers, and R can therefore be seen as a vector space over Q. Zermelo &#8211; Fraenkel set theory with the axiom of choice guarantees the existence of a basis of this vector space: there exists a set B of real numbers such that every real number can be written uniquely as a finite linear combination of elements of this set, using rational coefficients only, and such that no element of B is a rational linear combination of the others. However, this existence theorem is purely theoretical, as such a base has never been explicitly described.
The well-ordering theorem implies that the real numbers can be well-ordered if the axiom of choice is assumed: there exists a total order on R with the property that every non-empty subset of R has a least element in this ordering. (The standard ordering &#8804; of the real numbers is not a well-ordering since e.g. an open interval does not contain a least element in this ordering.) Again, the existence of such a well-ordering is purely theoretical, as it has not been explicitly described. If V=L is assumed in addition to the axioms of ZF, a well ordering of the real numbers can be shown to be explicitly definable by a formula.
== Applications and connections to other areas ==
=== Real numbers and logic ===
The real numbers are most often formalized using the Zermelo &#8211; Fraenkel axiomatization of set theory, but some mathematicians study the real numbers with other logical foundations of mathematics. In particular, the real numbers are also studied in reverse mathematics and in constructive mathematics.
The hyperreal numbers as developed by Edwin Hewitt, Abraham Robinson and others extend the set of the real numbers by introducing infinitesimal and infinite numbers, allowing for building infinitesimal calculus in a way closer to the original intuitions of Leibniz, Euler, Cauchy and others.
Edward Nelson's internal set theory enriches the Zermelo &#8211; Fraenkel set theory syntactically by introducing a unary predicate "standard". In this approach, infinitesimals are (non-"standard") elements of the set of the real numbers (rather than being elements of an extension thereof, as in Robinson's theory).
The continuum hypothesis posits that the cardinality of the set of the real numbers is ; i.e. the smallest infinite cardinal number after , the cardinality of the integers. Paul Cohen proved in 1963 that it is an axiom independent of the other axioms of set theory; that is, one may choose either the continuum hypothesis or its negation as an axiom of set theory, without contradiction.
=== In physics ===
In the physical sciences, most physical constants such as the universal gravitational constant, and physical variables, such as position, mass, speed, and electric charge, are modeled using real numbers. In fact, the fundamental physical theories such as classical mechanics, electromagnetism, quantum mechanics, general relativity and the standard model are described using mathematical structures, typically smooth manifolds or Hilbert spaces, that are based on the real numbers, although actual measurements of physical quantities are of finite accuracy and precision.
In some recent developments of theoretical physics stemming from the holographic principle, the Universe is seen fundamentally as an information store, essentially zeroes and ones, organized in much less geometrical fashion and manifesting itself as space-time and particle fields only on a more superficial level. This approach removes the real number system from its foundational role in physics and even prohibits the existence of infinite precision real numbers in the physical universe by considerations based on the Bekenstein bound.
=== In computation ===
With some exceptions, most calculators do not operate on real numbers. Instead, they work with finite-precision approximations called floating-point numbers. In fact, most scientific computation uses floating-point arithmetic. Real numbers satisfy the usual rules of arithmetic, but floating-point numbers do not.
Computers cannot directly store arbitrary real numbers with infinitely many digits.
The precision is limited by the number of bits allocated to store a number, whether as floating-point numbers or arbitrary precision numbers. However, computer algebra systems can operate on irrational quantities exactly by manipulating formulas for them (such as , , or) rather than their rational or decimal approximation; however, it is not in general possible to determine whether two such expressions are equal (the constant problem).
A real number is called computable if there exists an algorithm that yields its digits. Because there are only countably many algorithms, but an uncountable number of reals, almost all real numbers fail to be computable. Moreover, the equality of two computable numbers is an undecidable problem. Some constructivists accept the existence of only those reals that are computable. The set of definable numbers is broader, but still only countable.
=== "Reals" in set theory ===
In set theory, specifically descriptive set theory, the Baire space is used as a surrogate for the real numbers since the latter have some topological properties (connectedness) that are a technical inconvenience. Elements of Baire space are referred to as "reals".
== Notation ==
Mathematicians use the symbol R, or, alternatively, , the letter "R" in blackboard bold (encoded in Unicode as U+211D &#8477; double-struck capital r (HTML: &#8477; )), to represent the set of all real numbers. As this set is naturally endowed with the structure of a field, the expression field of real numbers is frequently used when its algebraic properties are under consideration.
The sets of positive real numbers and negative real numbers are often denoted by R+ and R-, respectively; R+ and R- are also used. The non-negative real numbers can be denoted by R &#8805; 0 but one often sees this set denoted by R+ &#8746; {0}. In French mathematics, the positive real numbers and negative real numbers commonly include zero, and these sets are denoted respectively by &#8477; + and &#8477; -. In this understanding, the respective sets without zero are called strictly positive real numbers and strictly negative real numbers, and are denoted as &#8477; +* and &#8477; -*.
The notation Rn refers to the cartesian product of n copies of R, which is an n-dimensional vector space over the field of the real numbers; this vector space may be identified to the n-dimensional space of Euclidean geometry as soon as a coordinate system has been chosen in the latter. For example, a value from R3 consists of three real numbers and specifies the coordinates of a point in 3 &#8209; dimensional space.
In mathematics, real is used as an adjective, meaning that the underlying field is the field of the real numbers (or the real field). For example real matrix, real polynomial and real Lie algebra. As a substantive, the word real is used almost strictly in reference to the real numbers themselves (e.g., the "set of all reals").
== Generalizations and extensions ==
The real numbers can be generalized and extended in several different directions:
The complex numbers contain solutions to all polynomial equations and hence are an algebraically closed field unlike the real numbers. However, the complex numbers are not an ordered field.
The affinely extended real number system adds two elements + &#8734; and &#8722; &#8734; . It is a compact space. It is no longer a field, not even an additive group, but it still has a total order; moreover, it is a complete lattice.
The real projective line adds only one value &#8734; . It is also a compact space. Again, it is no longer a field, not even an additive group. However, it allows division of a non-zero element by zero. It is not ordered anymore.
The long real line pastes together &#8501; 1* + &#8501; 1 copies of the real line plus a single point (here &#8501; 1* denotes the reversed ordering of &#8501; 1) to create an ordered set that is "locally" identical to the real numbers, but somehow longer; for instance, there is an order-preserving embedding of &#8501; 1 in the long real line but not in the real numbers. The long real line is the largest ordered set that is complete and locally Archimedean. As with the previous two examples, this set is no longer a field or additive group.
Ordered fields extending the reals are the hyperreal numbers and the surreal numbers; both of them contain infinitesimal and infinitely large numbers and are therefore non-Archimedean ordered fields.
Self-adjoint operators on a Hilbert space (for example, self-adjoint square complex matrices) generalize the reals in many respects: they can be ordered (though not totally ordered), they are complete, all their eigenvalues are real and they form a real associative algebra. Positive-definite operators correspond to the positive reals and normal operators correspond to the complex numbers.
== See also ==
== Notes ==
== References ==
Georg Cantor, 1874, " &#220; ber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen", Journal f &#252; r die Reine und Angewandte Mathematik, volume 77, pages 258 &#8211; 262.
Solomon Feferman,1989, The Numbers Systems: Foundations of Algebra and Analysis, AMS Chelsea, ISBN 0-8218-2915-7.
Robert Katz, 1964, Axiomatic Analysis, D. C. Heath and Company.
Edmund Landau, 2001, ISBN 0-8218-2693-X, Foundations of Analysis, American Mathematical Society.
Howie, John M., Real Analysis, Springer, 2005, ISBN 1-85233-314-6.
Schumacher, Carol (1996), ChapterZero / Fundamental Notions of Abstract Mathematics, Addison-Wesley, ISBN 0-201-82653-4 .
== External links ==
Hazewinkel, Michiel, ed. (2001), "Real number", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
The real numbers: Pythagoras to Stevin
The real numbers: Stevin to Hilbert
The real numbers: Attempts to understand
What are the "real numbers," really?
{{Navbox | name = Real numbers | state = {{{state|
PAGE	Shape	'Elementary geometry'
A shape is the form of an object or its external boundary, outline, or external surface, as opposed to other properties such as color, texture, material composition.
Psychologists have theorized that humans mentally break down images into simple geometric shapes called geons. Examples of geons include cones and spheres.
== Classification of simple shapes ==
Some simple shapes can be put into broad categories. For instance, polygons are classified according to their number of edges as triangles, quadrilaterals, pentagons, etc. Each of these is divided into smaller categories; triangles can be equilateral, isosceles, obtuse, acute, scalene, etc. while quadrilaterals can be rectangles, rhombi, trapezoids, squares, etc.
Other common shapes are points, lines, planes, and conic sections such as ellipses, circles, and parabolas.
Among the most common 3-dimensional shapes are polyhedra, which are shapes with flat faces; ellipsoids, which are egg-shaped or sphere-shaped objects; cylinders; and cones.
If an object falls into one of these categories exactly or even approximately, we can use it to describe the shape of the object. Thus, we say that the shape of a manhole cover is a circle, because it is approximately the same geometric object as an actual geometric circle.
== Shape in geometry ==
There are several ways to compare the shape of two objects:
Congruence: Two objects are congruent if one can be transformed into the other by a sequence of rotations, translations, and/or reflections.
Similarity: Two objects are similar if one can be transformed into the other by a uniform scaling, together with a sequence of rotations, translations, and/or reflections.
Isotopy: Two objects are isotopic if one can be transformed into the other by a sequence of deformations that do not tear the object or put holes in it.
Sometimes, two similar or congruent objects may be regarded as having a different shape if a reflection is required to transform one into the other. For instance, the letters "b" and "d" are a reflection of each other, and hence they are congruent and similar, but in some contexts they are not regarded as having the same shape. Sometimes, only the outline or external boundary of the object is considered to determine its shape. For instance, an hollow sphere may be considered to have the same shape as a solid sphere. Procrustes analysis is used in many sciences to determine whether or not two objects have the same shape, or to measure the difference between two shapes. In advanced mathematics, quasi-isometry can be used as a criterion to state that two shapes are approximately the same.
Simple shapes can often be classified into basic geometric objects such as a point, a line, a curve, a plane, a plane figure (e.g. square or circle), or a solid figure (e.g. cube or sphere). However, most shapes occurring in the physical world are complex. Some, such as plant structures and coastlines, may be so arbitrary as to defy traditional mathematical description &#8211; in which case they may be analyzed by differential geometry, or as fractals.
=== Equivalence of shapes ===
In geometry, two subsets of a Euclidean space have the same shape if one can be transformed to the other by a combination of translations, rotations (together also called rigid transformations), and uniform scalings. In other words, the shape of a set of points is all the geometrical information that is invariant to translations, rotations, and size changes. Having the same shape is an equivalence relation, and accordingly a precise mathematical definition of the notion of shape can be given as being an equivalence class of subsets of a Euclidean space having the same shape.
Mathematician and statistician David George Kendall writes:
In this paper &#8216; shape &#8217; is used in the vulgar sense, and means what one would normally expect it to mean. [...] We here define &#8216; shape &#8217; informally as &#8216; all the geometrical information that remains when location, scale and rotational effects are filtered out from an object. &#8217; 
Shapes of physical objects are equal if the subsets of space these objects occupy satisfy the definition above. In particular, the shape does not depend on the size and placement in space of the object. For instance, a "d" and a "p" have the same shape, as they can be perfectly superimposed if the "d" is translated to the right by a given distance, rotated upside down and magnified by a given factor (see Procrustes superimposition for details). However, a mirror image could be called a different shape. For instance, a "b" and a "p" have a different shape, at least when they are constrained to move within a two-dimensional space like the page on which they are written. Even though they have the same size, there's no way to perfectly superimpose them by translating and rotating them along the page. Similarly, within a three-dimensional space, a right hand and a left hand have a different shape, even if they are the mirror images of each other. Shapes may change if the object is scaled non uniformly. For example, a sphere becomes an ellipsoid when scaled differently in the vertical and horizontal directions. In other words, preserving axes of symmetry (if they exist) is important for preserving shapes. Also, shape is determined by only the outer boundary of an object.
=== Congruence and similarity ===
Objects that can be transformed into each other by rigid transformations and mirroring (but not scaling) are congruent. An object is therefore congruent to its mirror image (even if it is not symmetric), but not to a scaled version. Two congruent objects always have either the same shape or mirror image shapes, and have the same size.
Objects that have the same shape or mirror image shapes are called geometrically similar, whether or not they have the same size. Thus, objects that can be transformed into each other by rigid transformations, mirroring, and uniform scaling are similar. Similarity is preserved when one of the objects is uniformly scaled, while congruence is not. Thus, congruent objects are always geometrically similar, but similar objects may not be congruent, as they may have different size.
=== Homeomorphism ===
A more flexible definition of shape takes into consideration the fact that realistic shapes are often deformable, e.g. a person in different postures, a tree bending in the wind or a hand with different finger positions.
One way of modeling non-rigid movements is by homeomorphisms. Roughly speaking, a homeomorphism is a continuous stretching and bending of an object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a donut are not. An often-repeated mathematical joke is that topologists can't tell their coffee cup from their donut, since a sufficiently pliable donut could be reshaped to the form of a coffee cup by creating a dimple and progressively enlarging it, while preserving the donut hole in a cup's handle.
== Shape analysis ==
The above-mentioned mathematical definitions of rigid and non-rigid shape have arisen in the field of statistical shape analysis. In particular Procrustes analysis, which is a technique used for comparing shapes of similar objects (e.g bones of different animals), or measuring the deformation of a deformable object. Other methods are designed to work with non-rigid (bendable) objects, e.g. for posture independent shape retrieval (see for example Spectral shape analysis).
== Similarity classes ==
All similar triangles have the same shape. These shapes can be classified using complex numbers in a method advanced by J.A. Lester and Rafael Artzy. For example, an equilateral triangle can be expressed by complex numbers 0, 1, (1 + i &#8730; 3)/2. Lester and Artzy call the ratio
 the shape of triangle (u, v, w). Then the shape of the equilateral triangle is
(0 &#8211; (1+ &#8730; 3)/2)/(0 &#8211; 1) = ( 1 + i &#8730; 3)/2 = cos(60 &#176; ) + i sin(60 &#176; ) = exp( i &#960; /3).
For any affine transformation of the complex plane, a triangle is transformed but does not change its shape. Hence shape is an invariant of affine geometry. The shape p = S(u,v,w) depends on the order of the arguments of function S, but permutations lead to related values. For instance,
 Also 
Combining these permutations gives Furthermore,
 These relations are "conversion rules" for shape of a triangle.
The shape of a quadrilateral is associated with two complex numbers p,q. If the quadrilateral has vertices u,v,w,x, then p = S(u,v,w) and q = S(v,w,x). Artzy proves these propositions about quadrilateral shapes:
If then the quadrilateral is a parallelogram.
If a parallelogram has |arg p| = |arg q|, then it is a rhombus.
When p = 1 + i and q = (1 + i)/2, then the quadrilateral is square.
If and sgn r = sgn(Im p), then the quadrilateral is a trapezoid.
A polygon has a shape defined by n &#8211; 2 complex numbers The polygon bounds a convex set when all these shape components have imaginary components of the same sign.
== See also ==
Solid geometry
Glossary of shapes with metaphorical names
List of geometric shapes
== References ==
== External links ==
PAGE	Slope	'Elementary mathematics'
In mathematics, the slope or gradient of a line is a number that describes both the direction and the steepness of the line. Slope is often denoted by the letter m.
The direction of a line is either increasing, decreasing, horizontal or vertical.
A line is increasing if it goes up from left to right. The slope is positive, i.e. .
A line is decreasing if it goes down from left to right. The slope is negative, i.e. .
If a line is horizontal the slope is zero. This is a constant function.
If a line is vertical the slope is undefined (see below).
The steepness, incline, or grade of a line is measured by the absolute value of the slope. A slope with a greater absolute value indicates a steeper line
Slope is calculated by finding the ratio of the "vertical change" to the "horizontal change" between (any) two distinct points on a line. Sometimes the ratio is expressed as a quotient ("rise over run"), giving the same number for every two distinct points on the same line. A line that is decreasing has a negative "rise". The line may be practical - as set by a road surveyor, or in a diagram that models a road or a roof either as a description or as a plan.
The rise of a road between two points is the difference between the altitude of the road at those two points, say y1 and y2, or in other words, the rise is (y2 &#8722; y1) = &#916; y. For relatively short distances - where the earth's curvature may be neglected, the run is the difference in distance from a fixed point measured along a level, horizontal line, or in other words, the run is (x2 &#8722; x1) = &#916; x. Here the slope of the road between the two points is simply described as the ratio of the altitude change to the horizontal distance between any two points on the line.
In mathematical language, the slope m of the line is
The concept of slope applies directly to grades or gradients in geography and civil engineering. Through trigonometry, the grade m of a road is related to its angle of incline &#952; by the tangent function
Thus, a 45 &#176; rising line has a slope of +1 and a 45 &#176; falling line has a slope of &#8722; 1.
As a generalization of this practical description, the mathematics of differential calculus defines the slope of a curve at a point as the slope of the tangent line at that point. When the curve given by a series of points in a diagram or in a list of the coordinates of points, the slope may be calculated not at a point but between any two given points. When the curve is given as a continuous function, perhaps as an algebraic formula, then the differential calculus provides rules giving a formula for the slope of the curve at any point in the middle of the curve.
This generalization of the concept of slope allows very complex constructions to be planned and built that go well beyond static structures that are either horizontals or verticals, but can change in time, move in curves, and change depending on the rate of change of other factors. Thereby, the simple idea of slope becomes one of the main basis of the modern world in terms of both technology and the built environment.
== Definition ==
The slope of a line in the plane containing the x and y axes is generally represented by the letter m, and is defined as the change in the y coordinate divided by the corresponding change in the x coordinate, between two distinct points on the line. This is described by the following equation:
(The Greek letter delta, &#916; , is commonly used in mathematics to mean "difference" or "change".)
Given two points (x1,y1) and (x2,y2), the change in x from one to the other is x2 &#8722; x1 (run), while the change in y is y2 &#8722; y1 (rise). Substituting both quantities into the above equation generates the formula:
The formula fails for a vertical line, parallel to the y axis (see Division by zero), where the slope can be taken as infinite, so the slope of a vertical line is considered undefined.
=== Examples ===
Suppose a line runs through two points: P = (1, 2) and Q = (13, 8). By dividing the difference in y-coordinates by the difference in x-coordinates, one can obtain the slope of the line:
.
Since the slope is positive, the direction of the line is increasing. Since |m|<1, the incline is not very steep (incline <45 &#176; ).
As another example, consider a line which runs through the points (4, 15) and (3, 21). Then, the slope of the line is
Since the slope is negative, the direction of the line is decreasing. Since |m|>1, this decline is fairly steep (decline >45 &#176; ).
== Algebra and geometry ==
If y is a linear function of x, then the coefficient of x is the slope of the line created by plotting the function. Therefore, if the equation of the line is given in the form
then m is the slope. This form of a line's equation is called the slope-intercept form, because b can be interpreted as the y-intercept of the line, that is, the y-coordinate where the line intersects the y-axis.
If the slope m of a line and a point (x1,y1) on the line are both known, then the equation of the line can be found using the point-slope formula:
The slope of the line defined by the linear equation
is
.
Two lines are parallel if and only if their slopes are equal and they are not the same line (coincident) or if they both are vertical and therefore both have undefined slopes. Two lines are perpendicular if the product of their slopes is &#8722; 1 or one has a slope of 0 (a horizontal line) and the other has an undefined slope (a vertical line).
The angle &#952; between -90 &#176; and 90 &#176; that a line makes with the x-axis is related to the slope m as follows:
and
 (this is the inverse function of tangent; see trigonometry).
=== Examples ===
For example, consider a line running through the points (2,8) and (3,20). This line has a slope, m, of
One can then write the line's equation, in point-slope form:
or:
The angle &#952; between -90 &#176; and 90 &#176; that this line makes with the x axis is
Consider the two lines: y = -3x + 1 and y = -3 x - 2. Both lines have slope m = -3. They are not the same line. So they are parallel lines.
Consider the two lines y = -3x + 1 and y = x/3 - 2. The slope of the first line is m1 = -3. The slope of the second line is m2 = 1/3. The product of these two slopes is -1. So these two lines are perpendicular.
== Slope of a road or railway ==
Main articles: Grade (slope), Grade separation
There are two common ways to describe the steepness of a road or railroad. One is by the angle between 0 &#176; and 90 &#176; (in degrees), and the other is by the slope in a percentage. See also steep grade railway and rack railway.
The formulae for converting a slope given as a percentage into an angle in degrees and vice versa are:
 , (this is the inverse function of tangent; see trigonometry)
and
where angle is in degrees and the trigonometric functions operate in degrees. For example, a slope of 100% or 1000 &#8240; is an angle of 45 &#176; .
A third way is to give one unit of rise in say 10, 20, 50 or 100 horizontal units, e.g. 1:10. 1:20, 1:50 or 1:100 (or "1 in 10", "1 in 20" etc.) Note that 1:10 is steeper than 1:20. For example, steepness of 20% means 1:5 or an incline with angle 11,3 &#176; .
== Calculus ==
The concept of a slope is central to differential calculus. For non-linear functions, the rate of change varies along the curve. The derivative of the function at a point is the slope of the line tangent to the curve at the point, and is thus equal to the rate of change of the function at that point.
If we let &#916; x and &#916; y be the distances (along the x and y axes, respectively) between two points on a curve, then the slope given by the above definition,
,
is the slope of a secant line to the curve. For a line, the secant between any two points is the line itself, but this is not the case for any other type of curve.
For example, the slope of the secant intersecting y = x2 at (0,0) and (3,9) is 3. (The slope of the tangent at x = 3 &#8260; 2 is also 3 &#8212; a consequence of the mean value theorem.)
By moving the two points closer together so that &#916; y and &#916; x decrease, the secant line more closely approximates a tangent line to the curve, and as such the slope of the secant approaches that of the tangent. Using differential calculus, we can determine the limit, or the value that &#916; y/ &#916; x approaches as &#916; y and &#916; x get closer to zero; it follows that this limit is the exact slope of the tangent. If y is dependent on x, then it is sufficient to take the limit where only &#916; x approaches zero. Therefore, the slope of the tangent is the limit of &#916; y/ &#916; x as &#916; x approaches zero, or dy/dx. We call this limit the derivative.
Its value at a point on the function gives us the slope of the tangent at that point. For example, let y=x2. A point on this function is (-2,4). The derivative of this function is dy/dx=2x. So the slope of the line tangent to y at (-2,4) is 2 &#183; (-2) = -4. The equation of this tangent line is: y-4=(-4)(x-(-2)) or y = -4x - 4.
== Other generalizations ==
The concept of slope can be generalized to functions of more than one variable and is more often referred to as gradient.
== See also ==
Euclidean distance
Inclined plane
Linear function
Slope definitions
Theil &#8211; Sen estimator, a line with the median slope among a set of sample points
== References ==
== External links ==
"Slope of a Line (Coordinate Geometry)". Math Open Reference. 2009. Retrieved September 2013. interactive
PAGE	Sphere	'Elementary geometry'
A sphere (from Greek &#963; &#966; &#945; &#8150; &#961; &#945; &#8212; sphaira, "globe, ball") is a perfectly round geometrical and circular object in three-dimensional space that resembles the shape of a completely round ball. Like a circle, which, in geometric contexts, is in two dimensions, a sphere is defined mathematically as the set of points that are all the same distance r from a given point in three-dimensional space. This distance r is the radius of the sphere, and the given point is the center of the sphere. The maximum straight distance through the sphere passes through the center and is thus twice the radius; it is the diameter.
In mathematics, a distinction is made between the sphere (a two-dimensional closed surface embedded in three-dimensional Euclidean space) and the ball (a three-dimensional shape that includes the interior of a sphere).
== Area ==
The surface area of a sphere is:
Archimedes first derived this formula from the fact that the projection to the lateral surface of a circumscribed cylinder (i.e. the Lambert cylindrical equal-area projection) is area-preserving; it equals the derivative of the formula for the volume with respect to r because the total volume inside a sphere of radius r can be thought of as the summation of the surface area of an infinite number of spherical shells of infinitesimal thickness concentrically stacked inside one another from radius 0 to radius r. At infinitesimal thickness the discrepancy between the inner and outer surface area of any given shell is infinitesimal, and the elemental volume at radius r is simply the product of the surface area at radius r and the infinitesimal thickness.
At any given radius r, the incremental volume ( &#948; V) equals the product of the surface area at radius r (A(r)) and the thickness of a shell ( &#948; r):
The total volume is the summation of all shell volumes:
In the limit as &#948; r approaches zero this equation becomes:
Substitute V:
Differentiating both sides of this equation with respect to r yields A as a function of r:
Which is generally abbreviated as:
Alternatively, the area element on the sphere is given in spherical coordinates by . With Cartesian coordinates, the area element . More generally, see area element.
The total area can thus be obtained by integration:
== Enclosed volume ==
In 3 dimensions, the volume inside a sphere (that is, the volume of a ball) is derived to be
where r is the radius of the sphere and &#960; is the constant pi. Archimedes first derived this formula, which shows that the volume inside a sphere is 2/3 that of a circumscribed cylinder. (This assertion follows from Cavalieri's principle.) In modern mathematics, this formula can be derived using integral calculus, i.e. disk integration to sum the volumes of an infinite number of circular disks of infinitesimally small thickness stacked centered side by side along the x axis from x = 0 where the disk has radius r (i.e. y = r) to x = r where the disk has radius 0 (i.e. y = 0).
At any given x, the incremental volume ( &#948; V) equals the product of the cross-sectional area of the disk at x and its thickness ( &#948; x):
The total volume is the summation of all incremental volumes:
In the limit as &#948; x approaches zero this equation becomes:
At any given x, a right-angled triangle connects x, y and r to the origin; hence, applying the Pythagorean theorem yields:
Thus, substituting y with a function of x gives:
Which can now be evaluated as follows:
Therefore the volume of a sphere is:
Alternatively this formula is found using spherical coordinates, with volume element
so
For most practical purposes, the volume inside a sphere inscribed in a cube can be approximated as 52.4% of the volume of the cube, since . For example, a sphere with diameter 1m has 52.4% the volume of a cube with edge length 1m, or about 0.524m3.
In higher dimensions, the sphere (or hypersphere) is usually called an n-ball. General recursive formulas exist for the volume of an n-ball.
== Equations in ==
In analytic geometry, a sphere with center (x0, y0, z0) and radius r is the locus of all points (x, y, z) such that
The points on the sphere with radius r can be parameterized via
(see also trigonometric functions and spherical coordinates).
A sphere of any radius centered at zero is an integral surface of the following differential form:
This equation reflects that position and velocity vectors of a point traveling on the sphere are always orthogonal to each other.
The sphere has the smallest surface area of all surfaces that enclose a given volume, and it encloses the largest volume among all closed surfaces with a given surface area. The sphere therefore appears in nature: for example, bubbles and small water drops are roughly spherical because the surface tension locally minimizes surface area.
The surface area relative to the mass of a sphere is called the specific surface area and can be expressed from the above stated equations as
where is the ratio of mass to volume.
A sphere can also be defined as the surface formed by rotating a circle about any diameter. Replacing the circle with an ellipse rotated about its major axis, the shape becomes a prolate spheroid; rotated about the minor axis, an oblate spheroid.
== Terminology ==
Pairs of points on a sphere that lie on a straight line through the sphere's center are called antipodal points. A great circle is a circle on the sphere that has the same center and radius as the sphere and consequently divides it into two equal parts. The shortest distance along the surface between two distinct non-antipodal points on the surface is on the unique great circle that includes the two points. Equipped with the great-circle distance, a great circle becomes the Riemannian circle.
If a particular point on a sphere is (arbitrarily) designated as its north pole, then the corresponding antipodal point is called the south pole, and the equator is the great circle that is equidistant to them. Great circles through the two poles are called lines (or meridians) of longitude, and the line connecting the two poles is called the axis of rotation. Circles on the sphere that are parallel to the equator are lines of latitude. This terminology is also used for such approximately spheroidal astronomical bodies as the planet Earth (see geoid).
== Hemisphere ==
 Any plane that includes the center of a sphere divides it into two equal hemispheres. Any two intersecting planes that include the center of a sphere subdivide the sphere into four lunes or biangles, the vertices of which all coincide with the antipodal points lying on the line of intersection of the planes.
The antipodal quotient of the sphere is the surface called the real projective plane, which can also be thought of as the northern hemisphere with antipodal points of the equator identified.
The round hemisphere is conjectured to be the optimal (least area) filling of the Riemannian circle.
The circles of intersection of any plane not intersecting the sphere's center and the sphere's surface are called spheric sections.
== Generalization to other dimensions ==
Spheres can be generalized to spaces of any dimension. For any natural number n, an "n-sphere," often written as , is the set of points in (n + 1)-dimensional Euclidean space that are at a fixed distance r from a central point of that space, where r is, as before, a positive real number. In particular:
 : a 0-sphere is a pair of endpoints of an interval ( &#8722; r, r) of the real line
 : a 1-sphere is a circle of radius r
 : a 2-sphere is an ordinary sphere
 : a 3-sphere is a sphere in 4-dimensional Euclidean space.
Spheres for n > 2 are sometimes called hyperspheres.
The n-sphere of unit radius centered at the origin is denoted Sn and is often referred to as "the" n-sphere. Note that the ordinary sphere is a 2-sphere, because it is a 2-dimensional surface (which is embedded in 3-dimensional space).
The surface area of the (n &#8722; 1)-sphere of radius 1 is
where &#915; (z) is Euler's Gamma function.
Another expression for the surface area is
and the volume is the surface area times or
== Generalization to metric spaces ==
More generally, in a metric space (E,d), the sphere of center x and radius r > 0 is the set of points y such that d(x,y) = r.
If the center is a distinguished point that is considered to be the origin of E, as in a normed space, it is not mentioned in the definition and notation. The same applies for the radius if it is taken to equal one, as in the case of a unit sphere.
Unlike a ball, even a large sphere may be an empty set. For example, in Zn with Euclidean metric, a sphere of radius r is nonempty only if r2 can be written as sum of n squares of integers.
== Topology ==
In topology, an n-sphere is defined as a space homeomorphic to the boundary of an (n + 1)-ball; thus, it is homeomorphic to the Euclidean n-sphere, but perhaps lacking its metric.
a 0-sphere is a pair of points with the discrete topology
a 1-sphere is a circle (up to homeomorphism); thus, for example, (the image of) any knot is a 1-sphere
a 2-sphere is an ordinary sphere (up to homeomorphism); thus, for example, any spheroid is a 2-sphere
The n-sphere is denoted Sn. It is an example of a compact topological manifold without boundary. A sphere need not be smooth; if it is smooth, it need not be diffeomorphic to the Euclidean sphere.
The Heine &#8211; Borel theorem implies that a Euclidean n-sphere is compact. The sphere is the inverse image of a one-point set under the continuous function ||x||. Therefore, the sphere is closed. Sn is also bounded; therefore it is compact.
Smale's paradox shows that it is possible to turn an ordinary sphere inside out in a three-dimensional space with possible self-intersections but without creating any crease, a process more commonly and historically called sphere eversion.
== Spherical geometry ==
The basic elements of Euclidean plane geometry are points and lines. On the sphere, points are defined in the usual sense, but the analogue of "line" may not be immediately apparent. Measuring by arc length yields that the shortest path between two points that entirely lie in the sphere is a segment of the great circle the includes the points; see geodesic. Many, but not all (see parallel postulate) theorems from classical geometry hold true for this spherical geometry as well. In spherical trigonometry, angles are defined between great circles. Thus spherical trigonometry differs from ordinary trigonometry in many respects. For example, the sum of the interior angles of a spherical triangle exceeds 180 degrees. Also, any two similar spherical triangles are congruent.
== Eleven properties of the sphere ==
In their book Geometry and the imagination David Hilbert and Stephan Cohn-Vossen describe eleven properties of the sphere and discuss whether these properties uniquely determine the sphere. Several properties hold for the plane, which can be thought of as a sphere with infinite radius. These properties are:
The points on the sphere are all the same distance from a fixed point. Also, the ratio of the distance of its points from two fixed points is constant.
The first part is the usual definition of the sphere and determines it uniquely. The second part can be easily deduced and follows a similar result of Apollonius of Perga for the circle. This second part also holds for the plane.
The contours and plane sections of the sphere are circles.
This property defines the sphere uniquely.
The sphere has constant width and constant girth.
The width of a surface is the distance between pairs of parallel tangent planes. Numerous other closed convex surfaces have constant width, for example the Meissner body. The girth of a surface is the circumference of the boundary of its orthogonal projection on to a plane. Each of these properties implies the other.
All points of a sphere are umbilics.
At any point on a surface a normal direction is at right angles to the surface because the sphere these are the lines radiating out from the center of the sphere. The intersection of a plane that contains the normal with the surface will form a curve that is called a normal section, and the curvature of this curve is the normal curvature. For most points on most surfaces, different sections will have different curvatures; the maximum and minimum values of these are called the principal curvatures. Any closed surface will have at least four points called umbilical points. At an umbilic all the sectional curvatures are equal; in particular the principal curvatures are equal. Umbilical points can be thought of as the points where the surface is closely approximated by a sphere.
For the sphere the curvatures of all normal sections are equal, so every point is an umbilic. The sphere and plane are the only surfaces with this property.
The sphere does not have a surface of centers.
For a given normal section exists a circle of curvature that equals the sectional curvature, is tangent to the surface, and the center lines of which lie along on the normal line. For example, the two centers corresponding to the maximum and minimum sectional curvatures are called the focal points, and the set of all such centers forms the focal surface.
For most surfaces the focal surface forms two sheets that are each a surface and meet at umbilical points. Several cases are special:
For channel surfaces one sheet forms a curve and the other sheet is a surface
For cones, cylinders, tori and cyclides both sheets form curves.
For the sphere the center of every osculating circle is at the center of the sphere and the focal surface forms a single point. This property is unique to the sphere.
All geodesics of the sphere are closed curves.
Geodesics are curves on a surface that give the shortest distance between two points. They are a generalization of the concept of a straight line in the plane. For the sphere the geodesics are great circles. Many other surfaces share this property.
Of all the solids having a given volume, the sphere is the one with the smallest surface area; of all solids having a given surface area, the sphere is the one having the greatest volume.
It follows from isoperimetric inequality. These properties define the sphere uniquely and can be seen in soap bubbles: a soap bubble will enclose a fixed volume, and surface tension minimizes its surface area for that volume. A freely floating soap bubble therefore approximates a sphere (though such external forces as gravity will slightly distort the bubble's shape).
The sphere has the smallest total mean curvature among all convex solids with a given surface area.
The mean curvature is the average of the two principal curvatures, which is constant because the two principal curvatures are constant at all points of the sphere.
The sphere has constant mean curvature.
The sphere is the only imbedded surface that lacks boundary or singularities with constant positive mean curvature. Other such immersed surfaces as minimal surfaces have constant mean curvature.
The sphere has constant positive Gaussian curvature.
Gaussian curvature is the product of the two principal curvatures. It is an intrinsic property that can be determined by measuring length and angles and is independent of how the surface is embedded in space. Hence, bending a surface will not alter the Gaussian curvature, and other surfaces with constant positive Gaussian curvature can be obtained by cutting a small slit in the sphere and bending it. All these other surfaces would have boundaries, and the sphere is the only surface that lacks a boundary with constant, positive Gaussian curvature. The pseudosphere is an example of a surface with constant negative Gaussian curvature.
The sphere is transformed into itself by a three-parameter family of rigid motions.
Rotating around any axis a unit sphere at the origin will map the sphere onto itself. Any rotation about a line through the origin can be expressed as a combination of rotations around the three-coordinate axis (see Euler angles). Therefore a three-parameter family of rotations exists such that each rotation transforms the sphere onto itself; this family is the rotation group SO(3). The plane is the only other surface with a three-parameter family of transformations (translations along the x and y axis and rotations around the origin). Circular cylinders are the only surfaces with two-parameter families of rigid motions and the surfaces of revolution and helicoids are the only surfaces with a one-parameter family.
== Cubes in relation to spheres ==
For every sphere there are multiple cuboids that may be inscribed within the sphere. The largest cuboid which can be inscribed within a sphere is a cube.
== See also ==
== References ==
William Dunham. "Pages 28, 226", The Mathematical Universe: An Alphabetical Journey Through the Great Proofs, Problems and Personalities, ISBN 0-471-17661-3.
== External links ==
Sphere (PlanetMath.org website)
Weisstein, Eric W., "Sphere", MathWorld.
Mathematica/Uniform Spherical Distribution
Outside In. 2007-11-14. Retrieved 2007-11-24. (computer animation showing how the inside of a sphere can turn outside.)
Program in C++ to draw a sphere using parametric equation
Surface area of sphere proof.
PAGE	Square Roots	'Elementary mathematics'
Square Roots (previously called the Folk & Roots Festival) is a two- to three-day music festival that has been held each summer in the Lincoln Square neighborhood in Chicago since 1998. Organized by the Old Town School of Folk Music and the Lincoln Square Chamber of Commerce, the festival features world music and dance performances from a variety of genres with particular emphasis on folk and world music.
== History ==
The Folk & Roots Festival began in 1998 and was coordinated solely by the Old Town School of Folk Music. Each year the event was held at Welles Park in Lincoln Square. The festival showcased a variety of performances from different musical traditions. For instance, in 2001, the festival hosted, among others, the Super Rail Band from Mali, Nigerian afrobeat musician Femi Kuti, and the Texan country band The Flatlanders. It also featured well-known folk performers as well, such as Patti Smith and Richard Thompson. The lineup of performances at Folk & Roots festivals have been described as "eclectic." In addition to performances, The Folk & Roots Festival also held dance workshops and events for children.
In 2012, the Old Town School of Folk Music announced they would not be organizing the Folk & Roots Festival in 2012. This decision may have been based on changes in city policy that would have added significant financial burden to the school, such as increased costs in permits and rentals in Welles Park for performance space and no longer allowing vendors at the festival to use the city's power grid for electrical needs. However, Old Town School of Folk Music's executive director stated that the reason for the cancellation was to create a new festival that would highlight the school's new building completed in 2011. It is not clear if the festival will be planned for 2013, but the Old Town School of Folk Music director, Bau Graves, noted that he wouldn't "close the door" on bringing Folk & Roots back in the future.
== Square Roots ==
Like the Folk and Roots Festival, Square Roots has also hosted performances from a variety of music traditions. In its first year in 2012, Square Roots hosted the country group Waco Brothers, Malian blues singer Sidi Tour &#233; , funk band Kong Fuzi, and Colorado folk quintet Elephant Revival, among others.
Another focus of the festival is on supporting local businesses. Lincoln Square Chamber of Commerce vice president Jason Kraus has expressed that ideally, 80 to 100 percent of the food vendors would be from neighborhoods around Lincoln Square, such as Ravenswood or Northcenter. In 2012, Square Roots invited numerous local restaurants and breweries as vendors during the festival. Square Roots is expected to continue in 2013.
In 2012, entrance to the Square Roots fest was as suggested $10 donation for adults, and was discounted for children and seniors. Proceeds went to the Old Town School of Folk Music and the Lincoln Square Chamber of Commerce to benefit music programs, summer concerts, music education, and local farmer's markets in the area.
== References ==
PAGE	Square root	'Elementary mathematics'
In mathematics, a square root of a number a is a number y such that y2 = a, in other words, a number y whose square (the result of multiplying the number by itself, or y &#215; y) is a. For example, 4 and &#8722; 4 are square roots of 16 because 42 = ( &#8722; 4)2 = 16.
Every non-negative real number a has a unique non-negative square root, called the principal square root, which is denoted by &#8730; a, where &#8730; is called the radical sign or radix. For example, the principal square root of 9 is 3, denoted &#8730; 9 = 3, because 32 = 3 &#215; 3 = 9 and 3 is non-negative. The term whose root is being considered is known as the radicand. The radicand is the number or expression underneath the radical sign, in this example 9.
Every positive number a has two square roots: &#8730; a, which is positive, and &#8722; &#8730; a, which is negative. Together, these two roots are denoted &#177; &#8730; a (see &#177; shorthand). Although the principal square root of a positive number is only one of its two square roots, the designation "the square root" is often used to refer to the principal square root. For positive a, the principal square root can also be written in exponent notation, as a1/2.
Square roots of negative numbers can be discussed within the framework of complex numbers. More generally, square roots can be considered in any context in which a notion of "squaring" of some mathematical objects is defined (including algebras of matrices, endomorphism rings, etc.)
== History ==
The Yale Babylonian Collection YBC 7289 clay tablet was created between 1800 BC and 1600 BC, showing &#8730; 2 and 30 &#8730; 2 as 1;24,51,10 and 42;25,35 base 60 numbers on a square crossed by two diagonals.
The Rhind Mathematical Papyrus is a copy from 1650 BC of an even earlier work and shows how the Egyptians extracted square roots.
In Ancient India, the knowledge of theoretical and applied aspects of square and square root was at least as old as the Sulba Sutras, dated around 800 &#8211; 500 BC (possibly much earlier). A method for finding very good approximations to the square roots of 2 and 3 are given in the Baudhayana Sulba Sutra. Aryabhata in the Aryabhatiya (section 2.4), has given a method for finding the square root of numbers having many digits.
It was known to the ancient Greeks that square roots of positive whole numbers that are not perfect squares are always irrational numbers: numbers not expressible as a ratio of two integers (that is to say they cannot be written exactly as m/n, where m and n are integers). This is the theorem Euclid X, 9 almost certainly due to Theaetetus dating back to circa 380 BC. The particular case &#8730; 2 is assumed to date back earlier to the Pythagoreans and is traditionally attributed to Hippasus. It is exactly the length of the diagonal of a square with side length 1.
In the Chinese mathematical work Writings on Reckoning, written between 202 BC and 186 BC during the early Han Dynasty, the square root is approximated by using an "excess and deficiency" method, which says to "...combine the excess and deficiency as the divisor; (taking) the deficiency numerator multiplied by the excess denominator and the excess numerator times the deficiency denominator, combine them as the dividend."
Mah &#257; v &#299; ra, a 9th-century Indian mathematician, was the first to state that square roots of negative numbers do not exist.
A symbol for square roots, written as an elaborate R, was invented by Regiomontanus (1436 &#8211; 1476). An R was also used for Radix to indicate square roots in Giralamo Cardano's Ars Magna.
According to historian of mathematics D.E. Smith, Aryabhata's method for finding the square root was first introduced in Europe by Cataneo in 1546.
The symbol ' &#8730; ' for the square root was first used in print in 1525 in Christoph Rudolff's Coss, which was also the first to use the then-new signs '+' and ' &#8722; '.
== Properties and uses ==
The principal square root function f(x) = &#8730; x (usually just referred to as the "square root function") is a function that maps the set of non-negative real numbers onto itself. In geometrical terms, the square root function maps the area of a square to its side length.
The square root of x is rational if and only if x is a rational number that can be represented as a ratio of two perfect squares. (See square root of 2 for proofs that this is an irrational number, and quadratic irrational for a proof for all non-square natural numbers.) The square root function maps rational numbers into algebraic numbers (a superset of the rational numbers).
For all real numbers x
 (see absolute value)
For all non-negative real numbers x and y,
and
The square root function is continuous for all non-negative x and differentiable for all positive x. If f denotes the square-root function, its derivative is given by:
The Taylor series of &#8730; 1 + x about x = 0 converges for |x| &#8804; 1 and is given by
Square root of a non-negative number is used in the definition of Euclidean norm (and distance), as well as in generalizations such as Hilbert spaces. It defines an important concept of standard deviation used in probability theory and statistics. It has a major use in the formula for roots of a quadratic equation; quadratic fields and rings of quadratic integers, which are based on square roots, are important in algebra and have uses in geometry. Square roots frequently appear in mathematical formulas elsewhere, as well as in many physical laws.
== Computation ==
Most pocket calculators have a square root key. Computer spreadsheets and other software are also frequently used to calculate square roots. Pocket calculators typically implement efficient routines, such as the Newton's method (frequently with an initial guess of 1), to compute the square root of a positive real number. When computing square roots with logarithm tables or slide rules, one can exploit the identity
 or 
where and are the natural and base-10 logarithms.
By trial-and-error, one can square an estimate for &#8730; a and raise or lower the estimate until it agrees to sufficient accuracy. For this technique it's prudent to use the identity
as it allows one to adjust the estimate x by some amount c and measure the square of the adjustment in terms of the original estimate and its square. Furthermore, when c is close to 0, because the tangent line to the graph of at c=0, as a function of c alone, is . Thus, small adjustments to x can be planned out by setting to , or .
The most common iterative method of square root calculation by hand is known as the "Babylonian method" or "Heron's method" after the first-century Greek philosopher Heron of Alexandria, who first described it. The method uses the same iterative scheme as the Newton &#8211; Raphson method yields when applied to the function y = f(x)=x2 &#8722; a, using the fact that its slope at any point is but predates it by many centuries. The algorithm is to repeat a simple calculation that results in a number closer to the actual square root each time it is repeated with its result as the new input. The motivation is that if x is an overestimate to the square root of a non-negative real number a then a/x will be an underestimate and so the average of these two numbers is a better approximation than either of them. However, the inequality of arithmetic and geometric means shows this average is always an overestimate of the square root (as noted below), and so it can serve as a new overestimate with which to repeat the process, which converges as a consequence of the successive overestimates and underestimates being closer to each other after each iteration. To find x :
Start with an arbitrary positive start value x. The closer to the square root of a, the fewer the iterations that will be needed to achieve the desired precision.
Replace x by the average (x + a/x) / 2 between x and a/x.
Repeat from step 2, using this average as the new value of x.
That is, if an arbitrary guess for &#8730; a is , and xn+1 = (xn + a/xn)/2, then each xn is an approximation of &#8730; a which is better for large n than for small n. If a is positive, the convergence is quadratic, which means that in approaching the limit, the number of correct digits roughly doubles in each next iteration. If a = 0, the convergence is only linear.
Using the identity
the computation of the square root of a positive number can be reduced to that of a number in the range [1,4). This simplifies finding a start value for the iterative method that is close to the square root, for which a polynomial or piecewise-linear approximation can be used.
The time complexity for computing a square root with n digits of precision is equivalent to that of multiplying two n-digit numbers.
Another useful method for calculating the square root is the Shifting nth root algorithm, applied for n = 2.
== Square roots of negative and complex numbers ==
The square of any positive or negative number is positive, and the square of 0 is 0. Therefore, no negative number can have a real square root. However, it is possible to work with a more inclusive set of numbers, called the complex numbers, that does contain solutions to the square root of a negative number. This is done by introducing a new number, denoted by i (sometimes j, especially in the context of electricity where "i" traditionally represents electric current) and called the imaginary unit, which is defined such that i2 = &#8722; 1. Using this notation, we can think of i as the square root of &#8722; 1, but notice that we also have ( &#8722; i)2 = i2 = &#8722; 1 and so &#8722; i is also a square root of &#8722; 1. By convention, the principal square root of &#8722; 1 is i, or more generally, if x is any non-negative number, then the principal square root of &#8722; x is
The right side (as well as its negative) is indeed a square root of &#8722; x, since
For every non-zero complex number z there exist precisely two numbers w such that w2 = z: the principal square root of z (defined below), and its negative.
=== Square root of an imaginary number ===
The square root of i is given by
This result can be obtained algebraically by finding a and b such that
or equivalently
This gives the two simultaneous equations
with solutions
The choice of the principal root then gives
The result can also be obtained by using de Moivre's formula and setting
which produces
=== Principal square root of a complex number ===
To find a definition for the square root that allows us to consistently choose a single value, called the principal value, we start by observing that any complex number x + iy can be viewed as a point in the plane, (x, y), expressed using Cartesian coordinates. The same point may be reinterpreted using polar coordinates as the pair (r, &#966; ), where r &#8805; 0 is the distance of the point from the origin, and &#966; is the angle that the line from the origin to the point makes with the positive real (x) axis. In complex analysis, this value is conventionally written r &#8201; ei &#966; . If
then we define the principal square root of z as follows:
The principal square root function is thus defined using the nonpositive real axis as a branch cut. The principal square root function is holomorphic everywhere except on the set of non-positive real numbers (on strictly negative reals it isn't even continuous). The above Taylor series for &#8730; 1 + x remains valid for complex numbers x with |x| < 1.
The above can also be expressed in terms of trigonometric functions:
=== Algebraic formula ===
When the number is expressed using Cartesian coordinates the following formula can be used for the principal square root:
The sign of the imaginary part of the root is taken to be the same as the sign of the imaginary part of the original number. The real part of the principal value is always non-negative.
As the other square root is simply &#8722; 1 times the principal square root, both roots can be written as
=== Notes ===
Because of the discontinuous nature of the square root function in the complex plane, the law &#8730; zw = &#8730; z &#8730; w is in general not true. (Equivalently, the problem occurs because of the freedom in the choice of branch. The chosen branch may or may not yield the equality; in fact, the choice of branch for the square root need not contain the value of &#8730; z &#8730; w at all, leading to the equality's failure. A similar problem appears with the complex logarithm and the relation log &#8201; z + log &#8201; w = log(zw).) Wrongly assuming this law underlies several faulty "proofs", for instance the following one showing that &#8722; 1 = 1:
The third equality cannot be justified (see invalid proof). It can be made to hold by changing the meaning of &#8730; so that this no longer represents the principal square root (see above) but selects a branch for the square root that contains ( &#8730; &#8722; 1) &#183; ( &#8730; &#8722; 1). The left-hand side becomes either
if the branch includes +i or
if the branch includes &#8722; i, while the right-hand side becomes
where the last equality, &#8730; 1 = &#8722; 1, is a consequence of the choice of branch in the redefinition of &#8730; .
== Square roots of matrices and operators ==
If A is a positive-definite matrix or operator, then there exists precisely one positive definite matrix or operator B with B2 = A; we then define A1/2 = &#8730; A = B. In general matrices may have multiple square roots or even an infinitude of them. For example the 2 &#215; 2 identity matrix has an infinity of square roots.
== In integral domains, including fields ==
Each element of an integral domain has no more than 2 square roots. The difference of two squares identity u2 &#8722; v2 = (u &#8722; v)(u + v) is proved using the commutativity of multiplication. If u and v are square roots of the same element, then u2 &#8722; v2 = 0. Because there are no zero divisors this implies u = v or u + v = 0, where the latter means that two roots are additive inverses of each other. In other words, the square root of an element, if it exists, is unique up to a sign. The only square root of 0 in an integral domain is 0 itself.
In a field of characteristic 2, an element has either one square root, because each element is its own additive inverse, or does not have any at all (if the field is finite of characteristic 2 then every element has a unique square root). In a field of any other characteristic, any non-zero element either has two square roots, as explained above, or does not have any.
Given an odd prime number p, let q = pe for some positive integer e. A non-zero element of the field Fq with q elements is a quadratic residue if it is has a square root in Fq. Otherwise, it is a quadratic non-residue. There are (q &#8722; 1)/2 quadratic residues and (q &#8722; 1)/2 quadratic non-residues; zero is not counted in either class. The quadratic residues form a group under multiplication. The properties of quadratic residues are widely used in number theory.
== In rings in general ==
In a ring we call an element b a square root of a iff b2 = a. To see that the square root need not be unique up to sign in a general ring, consider the ring from modular arithmetic. Here, the element 1 has four distinct square roots, namely &#177; 1 and &#177; 3. On the other hand, the element 2 has no square root. See also the article quadratic residue for details.
Another example is provided by the quaternions in which the element &#8722; 1 has an infinitude of square roots including &#177; i, &#177; j, and &#177; k.
In fact, the set of square roots of &#8722; 1 is exactly
Hence this set is exactly the same size and shape as the unit sphere in 3-space.
The square root of 0 is by definition either 0 or a zero divisor, and where zero divisors do not exist (such as in quaternions and, generally, in division algebras), it is uniquely 0. It is not necessarily true in general rings, where Z/n2Z for any natural n provides an easy counterexample.
== Principal square roots of the positive integers ==
=== As decimal expansions ===
The square roots of the perfect squares (1, 4, 9, 16, etc.) are integers. In all other cases, the square roots of positive integers are irrational numbers, and therefore their decimal representations are non-repeating decimals.
Note that if the radicand is not square-free, then one can factorize, for example ; ; and .
=== As expansions in other numeral systems ===
The square roots of the perfect squares (1, 4, 9, 16, etc.) are integers. In all other cases, the square roots are irrational numbers, and therefore their representations in any standard positional notation system are non-repeating.
The square roots of small integers are used in both the SHA-1 and SHA-2 hash function designs to provide nothing up my sleeve numbers.
=== As periodic continued fractions ===
One of the most intriguing results from the study of irrational numbers as continued fractions was obtained by Joseph Louis Lagrange c. 1780. Lagrange found that the representation of the square root of any non-square positive integer as a continued fraction is periodic. That is, a certain pattern of partial denominators repeats indefinitely in the continued fraction. In a sense these square roots are the very simplest irrational numbers, because they can be represented with a simple repeating pattern of integers.
The square bracket notation used above is a sort of mathematical shorthand to conserve space. Written in more traditional notation the simple continued fraction for the square root of 11, [3; 3, 6, 3, 6, ...], looks like this:
where the two-digit pattern {3, 6} repeats over and over again in the partial denominators. Since 11 = 32 + 2, the above is also identical to the following generalized continued fractions:
== Geometric construction of the square root ==
Square root of a positive number is usually defined as the side length of a square with the area equal to the given number. But the square shape is not necessary for it: if one of two similar planar Euclidean objects has the area a times greater than another, then the ratio of their linear sizes is &#8730; a.
A square root can be constructed with a compass and straightedge. In his Elements, Euclid (fl. 300 BC) gave the construction of the geometric mean of two quantities in two different places: Proposition II.14 and Proposition VI.13. Since the geometric mean of a and b is , one can construct simply by taking b = 1.
The construction is also given by Descartes in his La G &#233; om &#233; trie, see figure 2 on page 2. However, Descartes made no claim to originality and his audience would have been quite familiar with Euclid.
Euclid's second proof in Book VI depends on the theory of similar triangles. Let AHB be a line segment of length a + b with AH = a and HB = b. Construct the circle with AB as diameter and let C be one of the two intersections of the perpendicular chord at H with the circle and denote the length CH as h. Then, using Thales' theorem and, as in the proof of Pythagoras' theorem by similar triangles, triangle AHC is similar to triangle CHB (as indeed both are to triangle ACB, though we don't need that, but it is the essence of the proof of Pythagoras' theorem) so that AH:CH is as HC:HB, i.e. from which we conclude by cross-multiplication that and finally that . Note further that if you were to mark the midpoint O of the line segment AB and draw the radius OC of length then clearly OC > CH, i.e. (with equality if and only if a = b), which is the arithmetic &#8211; geometric mean inequality for two variables and, as noted above, is the basis of the Ancient Greek understanding of "Heron's method".
Another method of geometric construction uses right triangles and induction: &#8730; 1 can, of course, be constructed, and once &#8730; x has been constructed, the right triangle with 1 and &#8730; x for its legs has a hypotenuse of &#8730; x + 1. The Spiral of Theodorus is constructed using successive square roots in this manner.
== See also ==
Apotome (mathematics)
Cube root
Integer square root
List of square roots
Methods of computing square roots
Nested radical
Nth root
Quadratic irrational
Root of unity
Solving quadratic equations with continued fractions
Square root principle
== Notes ==
== References ==
Dauben, Joseph W. (2007). "Chinese Mathematics I". In Katz, Victor J. The Mathematics of Egypt, Mesopotamia, China, India, and Islam. Princeton: Princeton University Press. ISBN 0-691-11485-4. 
Gel'fand, Izrael M.; Shen, Alexander (1993). Algebra (3rd ed.). Birkh &#228; user. p. 120. ISBN 0-8176-3677-3. 
Joseph, George (2000). The Crest of the Peacock. Princeton: Princeton University Press. ISBN 0-691-00659-8. 
Smith, David (1958). History of Mathematics 2. New York: Dover Publications. ISBN 978-0-486-20430-7. 
Selin, Helaine (2008), Encyclopaedia of the History of Science, Technology, and Medicine in Non-Western Cultures, Springer, ISBN 978-1-4020-4559-2 
== External links ==
Algorithms, implementations, and more &#8211; Paul Hsieh's square roots webpage
How to manually find a square root
PAGE	System of linear equations	'Equations'
In mathematics, a system of linear equations (or linear system) is a collection of linear equations involving the same set of variables. For example,
is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of numbers to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by
since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.
In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.
Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gr &#246; bner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.
== Elementary example ==
The simplest kind of linear system involves two equations and two variables:
One method for solving such a system is as follows. First, solve the top equation for in terms of :
Now substitute this expression for x into the bottom equation:
This results in a single equation involving only the variable . Solving gives , and substituting this back into the equation for yields . This method generalizes to systems with additional variables (see "elimination of variables" below, or the article on elementary algebra.)
== General form ==
A general system of m linear equations with n unknowns can be written as
Here are the unknowns, are the coefficients of the system, and are the constant terms.
Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.
=== Vector equation ===
One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.
This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.
=== Matrix equation ===
The vector equation is equivalent to a matrix equation of the form
where A is an m &#215; n matrix, x is a column vector with n entries, and b is a column vector with m entries.
The number of vectors in a basis for the span is now expressed as the rank of the matrix.
== Solution set ==
A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.
A linear system may behave in any one of three possible ways:
The system has infinitely many solutions.
The system has a single unique solution.
The system has no solution.
=== Geometric interpretation ===
For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.
For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.
For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, which may be a flat of any dimension.
=== General behavior ===
In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns:
Usually, a system with fewer equations than unknowns has infinitely many solutions, but it may have no solution. Such a system is known as an underdetermined system.
Usually, a system with the same number of equations and unknowns has a single unique solution.
Usually, a system with more equations than unknowns has no solution. Such a system is also known as an overdetermined system.
In the first case, the dimension of the solution set is usually equal to n &#8722; m, where n is the number of variables and m is the number of equations.
The following pictures illustrate this trichotomy in the case of two variables:
The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.
Keep in mind that the pictures above show only the most common case. It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). In general, a system of linear equations may behave differently from expected if the equations are linearly dependent, or if two or more of the equations are inconsistent.
== Properties ==
=== Independence ===
The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.
For example, the equations
are not independent &#8212; they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.
For a more complicated example, the equations
are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.
=== Consistency ===
A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent . When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten such as the statement 0 = 1.
For example, the equations
are inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.
It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equations
are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.
In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.
Putting it another way, according to the Rouch &#233; &#8211; Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.
=== Equivalence ===
Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice-versa. Two systems are equivalent if either both are inconsistent or each equation of any of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.
== Solving a linear system ==
There are several algorithms for solving a system of linear equations.
=== Describing the solution ===
When the solution set is finite, it is reduced to a single element. In this case, the unique solution is described by a sequence of equations whose left hand sides are the names of the unknowns and right hand sides are the corresponding values, for example . When an order on the unknowns has been fixed, for example the alphabetical order the solution may be described as a vector of values, like for the previous example.
It can be difficult to describe a set with infinite solutions. Typically, some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.
For example, consider the following system:
The solution set to this system can be described by the following equations:
Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.
Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.
Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:
Here x is the free variable, and y and z are dependent.
=== Elimination of variables ===
The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:
In the first equation, solve for one of the variables in terms of the others.
Substitute this expression into the remaining equations. This yields a system of equations with one fewer equation and one fewer unknown.
Continue until you have reduced the system to a single linear equation.
Solve this equation, and then back-substitute until the entire solution is found.
For example, consider the following system:
Solving the first equation for x gives x = 5 + 2z &#8722; 3y, and plugging this into the second and third equation yields
Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:
Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = &#8722; 15. Therefore, the solution set is the single point (x, y, z) = ( &#8722; 15, 8, 2).
=== Row reduction ===
In row reduction, the linear system is represented as an augmented matrix:
This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:
Type 1: Swap the positions of two rows.
Type 2: Multiply a row by a nonzero scalar.
Type 3: Add to one row a scalar multiple of another.
Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.
There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:
The last matrix is in reduced row echelon form, and represents the system x = &#8722; 15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.
=== Cramer's rule ===
Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the system
is given by
For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.
Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.
=== Matrix solution ===
If the equation system is expressed in the matrix form , the entire solution set can also be expressed in matrix form. If the matrix A is square (has m rows and n=m columns) and has full rank (all m rows are independent), then the system has a unique solution given by
where is the inverse of A. More generally, regardless of whether m=n or not and regardless of the rank of A, all solutions (if any exist) are given using the Moore-Penrose pseudoinverse of A, denoted , as follows:
where is a vector of free parameters that ranges over all possible n &#215; 1 vectors. A necessary and sufficient condition for any solution(s) to exist is that the potential solution obtained using satisfy &#8212; that is, that If this condition does not hold, the equation system is inconsistent and has no solution. If the condition holds, the system is consistent and at least one solution exists. For example, in the above-mentioned case in which A is square and of full rank, simply equals and the general solution equation simplifies to as previously stated, where has completely dropped out of the solution, leaving only a single solution. In other cases, though, remains and hence an infinitude of potential values of the free parameter vector give an infinitude of solutions of the equation.
=== Other methods ===
While systems of three or four equations can be readily solved by hand, computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.
If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.
A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.
== Homogeneous systems ==
A system of linear equations is homogeneous if all of the constant terms are zero:
A homogeneous system is equivalent to a matrix equation of the form
where A is an m &#215; n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.
=== Solution set ===
Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) &#8800; 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:
If u and v are two vectors representing solutions to a homogeneous system, then the vector sum u + v is also a solution to the system.
If u is a vector representing a solution to a homogeneous system, and r is any scalar, then ru is also a solution to the system.
These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. A numerical solutions to a homogeneous system can be found with a SVD decomposition.
=== Relation to nonhomogeneous systems ===
There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:
Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described as
Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.
This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.
== See also ==
Arrangement of hyperplanes
Iterative refinement
LAPACK (the free standard package to solve linear equations numerically; available in Fortran, C, C++)
Linear least squares
Matrix decomposition
Matrix splitting
NAG Numerical Library (NAG Library versions of LAPACK solvers)
Row reduction
Simultaneous equations
== Notes ==
== References ==
=== Textbooks ===
Axler, Sheldon Jay (1997), Linear Algebra Done Right (2nd ed.), Springer-Verlag, ISBN 0-387-98259-0 
Lay, David C. (August 22, 2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7 
Meyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8 
Poole, David (2006), Linear Algebra: A Modern Introduction (2nd ed.), Brooks/Cole, ISBN 0-534-99845-3 
Anton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International 
Leon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall 
Strang, Gilbert (2005), Linear Algebra and Its Applications
PAGE	System of polynomial equations	'Equations'	'Polynomials'
A system of polynomial equations is a set of simultaneous equations f1 = 0, ..., fh = 0 where the fi are polynomials in several variables, say x1, ..., xn, over some field k.
Usually, the field k is either the field of rational numbers or a finite field, although most of the theory applies to any field.
A solution is a set of the values for the xi which make all of the equations true and which belong to some algebraically closed field extension K of k. When k is the field of rational numbers, K is the field of complex numbers.
== Examples and extensions ==
=== Trigonometric equations ===
A trigonometric equation is an equation g = 0 where g is a trigonometric polynomial. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it, replacing sin(x) and cos(x) by two new variables s and c and adding the new equation s2 + c2 &#8722; 1 = 0.
For example the equation
is equivalent to the polynomial system
=== Solutions in a finite field ===
When solving a system over a finite field k with q elements, one is primarily interested in the solutions in k. As the elements of k are exactly the solutions of the equation xq &#8722; x = 0, it suffices, for restricting the solutions to k, to add the equation xiq &#8722; xi = 0 for each variable xi.
=== Coefficients in a number field or in a finite field with non-prime order ===
The elements of a number field are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers.
For example, if a system contains , a system over the rational numbers is obtained by adding the equation r22 &#8722; 2 = 0 and replacing by r2 in the other equations.
In the case of a finite field, the same transformation allows always to suppose that the field k has a prime order.
== Basic properties and definitions ==
A system is overdetermined if the number of equations is higher than the number of variables. A system is inconsistent if it has no solutions. By Hilbert's Nullstellensatz this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems are inconsistent. For example the system x3 &#8722; 1 = 0, x2 &#8722; 1 = 0 is overdetermined (having two equations but only one unknown), but it is not inconsistent since it has the solution x =1.
A system is underdetermined if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension K of k.
A system is zero-dimensional if it has a finite number of solutions in an algebraically closed extension K of k. This terminology comes from the fact that the algebraic variety of the solutions has dimension zero. A system with infinitely many solutions is said to be positive-dimensional.
A zero-dimensional system with as many equations as variables is said to be well-behaved. B &#233; zout's theorem asserts that a well-behaved system whose equations have degrees d1, ..., dn has at most d1...dn solutions. This bound is sharp. If all the degrees are equal to d, this bound becomes dn and is exponential in the number of variables.
This exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with B &#233; zout's bound higher than, say 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound).
== What is solving? ==
The first thing to do for solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a Gr &#246; bner basis of the left hand side of the equations. The system is inconsistent if this Gr &#246; bner basis is reduced to 1. The system is zero-dimensional if, for every variable there is a leading monomial of some element of the Gr &#246; bner basis which is a pure power of this variable. For this test, the best monomial order is usually the graded reverse lexicographic one (grevlex).
If the system is positive-dimensional, it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean "finding a description of the solutions from which the relevant properties of the solutions are easy to extract". There is no commonly accepted such description. In fact there are a lot of different "relevant properties", which involve almost every subfield of algebraic geometry.
A natural example of an open question about solving positive-dimensional systems is the following: decide if a polynomial system over the rational numbers has a finite number of real solutions and compute them. The only published algorithm which allows one to solve this question is cylindrical algebraic decomposition, which is not efficient enough, in practice, to be used for this.
For zero-dimensional systems, solving consists in computing all the solutions. There are two different ways of outputting the solutions. The most common, possible only for real or complex solutions consists in outputting numeric approximations of the solutions. Such a solution is called numeric. A solution is certified if it is provided with a bound on the error of the approximations which separates the different solutions.
The other way to represent the solutions is said to be algebraic. It uses the fact that, for a zero-dimensional system, the solutions belong to the algebraic closure of the field k of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation of the solutions which need only to solve only one univariate polynomial for each solution have to be preferred: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.
== Algebraic representation of the solutions ==
=== Regular chains ===
The usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists in a sequence of polynomials f1(x1), f2(x1, x2), ..., fn(x1, ..., xn) such that, for every i such that 1 &#8804; i &#8804; n
fi is a polynomial in x1, ..., xi only, which has a degree di > 0 in xi ;
the coefficient of xidi in fi is a polynomial in x1, ..., xi &#8722; 1 which does not have any common zero with f1, ..., fi &#8722; 1.
To such a regular chain is associated a triangular system of equations
The solutions of this system are obtained by solving the first univariate equations, substitute the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from fi has degree di and thus that this system has d1 ... dn solutions, provided that there is no multiple root in this resolution process (fundamental theorem of algebra).
Every zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.
There are several algorithms for computing a triangular decomposition of an arbitrary polynomial system (not necessarily zero-dimensional) into regular chains (or regular semi-algebraic systems).
There is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the Gr &#246; bner basis for the graded reverse lexicographic order (grevlex), then deducing the Gr &#246; bner basis by FGLM algorithm and finally applying the Lextriangular algorithm.
This representation of the solutions and the algorithms to compute it are presently, in practice, a very efficient way for solving zero-dimensional polynomial systems with coefficients in a finite field.
For rational coefficients, the Lextriangular algorithm has two drawbacks:
The output used to involve huge integers which may make the computation and the use of the result problematic.
To deduce the numeric values of the solutions from the output, one has to solve univariate polynomials with approximate coefficients, which is a highly unstable problem.
Most algorithms computing triangular decompositions directly (that is, without precomputing a Gr &#246; bner Basis) share above drawbacks, but the most recent ones do not suffer from the one related to output size, as shown by the experimental results reported by Changbo Chen and M. Moreno-Maza. Actually, this observation is predicted by a theoretical argument (which does not give rise to a practical algorithm, though): For a given polynomial system whose solutions can be described by a single regular chain, there exists one regular chain representing in a nearly optimal way in term of size.
In order to address both drawbacks, one can take advantage of the rational univariate representation, which follows. Its output is a single regular chain whose coefficient size is also nearly optimal. However, if the set of solutions has several components of various multiplicities, an output of smaller size may be obtained by decomposing it first with a triangular decomposition algorithm.
=== Rational Univariate Representation ===
The rational univariate representation or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by F. Rouillier for remedying to the above drawbacks of the regular chain representation.
A RUR of a zero-dimensional system consists in a linear combination x0 of the variables, called separating variable, and a system of equations
where h is a univariate polynomial in x0 of degree D and g0, ..., gn are univariate polynomials in x0 of degree less than D.
Given a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.
All but a finite number linear combinations of the variables are separating variables.
When the separating variable is chosen, the RUR exists and is unique. In particular h and the gi are defined independently of any algorithm to compute them.
The solutions of the system are in one to one correspondence with the roots of h and the multiplicity of each root of h equals the multiplicity of the corresponding solution.
The solutions of the system are obtained by substituting the roots of h in the other equations.
If h does not have any multiple root then g0 is the derivative of h.
For example, for above system, every linear combination of the variable, except the multiples of x, y and x + y, is a separating variable. If one choose t = (x &#8722; y)/2 as separating variable, then the RUR is
The RUR is uniquely defined for a given separating element, independently of any algorithm and it preserves the information on the multiplicities of the roots. Basically, a triangular decomposition of a zero-dimensional system does not preserve the multiplicities and is not uniquely defined, but, among all triangular decompositions of a given zero-dimensional system , the equiprojectable decomposition depends only on a coordinate choice of . For this latter, as for the RUR, sharp bounds are available for the coefficients. Consequently, efficient algorithms, based on so-called modular methods, exist for computing the equiprojectable decomposition and the RUR.
These bounds can trivially been obtained for complete intersection systems for the RUR by simply deriving the u-resultant associated with the system, which gives a quite direct way to bound those of an equiprojectable decomposition which are more or less equivalent.
On the computational point of view, there is one main difference between the equiprojectable decomposition and the RUR. The latter has the conceptual advantage of reducing the numeric computation of the solutions to computing the roots of a single univariate polynomial and substituting in some rational functions. One can easily show that the required computation time is then dominated by the isolation of the roots of the univariate polynomial and their refinement up to a sufficient precision.
Moreover, the RUR can trivially been decomposed to get a primary decomposition of the system and, in practice, to get much smaller coefficients than the non decomposed form, especially in the case of systems with high multiplicities. In short one can provide instantaneously a RUR of each primary component through a squarefree decomposition of the first polynomial.
On the other hand, one has to retain that triangular decomposition can be performed in positive dimension, which is not the case of the RUR.
== Algorithms for numerically solving ==
=== General solving algorithms ===
The general numerical algorithms which are designed for any system of simultaneous equations work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow to find all solutions. Especially, when a general method does not find any solution, this is usually not an indication that there is no solution.
Nevertheless two methods deserve to be mentioned here.
Newton's method may be used if the number of equations is equal to the number of variables. It does not allow to find all the solutions nor to prove that there is no solution. But it is very fast when starting from a point which is close to a solution. Therefore it is a basic tool for Homotopy Continuation method described below.
Optimization is rarely used for solving polynomial systems, but it succeeded, around 1970, to show that a system of 81 quadratic equations in 56 variables is not inconsistent. With the other known methods this system remains beyond the possibilities of modern technology. This method consists simply in minimizing the sum of the squares of the equations. If zero is found as a local minimum, then it is attained at a solution. This method works for overdetermined systems, but outputs an empty information if all local minimums which are found are positive.
=== Homotopy continuation method ===
This is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades by J. Verschelde and his associates.
This method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore it is computed by, at least, four different methods and the best value, say N, is kept.
In the second step, a system of polynomial equations is generated which has exactly N solutions that are easy to compute. This new system has the same number n of variables and the same number n of equations and the same general structure as the system to solve, .
Then a homotopy between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the system
.
The homotopy continuation consists in deforming the parameter t from 0 to 1 and following the N solutions during this deformation. This gives the desired solutions for t = 1. Following means that, if , the solutions for are deduced from the solutions for by Newton's method. The difficulty here is to well choose the value of Too large, Newton's convergence may be slow and may even jump from a solution path to another one. Too small, and the number of steps slows down the method.
=== Numerically solving from the Rational Univariate Representation ===
To deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.
The roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.
Aberth method, implemented in MPSolve computes all the complex roots to any precision.
Uspensky's algorithm of Collins and Akritas, improved by Rouillier and Zimmermann and based on Descartes' rule of signs. This algorithms computes the real roots, isolated in intervals of arbitrary small width. It is implemented in Maple (functions fsolve and RootFinding[Isolate]).
== Software packages ==
There are at least four software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.
The Maple function RootFinding[Isolate] takes as input any polynomial system over the rational numbers (if some coefficients are floating point numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error.
Internally, this solver, designed by F. Rouillier computes first a Gr &#246; bner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.
The rational univariate representation may be computed with Maple function Groebner[RationalUnivariateRepresentation].
To extract all the complex solutions from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.
The second solver is PHCpack, written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method. This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.
The third solver is Bertini, written by D. J. Bates, J. D. Hauenstein, A. J. Sommese, and C. W. Wampler. Bertini uses numerical homotopy continuation with adaptive precision. In addition to computing zero-dimensional solution sets, both PHCpack and Bertini are capable of working with positive dimensional solution sets.
The fourth solver is the Maple command RegularChains[RealTriangularize]. For any zero-dimensional input system with rational number coefficients it returns those solutions whose coordinates are real algebraic numbers. Each of these real numbers is encoded by an isolation interval and a defining polynomial.
The command RegularChains[RealTriangularize] is part of the Maple library RegularChains, written by Marc Moreno-Maza, his students and post-doctoral fellows (listed in chronological order of graduation) Francois Lemaire, Yuzhen Xie, Xin Li, Xiao Rong, Liyun Li, Wei Pan and Changbo Chen. Other contributors are Eric Schost, Bican Xia and Wenyuan Wu. This library provides a large set of functionalities for solving zero-dimensional and positive dimensional systems. In both cases, for input systems with rational number coefficients, routines for isolating the real solutions are available. For arbitrary input system of polynomial equations and inequations (with rational number coefficients or with coefficients in a prime field) one can use the command RegularChains[Triangularize] for computing the solutions whose coordinates are in the algebraic closure of the coefficient field. The underlying algorithms are based on the notion of a regular chain.
While the command RegularChains[RealTriangularize] is currently limited to zero-dimensional systems, a future release will be able to process any system of polynomial equations, inequations and inequalities. The corresponding new algorithm is based on the concept of a regular semi-algebraic system.
== See also ==
Triangular decomposition
Wu's method of characteristic set
== References ==
PAGE	Tangent	'Elementary geometry'
In geometry, the tangent line (or simply tangent) to a plane curve at a given point is the straight line that "just touches" the curve at that point. Leibniz defined it as the line through a pair of infinitely close points on the curve. More precisely, a straight line is said to be a tangent of a curve y = f(x) at a point x = c on the curve if the line passes through the point (c, f(c)) on the curve and has slope f &#8202; ' &#8203; (c) where f &#8202; ' &#8203; is the derivative of f. A similar definition applies to space curves and curves in n-dimensional Euclidean space.
As it passes through the point where the tangent line and the curve meet, called the point of tangency, the tangent line is "going in the same direction" as the curve, and is thus the best straight-line approximation to the curve at that point.
Similarly, the tangent plane to a surface at a given point is the plane that "just touches" the surface at that point. The concept of a tangent is one of the most fundamental notions in differential geometry and has been extensively generalized; see Tangent space.
The word tangent comes from the Latin tangere, to touch.
== History ==
The first definition of a tangent was "a right line which touches a curve, but which when produced, does not cut it". This old definition prevents inflection points from having any tangent. It has been dismissed and the modern definitions are equivalent to those of Leibniz. The tangent problem has given rise to differential calculus. The main ideas behind differential calculus are due to Pierre Fermat and were developed by John Wallis, Isaac Barrow, Isaac Newton and Gottfried Leibniz.
Pierre de Fermat developed a general technique for determining the tangents of a curve using his method of adequality in the 1630s.
Leibniz defined the tangent line as the line through a pair of infinitely close points on the curve.
== Tangent line to a curve ==
The intuitive notion that a tangent line "touches" a curve can be made more explicit by considering the sequence of straight lines (secant lines) passing through two points, A and B, those that lie on the function curve. The tangent at A is the limit when point B approximates or tends to A. The existence and uniqueness of the tangent line depends on a certain type of mathematical smoothness, known as "differentiability." For example, if two circular arcs meet at a sharp point (a vertex) then there is no uniquely defined tangent at the vertex because the limit of the progression of secant lines depends on the direction in which "point B" approaches the vertex.
At most points, the tangent touches the curve without crossing it (though it may, when continued, cross the curve at other places away from the point of tangent). A point where the tangent (at this point) crosses the curve is called an inflection point. Circles, parabolas, hyperbolas and ellipses do not have any inflection point, but more complicated curves do have, like the graph of a cubic function, which has exactly one inflection point.
Conversely, it may happen that the curve lies entirely on one side of a straight line passing through a point on it, and yet this straight line is not a tangent line. This is the case, for example, for a line passing through the vertex of a triangle and not intersecting the triangle &#8212; where the tangent line does not exist for the reasons explained above. In convex geometry, such lines are called supporting lines.
=== Analytical approach ===
The geometrical idea of the tangent line as the limit of secant lines serves as the motivation for analytical methods that are used to find tangent lines explicitly. The question of finding the tangent line to a graph, or the tangent line problem, was one of the central questions leading to the development of calculus in the 17th century. In the second book of his Geometry, Ren &#233; Descartes said of the problem of constructing the tangent to a curve, "And I dare say that this is not only the most useful and most general problem in geometry that I know, but even that I have ever desired to know".
==== Intuitive description ====
Suppose that a curve is given as the graph of a function, y = f(x). To find the tangent line at the point p = (a, f(a)), consider another nearby point q = (a + h, f(a + h)) on the curve. The slope of the secant line passing through p and q is equal to the difference quotient
As the point q approaches p, which corresponds to making h smaller and smaller, the difference quotient should approach a certain limiting value k, which is the slope of the tangent line at the point p. If k is known, the equation of the tangent line can be found in the point-slope form:
==== More rigorous description ====
To make the preceding reasoning rigorous, one has to explain what is meant by the difference quotient approaching a certain limiting value k. The precise mathematical formulation was given by Cauchy in the 19th century and is based on the notion of limit. Suppose that the graph does not have a break or a sharp edge at p and it is neither plumb nor too wiggly near p. Then there is a unique value of k such that, as h approaches 0, the difference quotient gets closer and closer to k, and the distance between them becomes negligible compared with the size of h, if h is small enough. This leads to the definition of the slope of the tangent line to the graph as the limit of the difference quotients for the function f. This limit is the derivative of the function f at x = a, denoted f &#8242; (a). Using derivatives, the equation of the tangent line can be stated as follows:
Calculus provides rules for computing the derivatives of functions that are given by formulas, such as the power function, trigonometric functions, exponential function, logarithm, and their various combinations. Thus, equations of the tangents to graphs of all these functions, as well as many others, can be found by the methods of calculus.
==== How the method can fail ====
Calculus also demonstrates that there are functions and points on their graphs for which the limit determining the slope of the tangent line does not exist. For these points the function f is non-differentiable. There are two possible reasons for the method of finding the tangents based on the limits and derivatives to fail: either the geometric tangent exists, but it is a vertical line, which cannot be given in the point-slope form since it does not have a slope, or the graph exhibits one of three behaviors that precludes a geometric tangent.
The graph y = x1/3 illustrates the first possibility: here the difference quotient at a = 0 is equal to h1/3/h = h &#8722; 2/3, which becomes very large as h approaches 0. This curve has a tangent line at the origin that is vertical.
The graph y = x2/3 illustrates another possibility: this graph has a cusp at the origin. This means that, when h approaches 0, the difference quotient at a = 0 approaches plus or minus infinity depending on the sign of x. Thus both branches of the curve are near to the half vertical line for which y=0, but none is near to the negative part of this line. Basically, there is no tangent at the origin in this case, but in some context one may consider this line as a tangent, and even, in algebraic geometry, as a double tangent.
The graph y = |x| of the absolute value function consists of two straight lines with different slopes joined at the origin. As a point q approaches the origin from the right, the secant line always has slope 1. As a point q approaches the origin from the left, the secant line always has slope &#8722; 1. Therefore, there is no unique tangent to the graph at the origin. Having two different (but finite) slopes is called a corner.
Finally, since differentiability implies continuity, the contrapositive states discontinuity implies non-differentiability. Any such jump or point discontinuity will have no tangent line. This includes cases where one slope approaches positive infinity while the other approaches negative infinity, leading to an infinite jump discontinuity
=== Equations ===
When the curve is given by y = f(x) then the slope of the tangent is so by the point &#8211; slope formula the equation of the tangent line at (X, Y) is
where (x, y) are the coordinates of any point on the tangent line, and where the derivative is evaluated at .
When the curve is given by y = f(x), the tangent line's equation can also be found by using polynomial division to divide by ; if the remainder is denoted by , then the equation of the tangent line is given by
When the equation of the curve is given in the form f(x, y) = 0 then the value of the slope can be found by implicit differentiation, giving
The equation of the tangent line at a point (X,Y) such that f(X,Y) = 0 is then
This equation remains true if but (in this case the slope of the tangent is infinite). If the tangent line is not defined and the point (X,Y) is said singular.
For algebraic curves, computations may be simplified somewhat by converting to homogeneous coordinates. Specifically, let the homogeneous equation of the curve be g(x, y, z) = 0 where g is a homogeneous function of degree n. Then, if (X, Y, Z) lies on the curve, Euler's theorem implies
It follows that the homogeneous equation of the tangent line is
The equation of the tangent line in Cartesian coordinates can be found by setting z=1 in this equation.
To apply this to algebraic curves, write f(x, y) as
where each ur is the sum of all terms of degree r. The homogeneous equation of the curve is then
Applying the equation above and setting z=1 produces
as the equation of the tangent line. The equation in this form is often simpler to use in practice since no further simplification is needed after it is applied.
If the curve is given parametrically by
then the slope of the tangent is
giving the equation for the tangent line at as
If , the tangent line is not defined. However, it may occur that the tangent line exists and may be computed from an implicit equation of the curve.
=== Normal line to a curve ===
The line perpendicular to the tangent line to a curve at the point of tangency is called the normal line to the curve at that point. The slopes of perpendicular lines have product &#8722; 1, so if the equation of the curve is y = f(x) then slope of the normal line is
and it follows that the equation of the normal line at (X, Y) is
Similarly, if the equation of the curve has the form f(x, y) = 0 then the equation of the normal line is given by
If the curve is given parametrically by
then the equation of the normal line is
=== Angle between curves ===
The angle between two curves at a point where they intersect is defined as the angle between their tangent lines at that point. More specifically, two curves are said to be tangent at a point if they have the same tangent at a point, and orthogonal if their tangent lines are orthogonal.
=== Multiple tangents at the origin ===
The formulas above fail when the point is a singular point. In this case there may be two or more branches of the curve which pass through the point, each branch having its own tangent line. When the point is the origin, the equations of these lines can be found for algebraic curves by factoring the equation formed by eliminating all but the lowest degree terms from the original equation. Since any point can be made the origin by a change of variables, this gives a method for finding the tangent lines at any singular point.
For example, the equation of the lima &#231; on trisectrix shown to the right is
Expanding this and eliminating all but terms of degree 2 gives
which, when factored, becomes
So these are the equations of the two tangent lines through the origin.
== Tangent circles ==
Two circles of non-equal radius, both in the same plane, are said to be tangent to each other if they meet at only one point. Equivalently, two circles, with radii of ri and centers at (xi, yi), for i = 1, 2 are said to be tangent to each other if
Two circles are externally tangent if the distance between their centres is equal to the sum of their radii.
Two circles are internally tangent if the distance between their centres is equal to the difference between their radii.
== Surfaces and higher-dimensional manifolds ==
The tangent plane to a surface at a given point p is defined in an analogous way to the tangent line in the case of curves. It is the best approximation of the surface by a plane at p, and can be obtained as the limiting position of the planes passing through 3 distinct points on the surface close to p as these points converge to p. More generally, there is a k-dimensional tangent space at each point of a k-dimensional manifold in the n-dimensional Euclidean space.
== See also ==
Newton's method
Normal (geometry)
Osculating circle
Osculating curve
Perpendicular
Subtangent
Supporting line
Tangent cone
Tangential angle
Tangential component
Tangent lines to circles
== References ==
J. Edwards (1892). Differential Calculus. London: MacMillan and Co. pp. 143 ff. 
== External links ==
Hazewinkel, Michiel, ed. (2001), "Tangent line", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Tangent Line", MathWorld.
Tangent to a circle With interactive animation
Tangent and first derivative &#8212; An interactive simulation
The Tangent Parabola by John H. Mathews
PAGE	Transversal (geometry)	'Elementary geometry'
In geometry, a transversal is a line that passes through two lines in the same plane at two distinct points. Transversals play a role in establishing whether two other lines in the Euclidean plane are parallel. The intersections of a transversal with two lines create various types of pairs of angles: consecutive interior angles, corresponding angles, and alternate angles. By Euclid's parallel postulate, if the two lines are parallel, consecutive interior angles are supplementary, corresponding angles are equal, and alternate angles are equal.
== Angles of a transversal ==
A transversal produces 8 angles, as shown in the graph at the above left:
4 with each of the two lines, namely &#945; , &#946; , &#947; and &#948; and then &#945; 1, &#946; 1, &#947; 1 and &#948; 1; and
4 of which are interior (between the two lines), namely &#945; , &#946; , &#947; 1 and &#948; 1 and 4 of which are exterior, namely &#945; 1, &#946; 1, &#947; and &#948; .
A transversal that cuts two parallel lines at right angles is called a perpendicular transversal. In this case, all 8 angles are right angles 
When the lines are parallel, a case that is often considered, a transversal produces several congruent and several supplementary angles. Some of these angle pairs have specific names and are discussed below:corresponding angles, alternate angles, and consecutive angles.
=== Corresponding angles ===
For an alternate use, see Corresponding angles (congruence and similarity).
Corresponding angles are the four pairs of angles that:
have distinct vertex points,
lie on the same side of the transversal and
one angle is interior and the other is exterior.
Note: This follows directly from Euclid's parallel postulate. Further, if the angles of one pair are congruent, then the angles of each of the other pairs are also congruent. In our images with parallel lines, corresponding angle pairs are: &#945; = &#945; 1, &#946; = &#946; 1, &#947; = &#947; 1 and &#948; = &#948; 1.
=== Alternate angles ===
Alternate angles are the four pairs of angles that:
have distinct vertex points,
lie on opposite sides of the transversal and
both angles are interior or both angles are exterior.
Note: This follows directly from Euclid's parallel postulate. Further, if the angles of one pair are congruent, then the angles of each of the other pairs are also congruent. In our images with parallel lines, alternate angle pairs with both angles interior are: &#945; = &#947; 1, &#946; = &#948; 1 and with both angles exterior are: &#947; = &#945; 1 and &#948; = &#946; 1.
=== Consecutive angles ===
Consecutive interior angles are the two pairs of angles that:
have distinct vertex points,
lie on the same side of the transversal and
are both interior.
By the definition of a straight line and the properties of vertical angles, if one pair is supplementary, the other pair is also supplementary.
== Other characteristics of transversals ==
If three lines in general position form a triangle are then cut by a transversal, the lengths of the six resulting segments satisfy Menelaus' theorem.
== Related theorems ==
Euclid's formulation of the parallel postulate may be stated in terms of a transversal. Specifically, if the interior angles on the same side of the transversal are less than two right angles then lines must intersect. In fact, Euclid uses the same phrase in Greek that is usually translated as "transversal".
Euclid's Proposition 27 states that if a transversal intersects two lines so that alternate interior angles are congruent, then the lines are parallel. Euclid proves this by contradiction: If the lines are not parallel then they must intersect and a triangle is formed. Then one of the alternate angles is an exterior angle equal to the other angle which is an opposite interior angle in the triangle. This contradicts Proposition 16 which states that an exterior angle on a triangle is always greater than the opposite interior angles.
Euclid's Proposition 28 extends this result in two ways. First, if a transversal intersects two lines so that corresponding angles are congruent, then the lines are parallel. Second, if a transversal intersects two lines so that interior angles on the same side of the transversal are supplementary, then the lines are parallel. These follow from the previous proposition by applying the fact than opposite angles on intersecting lines equal (Prop. 15) and that adjacent angles on a line are supplementary (Prop. 13). As noted by Proclus, Euclid gives only three of a possible six such criteria for parallel lines.
Euclid's Proposition 29 is a converse to the previous two. First, if a transversal intersects two parallel lines, then the alternate interior angles are congruent. If not then one is greater than the other, which implies its supplement is less than the supplement of the other angle. This implies that there are interior angles on the same side of the transversal which are less than two right angles, contradicting the fifth postulate. The proposition continues by stating that in a transversal of two parallel lines, corresponding angles are congruent and interior angles on the same side equal two right angles. These statements follow in the same way that Prop. 28 follows from Prop. 27.
Euclid's proof makes essential use of fifth postulate, however modern treatments of geometry use Playfair's axiom instead. To prove proposition 29 assuming Playfair's axiom, let a transversal cross two parallel lines and suppose alternate interior angles are not equal. Draw a third line through the point where the transversal crosses the first line, but with angle equal to the angle the transversal makes with the second angle. This produces two different lines through a point both parallel to another line, contradicting the axiom.
== References ==
Holgate, Thomas Franklin (1901). Elementary Geometry. Macmillan. 
Thomas Little Heath, T.L. (1908). The thirteen books of Euclid's Elements 1. The University Press. pp. 307 ff.
PAGE	Zero of a function	'Elementary mathematics'
In mathematics, a zero, also sometimes called a root, of a real-, complex- or generally vector-valued function f is a member x of the domain of f such that f(x) vanishes at x; that is,
In other words, a "zero" of a function is an input value that produces an output of zero (0).
A root of a polynomial is a zero of the associated polynomial function. The fundamental theorem of algebra shows that any non-zero polynomial has a number of roots at most equal to its degree and that the number of roots and the degree are equal when one considers the complex roots (or more generally the roots in an algebraically closed extension) counted with their multiplicities. For example, the polynomial f of degree two, defined by
has the two roots 2 and 3, since
If the function maps real numbers to real numbers, its zeroes are the x-coordinates of the points where its graph meets the x-axis. An alternative name for such a point (x,0) in this context is an x-intercept.
== Polynomial roots ==
Every real polynomial of odd degree has an odd number of real roots (counting multiplicities); likewise, a real polynomial of even degree must have an even number of real roots. Consequently, real odd polynomials must have at least one real root (because one is the smallest odd whole number), whereas even polynomials may have none. This principle can be proven by reference to the intermediate value theorem: since polynomial functions are continuous, the function value must cross zero in the process of changing from negative to positive or vice-versa.
=== Fundamental theorem of algebra ===
The fundamental theorem of algebra states that every polynomial of degree n has n complex roots, counted with their multiplicities. The non-real roots of polynomials with real coefficients come in conjugate pairs. Vieta's formulas relate the coefficients of a polynomial to sums and products of its roots.
== Computing roots ==
Computing roots of certain functions, especially polynomial functions, frequently requires the use of specialised or approximation techniques (for example, Newton's method).
== Zero set ==
In topology and other areas of mathematics, the zero set of a real-valued function f : X &#8594; R (or more generally, a function taking values in some additive group) is the subset of X (the inverse image of 0}).
Zero sets are important in many areas of mathematics. One area of particular importance is algebraic geometry, where the first definition of an algebraic variety is through zero-sets. For instance, for each set S of polynomials in k[x1, ..., xn], one defines the zero-locus Z(S) to be the set of points in An on which the functions in S simultaneously vanish, that is to say
 Then a subset V of An is called an affine algebraic set if V = Z(S) for some S. These affine algebraic sets are the fundamental building blocks of algebraic geometry.
== See also ==
Zero (complex analysis)
Pole (complex analysis)
Fundamental theorem of algebra
Newton's method
Sendov's conjecture
Marden's theorem
Vanish at infinity
== References ==
== Further reading ==
Weisstein, Eric W., "Root", MathWorld.
